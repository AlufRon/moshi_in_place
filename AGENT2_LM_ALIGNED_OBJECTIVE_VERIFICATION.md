# AGENT 2: LM-Aligned Objective Implementation Verification

**Verification Date:** 2025-11-13
**Focus:** Paper Section 3.2 (lines 444-476) - LM-aligned target generation and Conv1D semantics
**Critical Analysis:** Conv1D chunking behavior and causality at chunk boundaries
**Paper Reference:** `/home/user/moshi_in_place/papers/ttt_in_place_paper.txt`
**Code Reference:** `/home/user/moshi_in_place/moshi/moshi/moshi/modules/ttt_module.py`

---

## Executive Summary

**Overall Status:** ‚ö†Ô∏è **MAJOR ISSUE FOUND - Causality violation in Conv1D chunking**

This verification analyzes the implementation of the LM-aligned objective from Paper Section 3.2. While most components are correctly implemented, a **critical causality bug** was discovered: Conv1D is applied to the full sequence before chunking, violating the paper's explicit requirement that "each delta calculation is isolated to its respective chunk."

**Key Findings:**
- ‚úÖ Formula VÃÇ = Conv1D(X‚ÇÄ)¬∑W‚Çú‚Çê·µ£ùì∞‚Çë‚Çú correctly implemented
- ‚úÖ Conv1D semantics for next-token prediction working as intended
- ‚úÖ Loss function and gradient derivation mathematically correct
- üö® **CRITICAL BUG:** Conv1D applied to full sequence instead of per-chunk
- üö® Causes information leakage at every chunk boundary

---

## Part 1: Formula Verification - VÃÇ = Conv1D(X‚ÇÄ)¬∑W‚Çú‚Çê·µ£ùì∞‚Çë‚Çú

### Paper Specification (Lines 456-463)

```
VÃÇ = Conv1D(X‚ÇÄ)¬∑W‚Çú‚Çê·µ£ùì∞‚Çë‚Çú

where:
- X‚ÇÄ ‚àà R^{n√ód_model} = token embeddings
- Conv1D(¬∑) = 1D Convolution operator
- W‚Çú‚Çê·µ£ùì∞‚Çë‚Çú ‚àà R^{d_model√ód_model} = trainable projection matrix
- Next-Token target achieved by: W‚Çú‚Çê·µ£ùì∞‚Çë‚Çú = Identity, Conv1D kernel = [0,1]
```

### Code Implementation

**Location:** `ttt_module.py` lines 50-67

```python
class LMAlignedTargetGenerator(nn.Module):
    """Generates V_hat targets from token embeddings.

    V_hat = W_target( CausalConv1D(token_embeddings) )
    returns tensor [B, T, d_model]
    """

    def __init__(self, d_model: int, kernel_size: int = 2, device=None, dtype=None):
        super().__init__()
        factory_kwargs = {k: v for k, v in {"device": device, "dtype": dtype}.items() if v is not None}
        self.conv1d = CausalConv1D(d_model, d_model, kernel_size=kernel_size)
        # learnable projection (slow weight)
        self.W_target = nn.Linear(d_model, d_model, bias=False, **factory_kwargs)

    def forward(self, token_embeddings: torch.Tensor) -> torch.Tensor:
        # token_embeddings: [B, T, d_model]
        x = self.conv1d(token_embeddings)
        return self.W_target(x)  # VÃÇ = W_target(Conv1D(X‚ÇÄ))
```

### Verification

**Status:** ‚úÖ **CORRECT**

- ‚úÖ Order of operations matches paper: Conv1D first, then W_target projection
- ‚úÖ Input shape: token_embeddings [B, T, d_model]
- ‚úÖ Output shape: VÃÇ [B, T, d_model]
- ‚úÖ Both Conv1D kernel and W_target are learnable "slow weights"
- ‚úÖ No bias terms (matches paper's formulation)

---

## Part 2: Conv1D Semantics - Next-Token Prediction

### Paper Requirement (Lines 455-463)

> "we propose to align the objective with the Next-Token Prediction (NTP) goal governing LLMs. To achieve this, we specify the target v to include future token information... the Next-Token target can be achieved by parameterizing W‚Çú‚Çê·µ£ùì∞‚Çë‚Çú as an identity transformation and assigning Conv1D(¬∑)'s kernel weights to be 1 for the next token and 0 for other tokens"

**Expected Behavior:** Position t should see tokens [t, t+1] for next-token prediction

### Code Implementation

**Location:** `ttt_module.py` lines 17-47

```python
class CausalConv1D(nn.Module):
    """Causal 1D convolution that can look at future tokens via right padding.

    Behavior: input [B, T, C] -> conv over time with padding on the right so
    each output position can incorporate a fixed number of "future" tokens.
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 2):
        super().__init__()
        self.kernel_size = int(kernel_size)
        # use bias=False to match linear-like behaviour
        # Conv1d expects [B, C, T]
        self.conv = nn.Conv1d(in_channels, out_channels, self.kernel_size, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [B, T, C]
        if x.dim() != 3:
            raise ValueError(f"CausalConv1D expects 3D input [B,T,C], got {x.shape}")
        x = x.transpose(1, 2)  # [B, T, C] -> [B, C, T]

        # pad on the right so conv window can see future tokens
        pad = (0, self.kernel_size - 1)  # Right padding
        x_padded = F.pad(x, pad)

        x_conv = self.conv(x_padded)
        # sanity check: temporal length should be the same as original
        if x_conv.shape[-1] != x.shape[-1]:
            raise RuntimeError(...)

        x = x_conv.transpose(1, 2)  # [B, C, T] -> [B, T, C]
        return x
```

### Padding Analysis

**Padding configuration:**
- `pad = (0, kernel_size-1)` means:
  - Left padding: 0 (no past context beyond current position)
  - Right padding: kernel_size-1 (future context)
- For kernel_size=2: pad = (0, 1)

**Receptive field for position t:**
- Input length: T
- Padded length: T + (kernel_size-1)
- Position t in original sequence maps to position t in padded sequence
- Conv kernel at position t spans: [t, t+kernel_size-1]
- For kernel_size=2: position t sees [t, t+1]

**Mathematical verification:**
```
Output[t] = Œ£(k=0 to kernel_size-1) kernel[k] ¬∑ Input_padded[t+k]
         = kernel[0] ¬∑ Input[t] + kernel[1] ¬∑ Input[t+1]  (for kernel_size=2)
```

### Verification

**Status:** ‚úÖ **CORRECT for Next-Token Prediction**

- ‚úÖ Position t sees [t, t+1] = current token + next token
- ‚úÖ Matches paper's "Next-Token Prediction" objective
- ‚úÖ Kernel weights are learnable (not hardcoded to [0,1])
- ‚úÖ W_target is learnable projection (not identity)
- ‚úÖ Output temporal dimension preserved (T ‚Üí T)
- ‚úÖ Comment correctly describes: "can look at future tokens via right padding"

**Design notes:**
- The paper mentions kernel = [0,1] and W_target = Identity as an example
- In practice, both are learned, which is more flexible
- The RIGHT padding is intentional and correct for LM-aligned objectives

---

## Part 3: Loss Function and Gradient Derivation

### Paper Specification (Lines 464-471)

```
Loss: L(¬∑, ¬∑) = -‚ü®¬∑, ¬∑‚ü©_F  (negative Frobenius inner product)

Under this loss function, the gradient with respect to the fast weights can be
directly derived:

W^(i)_down = W^(i-1)_down + Œ∑¬∑VÃÇ^‚ä§_[i]¬∑Z_[i]  (Equation 1)
```

### Mathematical Derivation

**Step 1: Expand the loss**
```
L = -‚ü®O, VÃÇ‚ü©_F
  = -‚ü®Z¬∑W^T_down, VÃÇ‚ü©_F
  = -tr((Z¬∑W^T_down)^T ¬∑ VÃÇ)
  = -tr(W_down¬∑Z^T¬∑VÃÇ)
```

**Step 2: Compute gradient**
```
‚àá_{W_down} L = -VÃÇ^T¬∑Z
```

**Step 3: Gradient descent update**
```
W_new = W_old - Œ∑¬∑‚àáL
      = W_old - Œ∑¬∑(-VÃÇ^T¬∑Z)
      = W_old + Œ∑¬∑VÃÇ^T¬∑Z  ‚úÖ
```

This matches Equation 1 from the paper!

### Code Implementation

**Location:** `ttt_module.py` lines 200-216

```python
# compute deltas in parallel: result shape [num_chunks, B, dim, hidden]
# einsum to reorder directly
deltas = torch.einsum('b n t d, b n t h -> n b d h', Vc, Zc)
# This computes: V_c^T @ Z_c for each chunk
# Vc: [B, num_chunks, chunk_size, dim]
# Zc: [B, num_chunks, chunk_size, hidden]
# deltas[i] = V_c[i]^T @ Z_c[i]: [dim, hidden]

# prefix sum across chunks (causal): S_i = sum_{j=0..i-1} deltas_j
cumsum = torch.cumsum(deltas, dim=0)
zero = torch.zeros_like(cumsum[0:1])
S = torch.cat([zero, cumsum[:-1]], dim=0)  # [num_chunks, B, dim, hidden]

# effective weights per chunk
W_eff = W_init_bc + self.ttt_lr * S
```

### Einsum Breakdown

The einsum `'b n t d, b n t h -> n b d h'` computes:
- For each batch b and chunk n
- Sum over time t: V[b,n,t,d] * Z[b,n,t,h]
- Result: deltas[n,b,d,h] = Œ£_t V[b,n,t,d] * Z[b,n,t,h]
- This is exactly V^T @ Z for each chunk

### Verification

**Status:** ‚úÖ **CORRECT - Direct Gradient Application**

- ‚úÖ No explicit loss computation (optimization: directly apply gradient)
- ‚úÖ `deltas[i] = V^T[i] @ Z[i]` correctly computes ‚àá_{W_down} L for chunk i
- ‚úÖ Causal prefix sum ensures W^(i) uses updates from chunks 0 to i-1 only
- ‚úÖ Mathematically equivalent to sequential update with loss L(¬∑, ¬∑) = -‚ü®¬∑, ¬∑‚ü©_F
- ‚úÖ Efficient: avoids materializing loss value
- ‚úÖ Numerically stable: direct computation of gradient

---

## Part 4: Kernel Size and Padding Behavior

### Default Configuration

**Paper (Lines 461-463):**
- Next-Token target implies kernel_size=2
- Position t sees [t, t+1]

**Code (Lines 108, 60):**
```python
self.conv_kernel_size = int(self.ttt_config.get("conv_kernel_size", 2))  # default=2
self.conv1d = CausalConv1D(d_model, d_model, kernel_size=kernel_size)
```

### Padding Math Verification

For kernel_size=K:
- Input length: T
- Padding: (0, K-1) on right
- Padded length: T + K - 1
- Conv output length: (T + K - 1) - K + 1 = T ‚úÖ

For default kernel_size=2:
- Padding: (0, 1)
- Position t sees: [t, t+1]
- Perfect for next-token prediction ‚úÖ

### Verification

**Status:** ‚úÖ **CORRECT**

- ‚úÖ Default kernel_size=2 matches paper's implicit requirement
- ‚úÖ Right padding (0, kernel_size-1) allows next-token visibility
- ‚úÖ Output temporal dimension preserved: T ‚Üí T
- ‚úÖ Configurable via ttt_config for experimentation
- ‚úÖ Consistent with LM-aligned objective design

---

## Part 5: CRITICAL ISSUE - Conv1D Application Scope

### Paper's Intent - Algorithm 1 (Lines 2003-2006)

```
Algorithm 1 In-Place TTT with Context Parallelism (Single Layer)
...
Stage 1 (Lines 1995-2010): Compute update deltas in parallel
  for all i ‚àà {1, ..., T} in parallel do
    Hi ‚Üê AttentionBlock(X^(i); Œ∏)
    Ui, Gi ‚Üê Hi¬∑W^‚ä§_up, Hi¬∑W^‚ä§_gate
    Zi ‚Üê œï(Gi) ‚äô Ui
    Vi ‚Üê Conv1D_K(X^(i)_0)¬∑W_target  ‚óÑ‚îÄ‚îÄ KEY LINE
    ŒîWi ‚Üê V^‚ä§_i ¬∑ Zi
  end for
```

**Key notation:** `X^(i)_0` with superscript (i) indicates **chunk i only**, not full sequence

### Paper's Explicit Causality Requirement (Lines 736-738)

> "To ensure that the update delta for chunk i itself contains **no future information**, we apply **causal padding to the 1D convolution** when generating the value. This **isolates each delta calculation to its respective chunk**, making the parallel scan mathematically equivalent to a sequential update."

**Three explicit requirements:**
1. ‚úó "delta for chunk i contains no future information"
2. ‚úó "causal padding" at chunk boundaries
3. ‚úó "isolates each delta calculation to its respective chunk"

### Current Code Implementation

**Location:** `ttt_module.py` lines 144-156

```python
def _ttt_forward(self, x: torch.Tensor, token_embeddings: torch.Tensor) -> torch.Tensor:
    # x: [B, T, dim]
    B, T, _ = x.shape

    # compute Z - use the module directly (handles both Linear and LoRALinear)
    h = self.linear_in(x)
    h = h.view(B, T, 2, -1)
    Z = self.activation(h[..., 0, :]) * h[..., 1, :]  # [B, T, hidden]

    # V_hat from token embeddings
    V_hat = self.target_generator(token_embeddings)  # [B, T, dim] ‚óÑ‚îÄ‚îÄ FULL SEQUENCE!

    # chunk-wise parallel update
    return self._parallel_ttt_update(Z, V_hat)
```

**What actually happens:**
1. `self.target_generator(token_embeddings)` applies Conv1D to **ENTIRE sequence** [0, T-1]
2. Inside `_parallel_ttt_update`, V_hat is **then chunked**: V_hat[0:256], V_hat[256:512], etc.
3. Deltas computed from already-chunked V_hat

**Problem:** Conv1D was already applied across chunk boundaries before chunking!

---

## Part 6: The Causality Violation - Concrete Analysis

### Setup for Analysis

Consider:
- Sequence length: T = 512 tokens
- chunk_size = 256
- kernel_size = 2
- **Chunk 0:** positions [0, 1, ..., 255]
- **Chunk 1:** positions [256, 257, ..., 511]

### The Bug at Chunk Boundary (Position 255)

**With current implementation (Conv1D on full sequence):**

```
Step 1: Apply Conv1D to full sequence [0..511]
  V_hat = self.target_generator(token_embeddings)  # token_embeddings: [B, 512, d_model]

Step 2: At position 255 (last token of chunk 0)
  Conv1D receptive field at position 255: [255, 256]
  ‚Üë Position 256 is the FIRST token of Chunk 1!

  V_hat[255] = W_target(Conv1D[255])
             = W_target(kernel[0]¬∑X_0[255] + kernel[1]¬∑X_0[256])
                                                        ‚Üë
                                        Information from Chunk 1!

Step 3: Compute delta_0
  delta_0 = Œ£(t=0 to 255) V_hat[t]^T @ Z[t]
          = V_hat[0]^T¬∑Z[0] + ... + V_hat[255]^T¬∑Z[255]
                                        ‚Üë
                                Includes leaked information!

Result: delta_0 contains information from token 256 (Chunk 1)
```

### Why This Violates the Paper

**Paper requirement (line 736-737):**
> "the update delta for chunk i itself contains **no future information**"

**Violation:**
- delta_0 should only depend on chunk 0 tokens [0..255]
- But V_hat[255] depends on token 256 (chunk 1)
- Therefore delta_0 depends on chunk 1's input
- **This violates causality!**

### What Should Happen (Per Paper)

**Correct implementation (per-chunk Conv1D):**

```
Step 1: Chunk the token embeddings FIRST
  X_chunk0 = token_embeddings[:, 0:256, :]   # Chunk 0 only
  X_chunk1 = token_embeddings[:, 256:512, :]  # Chunk 1 only

Step 2: Apply Conv1D to each chunk independently
  V_hat_chunk0 = self.target_generator(X_chunk0)  # [B, 256, d_model]

Step 3: At position 255 (last token of chunk 0)
  Conv1D receptive field at position 255: [255, PAD]
  ‚Üë Padding, not position 256!

  V_hat_chunk0[255] = W_target(kernel[0]¬∑X_0[255] + kernel[1]¬∑PAD)
                                                        ‚Üë
                                        No information from Chunk 1!

Step 4: Compute delta_0
  delta_0 = Œ£(t=0 to 255) V_hat_chunk0[t]^T @ Z[t]

Result: delta_0 contains NO information beyond Chunk 0 ‚úÖ
```

---

## Part 7: Impact Analysis

### Theoretical Impact

**1. Breaks Associativity**

The parallel algorithm relies on:
```
Œî_i = VÃÇ^T_[i] ¬∑ Z_[i]
```
where Œî_i is **independent** of Œî_j for j ‚â† i.

Current bug: Œî_i depends on chunk i+1's input through VÃÇ_[i][boundary]

**2. Violates Causal Equivalence**

Paper claims (line 738):
> "making the parallel scan mathematically equivalent to a sequential update"

This equivalence **requires** that each chunk's delta is independent.

Current bug breaks this equivalence because:
- Parallel: delta_i sees token (i+1)¬∑C via Conv1D
- Sequential: delta_i would only see up to token i¬∑C

**3. Undermines Context-Parallelism Property**

The paper's main contribution is context-parallelism with strict causality.
Current bug: causality is violated at every chunk boundary.

### Practical Impact

**For chunk_size=256 and long sequences:**
- 4096 tokens ‚Üí 16 chunks ‚Üí 15 chunk boundaries with leakage
- 8192 tokens ‚Üí 32 chunks ‚Üí 31 chunk boundaries with leakage

**Each boundary leaks kernel_size-1 tokens:**
- For kernel_size=2: 1 token leaks per boundary
- Total leakage: (num_chunks - 1) √ó (kernel_size - 1) tokens

**Example:**
- 4096 tokens, chunk_size=256, kernel_size=2
- 16 chunks, 15 boundaries
- 15 tokens leaked across boundaries

### Severity Assessment

**Severity: CRITICAL (HIGH)**

**Justification:**
1. Violates core theoretical property of the paper
2. Affects every forward pass during training and inference
3. Paper explicitly states this must be avoided (line 736-738)
4. Algorithm 1 notation clearly indicates per-chunk application
5. Breaks mathematical equivalence between parallel and sequential

**NOT a minor implementation detail - this is a fundamental algorithmic violation**

---

## Part 8: Evidence Summary

### Evidence for Bug

**1. Paper's Algorithm 1 (Line 2006):**
```
Vi ‚Üê Conv1D_K(X^(i)_0)¬∑W_target
```
Superscript (i) on X_0 means **chunk i only**

**2. Paper's Line 736-738:**
> "This **isolates each delta calculation to its respective chunk**"

Unambiguous: each delta must be isolated to its chunk.

**3. Paper's Line 738:**
> "making the parallel scan **mathematically equivalent** to a sequential update"

This equivalence requires strict causality.

**4. Code Evidence:**
```python
# Line 153 in ttt_module.py
V_hat = self.target_generator(token_embeddings)  # [B, T, dim] - FULL SEQUENCE
```
Conv1D applied to full T, not per-chunk.

### Alternative Interpretation (Considered and Rejected)

**Could this be intentional?**

‚ùå **NO** - Multiple reasons:
1. Paper says "isolates... to its respective chunk" - very explicit
2. Algorithm 1 uses X^(i)_0 notation - indicates per-chunk
3. Line 738: "equivalent to sequential" - violated by cross-chunk Conv1D
4. Paper emphasizes causality as key contribution
5. No mention of relaxing causality at chunk boundaries

**Conclusion:** This is definitively a bug, not a design choice.

---

## Part 9: Recommendations

### Fix Strategy

**Option 1: Per-Chunk Conv1D (Aligns with Paper)**

```python
def _ttt_forward(self, x: torch.Tensor, token_embeddings: torch.Tensor) -> torch.Tensor:
    B, T, _ = x.shape

    # Compute Z
    h = self.linear_in(x)
    h = h.view(B, T, 2, -1)
    Z = self.activation(h[..., 0, :]) * h[..., 1, :]

    # Chunk token_embeddings FIRST
    effective_chunk_size = min(T, self.chunk_size)
    num_chunks = (T + effective_chunk_size - 1) // effective_chunk_size
    pad_size = num_chunks * effective_chunk_size - T

    if pad_size != 0:
        token_embeddings = F.pad(token_embeddings, (0, 0, 0, pad_size))

    # Reshape to chunks
    token_emb_chunks = token_embeddings.view(B, num_chunks, effective_chunk_size, -1)

    # Apply Conv1D per chunk
    V_hat_chunks = []
    for i in range(num_chunks):
        chunk_i = token_emb_chunks[:, i, :, :]  # [B, chunk_size, d_model]
        V_hat_i = self.target_generator(chunk_i)  # Per-chunk Conv1D
        V_hat_chunks.append(V_hat_i)

    V_hat = torch.cat(V_hat_chunks, dim=1)  # [B, T_padded, dim]

    if pad_size != 0:
        V_hat = V_hat[:, :T, :]  # Remove padding

    return self._parallel_ttt_update(Z, V_hat)
```

**Pros:**
- ‚úÖ Matches paper exactly
- ‚úÖ Maintains strict causality
- ‚úÖ No information leakage at boundaries

**Cons:**
- ‚ö†Ô∏è Requires loop over chunks (less efficient)
- ‚ö†Ô∏è May not compile well with torch.compile

**Option 2: Vectorized Per-Chunk Conv1D**

Refactor `LMAlignedTargetGenerator` to accept chunked input and apply Conv1D per chunk using grouped convolutions or masking.

### Testing Strategy

**Unit test to verify the fix:**

```python
def test_conv1d_chunk_boundary_causality():
    """Verify that Conv1D doesn't leak across chunk boundaries."""
    model = TTTGating(...)
    chunk_size = 256
    kernel_size = 2

    # Create test input
    token_embeddings = torch.randn(1, 512, d_model)

    # Perturb token 256 (first of chunk 1)
    token_embeddings_perturbed = token_embeddings.clone()
    token_embeddings_perturbed[0, 256, :] += 10.0  # Large perturbation

    # Compute V_hat for both
    V_hat_original = model.target_generator(token_embeddings)
    V_hat_perturbed = model.target_generator(token_embeddings_perturbed)

    # Check position 255 (last of chunk 0)
    # Should NOT be affected by perturbation at position 256
    diff = (V_hat_original[0, 255, :] - V_hat_perturbed[0, 255, :]).abs().max()

    assert diff < 1e-6, f"Position 255 affected by position 256! diff={diff}"
```

**Current implementation would FAIL this test.**
**Fixed implementation should PASS.**

### Priority

**Priority: HIGH**

This should be fixed before:
1. Publishing any results based on this implementation
2. Claiming strict causality or equivalence to paper
3. Using for production inference

---

## Part 10: Summary

### What's Correct ‚úÖ

1. ‚úÖ VÃÇ = Conv1D(X‚ÇÄ)¬∑W‚Çú‚Çê·µ£ùì∞‚Çë‚Çú formula implementation
2. ‚úÖ Conv1D semantics for next-token prediction (right padding)
3. ‚úÖ Loss function L(¬∑,¬∑) = -‚ü®¬∑,¬∑‚ü©_F and gradient derivation
4. ‚úÖ Kernel size default (kernel_size=2)
5. ‚úÖ Padding preserves temporal dimension
6. ‚úÖ Mathematical correctness of update rule
7. ‚úÖ Efficient einsum-based delta computation
8. ‚úÖ Both Conv1D and W_target are learnable
9. ‚úÖ No bias terms (matches paper)

### What's Broken üö®

1. üö® **CRITICAL:** Conv1D applied to full sequence instead of per-chunk
2. üö® **CRITICAL:** Causality violation at every chunk boundary
3. üö® **CRITICAL:** delta_i contains future information from chunk i+1
4. üö® Violates paper's explicit requirement: "isolates each delta calculation to its respective chunk"
5. üö® Breaks theoretical guarantee: parallel ‚â° sequential
6. üö® Algorithm 1 notation X^(i)_0 (chunk i) not followed

### Root Cause

**File:** `/home/user/moshi_in_place/moshi/moshi/moshi/modules/ttt_module.py`
**Line:** 153

```python
# Current (WRONG):
V_hat = self.target_generator(token_embeddings)  # Full sequence [B, T, dim]
return self._parallel_ttt_update(Z, V_hat)       # Then chunked

# Should be (CORRECT per paper):
# Chunk first, then apply Conv1D per chunk
```

### Final Assessment

**Overall Implementation Quality:** 85/100
- Core logic is sound
- Gradient math is correct
- Efficiency is good
- **But critical causality bug in Conv1D scoping**

**Alignment with Paper:** 60/100
- Most formulas match
- **Key algorithmic requirement (per-chunk Conv1D) violated**

**Recommended Action:** **FIX BEFORE PRODUCTION USE**

---

## Appendices

### Appendix A: Paper Quotes

**Quote 1 - Algorithm 1 Line 2006:**
```
Vi ‚Üê Conv1D_K(X^(i)_0)¬∑Wtarget ‚ñ∑ Compute NTP-aligned target with causal padding.
```

**Quote 2 - Section 3.4 Lines 736-738:**
> "To ensure that the update delta for chunk i itself contains no future information, we apply causal padding to the 1D convolution when generating the value. This isolates each delta calculation to its respective chunk, making the parallel scan mathematically equivalent to a sequential update."

**Quote 3 - Section 3.2 Lines 456-463:**
> "To achieve this, we specify the target v to include future token information. Formally, we derive our target VÃÇ = Conv1D(X‚ÇÄ)¬∑Wtarget, where X‚ÇÄ ‚àà R^{n√ódmodel} denote the token embedding, Conv1D(¬∑) is the 1D Convolution operator and Wtarget ‚àà R^{dmodel√ódmodel} is a trainable projection matrix."

### Appendix B: Related File Locations

**Implementation:**
- Main TTT module: `/home/user/moshi_in_place/moshi/moshi/moshi/modules/ttt_module.py`
- CausalConv1D class: Lines 17-47
- LMAlignedTargetGenerator class: Lines 50-67
- TTTGating._ttt_forward method: Lines 144-156 (bug location)

**Paper:**
- Full paper: `/home/user/moshi_in_place/papers/ttt_in_place_paper.txt`
- Section 3.2 (LM-aligned objective): Lines 444-476
- Algorithm 1: Lines 1988-2029
- Causality discussion: Lines 736-741

---

**END OF VERIFICATION REPORT**

**Agent 2 - LM-Aligned Objective Verification**
**Status: COMPLETE**
**Critical Issues Found: 1 (Conv1D chunking)**
**Recommendation: HIGH PRIORITY FIX REQUIRED**
