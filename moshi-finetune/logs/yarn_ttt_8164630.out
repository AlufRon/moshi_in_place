==================================================
Job started at: Fri 14 Nov 2025 17:25:55 IST
Running on node: ise-6000-01.auth.ad.bgu.ac.il
GPU info:
Fri Nov 14 17:25:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:E1:00.0 Off |                  Off |
| 30%   27C    P8             25W /  300W |       2MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==================================================
Set CUDA_VISIBLE_DEVICES=0
Starting YARN + TTT training...
Context Extension: 4x (3000 -> 12000 tokens)
TTT Layers: 3 (layers 10, 20, 30)
Base Model: Frozen (only TTT params trained)
Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.
2025-11-14 17:26:03 (IST) - 0:00:04 - distributed - INFO - torch.cuda.device_count: 1
2025-11-14 17:26:03 (IST) - 0:00:04 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0
2025-11-14 17:26:03 (IST) - 0:00:04 - distributed - INFO - local rank: 0
2025-11-14 17:26:03 (IST) - 0:00:04 - train - INFO - Going to init comms...
2025-11-14 17:26:03 (IST) - 0:00:04 - train - INFO - Run dir: /sise/eliyanac-group/ron_al/ttt_training_run2
2025-11-14 17:26:03 (IST) - 0:00:04 - train - INFO - Removing run dir /sise/eliyanac-group/ron_al/ttt_training_run2...
2025-11-14 17:26:03 (IST) - 0:00:05 - train - INFO - TrainArgs: {'batch_size': 2,
 'ckpt_freq': 100,
 'data': {'eval_data': '',
          'shuffle': False,
          'train_data': '/sise/eliyanac-group/ron_al/talkbank_callhome_english/talkbank.jsonl'},
 'do_ckpt': True,
 'do_eval': False,
 'duration_sec': 150.0,
 'eval_freq': 100,
 'first_codebook_weight_multiplier': 100.0,
 'full_finetuning': False,
 'gradient_checkpointing': True,
 'log_freq': 1,
 'lora': {'enable': False, 'ft_embed': False, 'rank': 64, 'scaling': 2.0},
 'max_norm': 1.0,
 'max_steps': 2000,
 'moshi_paths': {'config_path': None,
                 'hf_repo_id': 'kyutai/moshiko-pytorch-bf16',
                 'mimi_path': None,
                 'moshi_path': None,
                 'tokenizer_path': None},
 'num_ckpt_keep': 3,
 'num_microbatches': 1,
 'optim': {'lr': 0.0001, 'pct_start': 0.05, 'weight_decay': 0.1},
 'overwrite_run_dir': True,
 'param_dtype': 'bfloat16',
 'run_dir': '/sise/eliyanac-group/ron_al/ttt_training_run2',
 'save_adapters': True,
 'seed': 0,
 'text_padding_weight': 0.5,
 'ttt': {'chunk_size': 256,
         'conv_kernel_size': 2,
         'delta_clip_fro_norm': 100.0,
         'enabled': True,
         'layer_frequency': 10,
         'learning_rate': 0.0001,
         'start_layer': 10,
         'unfreeze_ttt_layers': False},
 'wandb': {'key': '',
           'offline': False,
           'project': 'moshi_in_place',
           'run_name': 'run2'},
 'world_size': 1,
 'yarn': {'beta_fast': 32,
          'beta_slow': 1,
          'enabled': True,
          'mscale': 1.0,
          'mscale_all_dim': 0.0,
          'original_max_seq_len': 3000,
          'scale': 4.0}}
2025-11-14 17:26:03 (IST) - 0:00:05 - metrics_logger - INFO - initializing wandb
2025-11-14 17:26:05 (IST) - 0:00:07 - train - INFO - Loading Mimi and Moshi...
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - TTT (Test-Time Training) ENABLED
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Layer frequency: 10
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Start layer: 10
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Chunk size: 256
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Learning rate: 0.0001
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Conv kernel: 2
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - YaRN (Context Window Extension) ENABLED
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Scale: 4.0x
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Original max seq len: 3000
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Beta fast: 32
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Beta slow: 1
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
[YaRN] Enabled with scale=4.0, original_len=3000
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[YaRN] Initializing RoPE buffers on device=meta
[YaRN] RoPE buffers initialized successfully
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - Initializing TTT w_down from pretrained checkpoint...
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   ✓ transformer.layers.10.gating.w_down <- transformer.layers.10.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   ✓ transformer.layers.20.gating.w_down <- transformer.layers.20.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   ✓ transformer.layers.30.gating.w_down <- transformer.layers.30.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:26:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO - Initializing TTT layers ...
2025-11-14 17:26:08 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:26:08 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:26:08 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.10.gating.w_down_pretrained from w_down
2025-11-14 17:26:08 (IST) - 0:00:09 - finetune.wrapped_model - WARNING - Buffer transformer.layers.10.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:26:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:26:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:26:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.20.gating.w_down_pretrained from w_down
2025-11-14 17:26:09 (IST) - 0:00:10 - finetune.wrapped_model - WARNING - Buffer transformer.layers.20.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.30.gating.w_down_pretrained from w_down
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - WARNING - Buffer transformer.layers.30.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO - Initializing YaRN RoPE buffers ...
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.rope.inv_freq (shape: torch.Size([64]), scale: 4.0x)
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO - Finished initialization!
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO - TTT ACTIVE: 3 layers enabled
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO - TTT layer indices: [10, 20, 30]
2025-11-14 17:26:10 (IST) - 0:00:11 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:26:16 (IST) - 0:00:17 - train - INFO - [DocStream] step=1 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=0-1]
2025-11-14 17:26:16 (IST) - 0:00:17 - train - INFO - [TTT RESET] Document switch detected: None -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav
2025-11-14 17:26:16 (IST) - 0:00:17 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:26:16 (IST) - 0:00:17 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:26:16 (IST) - 0:00:17 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.367003
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.483796
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.794512
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   requires_grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   has grad: True
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - =========================

2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - [TTT] Step 1: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.697e-02, relative_change=0.0373% (9 params)
2025-11-14 17:26:24 (IST) - 0:00:26 - train - INFO - step: 000001 - done (%): 0.1 - loss: 3.378 - lr: 4.0e-06 - peak_alloc_mem (GB): 36.0 - alloc_mem (GB): 21.0 - words_per_second: 2785.2 - avg_words_per_second: 2785.2 - ETA: >2025-11-15 00:10:08
2025-11-14 17:26:25 (IST) - 0:00:26 - train - INFO - [DocStream] step=2 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=2-3]
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.375123
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.494712
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.783928
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   requires_grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   has grad: True
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - =========================

2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - [TTT] Step 2: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.899e-02, relative_change=0.0310% (9 params)
2025-11-14 17:26:30 (IST) - 0:00:32 - train - INFO - step: 000002 - done (%): 0.1 - loss: 3.930 - lr: 4.0e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5568.3 - avg_words_per_second: 3713.1 - ETA: >2025-11-14 22:29:11
2025-11-14 17:26:31 (IST) - 0:00:32 - train - INFO - [DocStream] step=3 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=4-5]
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.405708
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.506723
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.760679
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   requires_grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   has grad: True
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - =========================

2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - [TTT] Step 3: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.553e-02, relative_change=0.0282% (9 params)
2025-11-14 17:26:37 (IST) - 0:00:38 - train - INFO - step: 000003 - done (%): 0.1 - loss: 3.342 - lr: 4.1e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5146.4 - avg_words_per_second: 4093.1 - ETA: >2025-11-14 22:01:03
2025-11-14 17:26:37 (IST) - 0:00:39 - train - INFO - [DocStream] step=4 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=0-1]
2025-11-14 17:26:37 (IST) - 0:00:39 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav
2025-11-14 17:26:37 (IST) - 0:00:39 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:26:37 (IST) - 0:00:39 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:26:37 (IST) - 0:00:39 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.406559
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.474103
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.780984
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   requires_grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   has grad: True
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO -   grad norm: 0.000000
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - =========================

2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - [TTT] Step 4: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.321e-02, relative_change=0.0264% (9 params)
2025-11-14 17:26:43 (IST) - 0:00:44 - train - INFO - step: 000004 - done (%): 0.2 - loss: 4.809 - lr: 4.2e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5499.0 - avg_words_per_second: 4372.6 - ETA: >2025-11-14 21:43:29
2025-11-14 17:26:43 (IST) - 0:00:45 - train - INFO - [DocStream] step=5 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=2-3]
2025-11-14 17:26:49 (IST) - 0:00:51 - train - INFO - [TTT] Step 5: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.228e-02, relative_change=0.0256% (9 params)
2025-11-14 17:26:49 (IST) - 0:00:51 - train - INFO - step: 000005 - done (%): 0.2 - loss: 4.886 - lr: 4.4e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5485.4 - avg_words_per_second: 4557.5 - ETA: >2025-11-14 21:33:03
2025-11-14 17:26:50 (IST) - 0:00:51 - train - INFO - [DocStream] step=6 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=4-5]
2025-11-14 17:26:55 (IST) - 0:00:57 - train - INFO - [TTT] Step 6: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.241e-02, relative_change=0.0257% (9 params)
2025-11-14 17:26:55 (IST) - 0:00:57 - train - INFO - step: 000006 - done (%): 0.3 - loss: 3.287 - lr: 4.6e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5467.0 - avg_words_per_second: 4687.5 - ETA: >2025-11-14 21:26:12
2025-11-14 17:26:56 (IST) - 0:00:57 - train - INFO - [DocStream] step=7 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=6-7]
2025-11-14 17:27:02 (IST) - 0:01:03 - train - INFO - [TTT] Step 7: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.159e-02, relative_change=0.0251% (9 params)
2025-11-14 17:27:02 (IST) - 0:01:03 - train - INFO - step: 000007 - done (%): 0.3 - loss: 2.586 - lr: 4.9e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5461.3 - avg_words_per_second: 4784.3 - ETA: >2025-11-14 21:21:21
2025-11-14 17:27:02 (IST) - 0:01:03 - train - INFO - [DocStream] step=8 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=8-9]
2025-11-14 17:27:08 (IST) - 0:01:09 - train - INFO - [TTT] Step 8: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.188e-02, relative_change=0.0253% (9 params)
2025-11-14 17:27:08 (IST) - 0:01:09 - train - INFO - step: 000008 - done (%): 0.4 - loss: 2.288 - lr: 5.2e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5443.6 - avg_words_per_second: 4857.9 - ETA: >2025-11-14 21:17:47
2025-11-14 17:27:08 (IST) - 0:01:09 - train - INFO - [DocStream] step=9 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=10-11]
2025-11-14 17:27:14 (IST) - 0:01:15 - train - INFO - [TTT] Step 9: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.218e-02, relative_change=0.0255% (9 params)
2025-11-14 17:27:14 (IST) - 0:01:15 - train - INFO - step: 000009 - done (%): 0.5 - loss: 2.285 - lr: 5.5e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5417.9 - avg_words_per_second: 4914.3 - ETA: >2025-11-14 21:15:08
2025-11-14 17:27:14 (IST) - 0:01:16 - train - INFO - [DocStream] step=10 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=0-1]
2025-11-14 17:27:14 (IST) - 0:01:16 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav
2025-11-14 17:27:14 (IST) - 0:01:16 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:27:14 (IST) - 0:01:16 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:27:14 (IST) - 0:01:16 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:27:20 (IST) - 0:01:22 - train - INFO - [TTT] Step 10: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.309e-02, relative_change=0.0263% (9 params)
2025-11-14 17:27:20 (IST) - 0:01:22 - train - INFO - step: 000010 - done (%): 0.5 - loss: 5.038 - lr: 5.9e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5411.8 - avg_words_per_second: 4959.9 - ETA: >2025-11-14 21:13:01
2025-11-14 17:27:21 (IST) - 0:01:22 - train - INFO - [DocStream] step=11 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=2-3]
2025-11-14 17:27:27 (IST) - 0:01:28 - train - INFO - [TTT] Step 11: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.394e-02, relative_change=0.0269% (9 params)
2025-11-14 17:27:27 (IST) - 0:01:28 - train - INFO - step: 000011 - done (%): 0.6 - loss: 4.504 - lr: 6.4e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5388.6 - avg_words_per_second: 4996.0 - ETA: >2025-11-14 21:11:23
2025-11-14 17:27:27 (IST) - 0:01:28 - train - INFO - [DocStream] step=12 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=4-5]
2025-11-14 17:27:33 (IST) - 0:01:34 - train - INFO - [TTT] Step 12: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.510e-02, relative_change=0.0279% (9 params)
2025-11-14 17:27:33 (IST) - 0:01:34 - train - INFO - step: 000012 - done (%): 0.6 - loss: 2.669 - lr: 6.9e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5396.6 - avg_words_per_second: 5027.1 - ETA: >2025-11-14 21:09:59
2025-11-14 17:27:33 (IST) - 0:01:34 - train - INFO - [DocStream] step=13 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=6-7]
2025-11-14 17:27:39 (IST) - 0:01:40 - train - INFO - [TTT] Step 13: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.584e-02, relative_change=0.0285% (9 params)
2025-11-14 17:27:39 (IST) - 0:01:40 - train - INFO - step: 000013 - done (%): 0.7 - loss: 3.057 - lr: 7.4e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5378.4 - avg_words_per_second: 5052.5 - ETA: >2025-11-14 21:08:52
2025-11-14 17:27:39 (IST) - 0:01:41 - train - INFO - [DocStream] step=14 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=8-9]
2025-11-14 17:27:45 (IST) - 0:01:47 - train - INFO - [TTT] Step 14: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.767e-02, relative_change=0.0299% (9 params)
2025-11-14 17:27:45 (IST) - 0:01:47 - train - INFO - step: 000014 - done (%): 0.7 - loss: 2.873 - lr: 8.0e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5367.4 - avg_words_per_second: 5073.8 - ETA: >2025-11-14 21:07:56
2025-11-14 17:27:46 (IST) - 0:01:47 - train - INFO - [DocStream] step=15 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=10-11]
2025-11-14 17:27:52 (IST) - 0:01:53 - train - INFO - [TTT] Step 15: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=4.057e-02, relative_change=0.0322% (9 params)
2025-11-14 17:27:52 (IST) - 0:01:53 - train - INFO - step: 000015 - done (%): 0.8 - loss: 2.740 - lr: 8.7e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5355.7 - avg_words_per_second: 5091.6 - ETA: >2025-11-14 21:07:09
2025-11-14 17:27:52 (IST) - 0:01:53 - train - INFO - [DocStream] step=16 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=0-1]
2025-11-14 17:27:52 (IST) - 0:01:53 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav
2025-11-14 17:27:52 (IST) - 0:01:53 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:27:52 (IST) - 0:01:53 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:27:52 (IST) - 0:01:53 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:27:58 (IST) - 0:01:59 - train - INFO - [TTT] Step 16: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.208e-02, relative_change=0.0334% (9 params)
2025-11-14 17:27:58 (IST) - 0:01:59 - train - INFO - step: 000016 - done (%): 0.8 - loss: 4.238 - lr: 9.3e-06 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5357.5 - avg_words_per_second: 5107.5 - ETA: >2025-11-14 21:06:28
2025-11-14 17:27:58 (IST) - 0:02:00 - train - INFO - [DocStream] step=17 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=2-3]
2025-11-14 17:28:04 (IST) - 0:02:06 - train - INFO - [TTT] Step 17: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.259e-02, relative_change=0.0338% (9 params)
2025-11-14 17:28:04 (IST) - 0:02:06 - train - INFO - step: 000017 - done (%): 0.8 - loss: 3.716 - lr: 1.0e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5344.0 - avg_words_per_second: 5120.8 - ETA: >2025-11-14 21:05:54
2025-11-14 17:28:05 (IST) - 0:02:06 - train - INFO - [DocStream] step=18 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=4-5]
2025-11-14 17:28:11 (IST) - 0:02:12 - train - INFO - [TTT] Step 18: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.323e-02, relative_change=0.0343% (9 params)
2025-11-14 17:28:11 (IST) - 0:02:12 - train - INFO - step: 000018 - done (%): 0.9 - loss: 2.477 - lr: 1.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5347.7 - avg_words_per_second: 5132.9 - ETA: >2025-11-14 21:05:23
2025-11-14 17:28:11 (IST) - 0:02:12 - train - INFO - [DocStream] step=19 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=6-7]
2025-11-14 17:28:17 (IST) - 0:02:18 - train - INFO - [TTT] Step 19: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.479e-02, relative_change=0.0356% (9 params)
2025-11-14 17:28:17 (IST) - 0:02:18 - train - INFO - step: 000019 - done (%): 0.9 - loss: 3.770 - lr: 1.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5339.6 - avg_words_per_second: 5143.4 - ETA: >2025-11-14 21:04:56
2025-11-14 17:28:17 (IST) - 0:02:19 - train - INFO - [DocStream] step=20 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=8-9]
2025-11-14 17:28:23 (IST) - 0:02:25 - train - INFO - [TTT] Step 20: grad_norm=1.000e+00, param_norm=125.9593, delta_norm=4.732e-02, relative_change=0.0376% (9 params)
2025-11-14 17:28:23 (IST) - 0:02:25 - train - INFO - step: 000020 - done (%): 1.0 - loss: 3.360 - lr: 1.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5335.4 - avg_words_per_second: 5152.7 - ETA: >2025-11-14 21:04:32
2025-11-14 17:28:24 (IST) - 0:02:25 - train - INFO - [DocStream] step=21 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=10-11]
2025-11-14 17:28:30 (IST) - 0:02:31 - train - INFO - [TTT] Step 21: grad_norm=1.000e+00, param_norm=125.9593, delta_norm=5.018e-02, relative_change=0.0398% (9 params)
2025-11-14 17:28:30 (IST) - 0:02:31 - train - INFO - step: 000021 - done (%): 1.1 - loss: 1.915 - lr: 1.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5324.5 - avg_words_per_second: 5160.6 - ETA: >2025-11-14 21:04:12
2025-11-14 17:28:30 (IST) - 0:02:31 - train - INFO - [DocStream] step=22 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=0-1]
2025-11-14 17:28:30 (IST) - 0:02:31 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav
2025-11-14 17:28:30 (IST) - 0:02:31 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:28:30 (IST) - 0:02:31 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:28:30 (IST) - 0:02:31 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:28:36 (IST) - 0:02:37 - train - INFO - [TTT] Step 22: grad_norm=1.000e+00, param_norm=125.9594, delta_norm=5.325e-02, relative_change=0.0423% (9 params)
2025-11-14 17:28:36 (IST) - 0:02:37 - train - INFO - step: 000022 - done (%): 1.1 - loss: 3.864 - lr: 1.4e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5321.2 - avg_words_per_second: 5167.7 - ETA: >2025-11-14 21:03:54
2025-11-14 17:28:36 (IST) - 0:02:38 - train - INFO - [DocStream] step=23 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=2-3]
2025-11-14 17:28:42 (IST) - 0:02:44 - train - INFO - [TTT] Step 23: grad_norm=1.000e+00, param_norm=125.9594, delta_norm=5.358e-02, relative_change=0.0425% (9 params)
2025-11-14 17:28:42 (IST) - 0:02:44 - train - INFO - step: 000023 - done (%): 1.1 - loss: 3.616 - lr: 1.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5324.9 - avg_words_per_second: 5174.3 - ETA: >2025-11-14 21:03:38
2025-11-14 17:28:43 (IST) - 0:02:44 - train - INFO - [DocStream] step=24 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=4-5]
2025-11-14 17:28:49 (IST) - 0:02:50 - train - INFO - [TTT] Step 24: grad_norm=1.000e+00, param_norm=125.9595, delta_norm=5.645e-02, relative_change=0.0448% (9 params)
2025-11-14 17:28:49 (IST) - 0:02:50 - train - INFO - step: 000024 - done (%): 1.2 - loss: 2.441 - lr: 1.6e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5326.5 - avg_words_per_second: 5180.5 - ETA: >2025-11-14 21:03:22
2025-11-14 17:28:49 (IST) - 0:02:50 - train - INFO - [DocStream] step=25 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=6-7]
2025-11-14 17:28:55 (IST) - 0:02:56 - train - INFO - [TTT] Step 25: grad_norm=1.000e+00, param_norm=125.9596, delta_norm=5.940e-02, relative_change=0.0472% (9 params)
2025-11-14 17:28:55 (IST) - 0:02:56 - train - INFO - step: 000025 - done (%): 1.2 - loss: 2.450 - lr: 1.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5324.6 - avg_words_per_second: 5186.1 - ETA: >2025-11-14 21:03:08
2025-11-14 17:28:55 (IST) - 0:02:57 - train - INFO - [DocStream] step=26 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=8-9]
2025-11-14 17:29:01 (IST) - 0:03:03 - train - INFO - [TTT] Step 26: grad_norm=8.320e-01, param_norm=125.9597, delta_norm=6.347e-02, relative_change=0.0504% (9 params)
2025-11-14 17:29:01 (IST) - 0:03:03 - train - INFO - step: 000026 - done (%): 1.3 - loss: 1.704 - lr: 1.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5330.0 - avg_words_per_second: 5191.5 - ETA: >2025-11-14 21:02:54
2025-11-14 17:29:02 (IST) - 0:03:03 - train - INFO - [DocStream] step=27 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=10-11]
2025-11-14 17:29:08 (IST) - 0:03:09 - train - INFO - [TTT] Step 27: grad_norm=8.607e-01, param_norm=125.9598, delta_norm=6.601e-02, relative_change=0.0524% (9 params)
2025-11-14 17:29:08 (IST) - 0:03:09 - train - INFO - step: 000027 - done (%): 1.4 - loss: 2.323 - lr: 1.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5326.2 - avg_words_per_second: 5196.4 - ETA: >2025-11-14 21:02:42
2025-11-14 17:29:08 (IST) - 0:03:09 - train - INFO - [DocStream] step=28 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=0-1]
2025-11-14 17:29:08 (IST) - 0:03:09 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav
2025-11-14 17:29:08 (IST) - 0:03:09 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:29:08 (IST) - 0:03:09 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:29:08 (IST) - 0:03:09 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:29:14 (IST) - 0:03:15 - train - INFO - [TTT] Step 28: grad_norm=1.000e+00, param_norm=125.9599, delta_norm=6.861e-02, relative_change=0.0545% (9 params)
2025-11-14 17:29:14 (IST) - 0:03:15 - train - INFO - step: 000028 - done (%): 1.4 - loss: 3.956 - lr: 2.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5322.2 - avg_words_per_second: 5200.8 - ETA: >2025-11-14 21:02:31
2025-11-14 17:29:14 (IST) - 0:03:16 - train - INFO - [DocStream] step=29 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=2-3]
2025-11-14 17:29:20 (IST) - 0:03:22 - train - INFO - [TTT] Step 29: grad_norm=1.000e+00, param_norm=125.9601, delta_norm=7.073e-02, relative_change=0.0562% (9 params)
2025-11-14 17:29:20 (IST) - 0:03:22 - train - INFO - step: 000029 - done (%): 1.4 - loss: 3.752 - lr: 2.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5316.2 - avg_words_per_second: 5204.7 - ETA: >2025-11-14 21:02:21
2025-11-14 17:29:21 (IST) - 0:03:22 - train - INFO - [DocStream] step=30 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=4-5]
2025-11-14 17:29:27 (IST) - 0:03:28 - train - INFO - [TTT] Step 30: grad_norm=1.000e+00, param_norm=125.9603, delta_norm=7.270e-02, relative_change=0.0577% (9 params)
2025-11-14 17:29:27 (IST) - 0:03:28 - train - INFO - step: 000030 - done (%): 1.5 - loss: 1.847 - lr: 2.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5325.1 - avg_words_per_second: 5208.6 - ETA: >2025-11-14 21:02:12
2025-11-14 17:29:27 (IST) - 0:03:28 - train - INFO - [DocStream] step=31 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=6-7]
2025-11-14 17:29:33 (IST) - 0:03:34 - train - INFO - [TTT] Step 31: grad_norm=8.763e-01, param_norm=125.9605, delta_norm=7.606e-02, relative_change=0.0604% (9 params)
2025-11-14 17:29:33 (IST) - 0:03:34 - train - INFO - step: 000031 - done (%): 1.6 - loss: 2.134 - lr: 2.4e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5326.1 - avg_words_per_second: 5212.3 - ETA: >2025-11-14 21:02:02
2025-11-14 17:29:33 (IST) - 0:03:35 - train - INFO - [DocStream] step=32 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=8-9]
2025-11-14 17:29:39 (IST) - 0:03:41 - train - INFO - [TTT] Step 32: grad_norm=1.000e+00, param_norm=125.9608, delta_norm=8.068e-02, relative_change=0.0641% (9 params)
2025-11-14 17:29:39 (IST) - 0:03:41 - train - INFO - step: 000032 - done (%): 1.6 - loss: 1.938 - lr: 2.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5326.3 - avg_words_per_second: 5215.8 - ETA: >2025-11-14 21:01:54
2025-11-14 17:29:40 (IST) - 0:03:41 - train - INFO - [DocStream] step=33 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=10-11]
2025-11-14 17:29:46 (IST) - 0:03:47 - train - INFO - [TTT] Step 33: grad_norm=8.174e-01, param_norm=125.9610, delta_norm=8.744e-02, relative_change=0.0694% (9 params)
2025-11-14 17:29:46 (IST) - 0:03:47 - train - INFO - step: 000033 - done (%): 1.6 - loss: 1.776 - lr: 2.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5322.3 - avg_words_per_second: 5218.9 - ETA: >2025-11-14 21:01:46
2025-11-14 17:29:46 (IST) - 0:03:47 - train - INFO - [DocStream] step=34 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=0-1]
2025-11-14 17:29:46 (IST) - 0:03:47 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav
2025-11-14 17:29:46 (IST) - 0:03:47 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:29:46 (IST) - 0:03:47 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:29:46 (IST) - 0:03:47 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:29:52 (IST) - 0:03:53 - train - INFO - [TTT] Step 34: grad_norm=1.000e+00, param_norm=125.9614, delta_norm=9.109e-02, relative_change=0.0723% (9 params)
2025-11-14 17:29:52 (IST) - 0:03:53 - train - INFO - step: 000034 - done (%): 1.7 - loss: 4.182 - lr: 2.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5312.8 - avg_words_per_second: 5221.7 - ETA: >2025-11-14 21:01:39
2025-11-14 17:29:52 (IST) - 0:03:54 - train - INFO - [DocStream] step=35 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=2-3]
2025-11-14 17:29:58 (IST) - 0:04:00 - train - INFO - [TTT] Step 35: grad_norm=1.000e+00, param_norm=125.9618, delta_norm=9.310e-02, relative_change=0.0739% (9 params)
2025-11-14 17:29:58 (IST) - 0:04:00 - train - INFO - step: 000035 - done (%): 1.8 - loss: 3.215 - lr: 2.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5323.9 - avg_words_per_second: 5224.5 - ETA: >2025-11-14 21:01:32
2025-11-14 17:29:59 (IST) - 0:04:00 - train - INFO - [DocStream] step=36 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=4-5]
2025-11-14 17:30:05 (IST) - 0:04:06 - train - INFO - [TTT] Step 36: grad_norm=1.000e+00, param_norm=125.9621, delta_norm=9.724e-02, relative_change=0.0772% (9 params)
2025-11-14 17:30:05 (IST) - 0:04:06 - train - INFO - step: 000036 - done (%): 1.8 - loss: 2.732 - lr: 3.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5320.9 - avg_words_per_second: 5227.2 - ETA: >2025-11-14 21:01:26
2025-11-14 17:30:05 (IST) - 0:04:06 - train - INFO - [DocStream] step=37 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=6-7]
2025-11-14 17:30:11 (IST) - 0:04:12 - train - INFO - [TTT] Step 37: grad_norm=9.290e-01, param_norm=125.9626, delta_norm=1.025e-01, relative_change=0.0814% (9 params)
2025-11-14 17:30:11 (IST) - 0:04:12 - train - INFO - step: 000037 - done (%): 1.9 - loss: 2.470 - lr: 3.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5321.8 - avg_words_per_second: 5229.7 - ETA: >2025-11-14 21:01:19
2025-11-14 17:30:11 (IST) - 0:04:13 - train - INFO - [DocStream] step=38 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=8-9]
2025-11-14 17:30:17 (IST) - 0:04:19 - train - INFO - [TTT] Step 38: grad_norm=1.000e+00, param_norm=125.9630, delta_norm=1.071e-01, relative_change=0.0850% (9 params)
2025-11-14 17:30:17 (IST) - 0:04:19 - train - INFO - step: 000038 - done (%): 1.9 - loss: 2.235 - lr: 3.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5327.7 - avg_words_per_second: 5232.2 - ETA: >2025-11-14 21:01:13
2025-11-14 17:30:18 (IST) - 0:04:19 - train - INFO - [DocStream] step=39 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=10-11]
2025-11-14 17:30:24 (IST) - 0:04:25 - train - INFO - [TTT] Step 39: grad_norm=8.326e-01, param_norm=125.9635, delta_norm=1.132e-01, relative_change=0.0899% (9 params)
2025-11-14 17:30:24 (IST) - 0:04:25 - train - INFO - step: 000039 - done (%): 1.9 - loss: 1.863 - lr: 3.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5325.3 - avg_words_per_second: 5234.5 - ETA: >2025-11-14 21:01:07
2025-11-14 17:30:24 (IST) - 0:04:25 - train - INFO - [DocStream] step=40 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=0-1]
2025-11-14 17:30:24 (IST) - 0:04:25 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav
2025-11-14 17:30:24 (IST) - 0:04:25 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:30:24 (IST) - 0:04:25 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:30:24 (IST) - 0:04:25 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:30:30 (IST) - 0:04:31 - train - INFO - [TTT] Step 40: grad_norm=1.000e+00, param_norm=125.9643, delta_norm=1.168e-01, relative_change=0.0927% (9 params)
2025-11-14 17:30:30 (IST) - 0:04:31 - train - INFO - step: 000040 - done (%): 2.0 - loss: 4.980 - lr: 3.6e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5310.6 - avg_words_per_second: 5236.4 - ETA: >2025-11-14 21:01:03
2025-11-14 17:30:30 (IST) - 0:04:32 - train - INFO - [DocStream] step=41 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=2-3]
2025-11-14 17:30:37 (IST) - 0:04:38 - train - INFO - [TTT] Step 41: grad_norm=1.000e+00, param_norm=125.9650, delta_norm=1.150e-01, relative_change=0.0913% (9 params)
2025-11-14 17:30:37 (IST) - 0:04:38 - train - INFO - step: 000041 - done (%): 2.0 - loss: 3.347 - lr: 3.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5313.1 - avg_words_per_second: 5238.3 - ETA: >2025-11-14 21:00:58
2025-11-14 17:30:37 (IST) - 0:04:38 - train - INFO - [DocStream] step=42 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=4-5]
2025-11-14 17:30:43 (IST) - 0:04:44 - train - INFO - [TTT] Step 42: grad_norm=1.000e+00, param_norm=125.9657, delta_norm=1.158e-01, relative_change=0.0920% (9 params)
2025-11-14 17:30:43 (IST) - 0:04:44 - train - INFO - step: 000042 - done (%): 2.1 - loss: 3.458 - lr: 3.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5323.7 - avg_words_per_second: 5240.3 - ETA: >2025-11-14 21:00:53
2025-11-14 17:30:43 (IST) - 0:04:44 - train - INFO - [DocStream] step=43 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=6-7]
2025-11-14 17:30:49 (IST) - 0:04:50 - train - INFO - [TTT] Step 43: grad_norm=1.000e+00, param_norm=125.9665, delta_norm=1.159e-01, relative_change=0.0920% (9 params)
2025-11-14 17:30:49 (IST) - 0:04:50 - train - INFO - step: 000043 - done (%): 2.1 - loss: 1.701 - lr: 4.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5328.4 - avg_words_per_second: 5242.3 - ETA: >2025-11-14 21:00:48
2025-11-14 17:30:50 (IST) - 0:04:51 - train - INFO - [DocStream] step=44 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=8-9]
2025-11-14 17:30:56 (IST) - 0:04:57 - train - INFO - [TTT] Step 44: grad_norm=1.000e+00, param_norm=125.9673, delta_norm=1.211e-01, relative_change=0.0962% (9 params)
2025-11-14 17:30:56 (IST) - 0:04:57 - train - INFO - step: 000044 - done (%): 2.2 - loss: 2.411 - lr: 4.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5312.2 - avg_words_per_second: 5243.9 - ETA: >2025-11-14 21:00:45
2025-11-14 17:30:56 (IST) - 0:04:57 - train - INFO - [DocStream] step=45 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=10-11]
2025-11-14 17:31:02 (IST) - 0:05:03 - train - INFO - [TTT] Step 45: grad_norm=8.264e-01, param_norm=125.9681, delta_norm=1.291e-01, relative_change=0.1025% (9 params)
2025-11-14 17:31:02 (IST) - 0:05:03 - train - INFO - step: 000045 - done (%): 2.2 - loss: 2.057 - lr: 4.4e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5310.8 - avg_words_per_second: 5245.3 - ETA: >2025-11-14 21:00:41
2025-11-14 17:31:02 (IST) - 0:05:04 - train - INFO - [DocStream] step=46 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=0-1]
2025-11-14 17:31:02 (IST) - 0:05:04 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4156.wav
2025-11-14 17:31:02 (IST) - 0:05:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:02 (IST) - 0:05:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:02 (IST) - 0:05:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:08 (IST) - 0:05:10 - train - INFO - [TTT] Step 46: grad_norm=1.000e+00, param_norm=125.9691, delta_norm=1.349e-01, relative_change=0.1071% (9 params)
2025-11-14 17:31:08 (IST) - 0:05:10 - train - INFO - step: 000046 - done (%): 2.3 - loss: 4.403 - lr: 4.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5256.1 - avg_words_per_second: 5245.6 - ETA: >2025-11-14 21:00:40
2025-11-14 17:31:09 (IST) - 0:05:10 - train - INFO - [DocStream] step=47 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=2-3]
2025-11-14 17:31:15 (IST) - 0:05:16 - train - INFO - [TTT] Step 47: grad_norm=1.000e+00, param_norm=125.9702, delta_norm=1.343e-01, relative_change=0.1066% (9 params)
2025-11-14 17:31:15 (IST) - 0:05:16 - train - INFO - step: 000047 - done (%): 2.4 - loss: 4.223 - lr: 4.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.6 - avg_words_per_second: 5246.9 - ETA: >2025-11-14 21:00:37
2025-11-14 17:31:15 (IST) - 0:05:16 - train - INFO - [DocStream] step=48 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=4-5]
2025-11-14 17:31:21 (IST) - 0:05:22 - train - INFO - [TTT] Step 48: grad_norm=1.000e+00, param_norm=125.9712, delta_norm=1.397e-01, relative_change=0.1109% (9 params)
2025-11-14 17:31:21 (IST) - 0:05:22 - train - INFO - step: 000048 - done (%): 2.4 - loss: 4.525 - lr: 4.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5308.6 - avg_words_per_second: 5248.2 - ETA: >2025-11-14 21:00:34
2025-11-14 17:31:21 (IST) - 0:05:23 - train - INFO - [DocStream] step=49 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=6-7]
2025-11-14 17:31:27 (IST) - 0:05:29 - train - INFO - [TTT] Step 49: grad_norm=1.000e+00, param_norm=125.9722, delta_norm=1.514e-01, relative_change=0.1202% (9 params)
2025-11-14 17:31:27 (IST) - 0:05:29 - train - INFO - step: 000049 - done (%): 2.5 - loss: 4.641 - lr: 5.0e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5307.7 - avg_words_per_second: 5249.4 - ETA: >2025-11-14 21:00:31
2025-11-14 17:31:28 (IST) - 0:05:29 - train - INFO - [DocStream] step=50 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=8-9]
2025-11-14 17:31:34 (IST) - 0:05:35 - train - INFO - [TTT] Step 50: grad_norm=1.000e+00, param_norm=125.9733, delta_norm=1.563e-01, relative_change=0.1241% (9 params)
2025-11-14 17:31:34 (IST) - 0:05:35 - train - INFO - step: 000050 - done (%): 2.5 - loss: 4.066 - lr: 5.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5396.5 - avg_words_per_second: 5252.3 - ETA: >2025-11-14 21:00:24
2025-11-14 17:31:34 (IST) - 0:05:35 - train - INFO - [DocStream] step=51 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=0-1]
2025-11-14 17:31:34 (IST) - 0:05:35 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4156.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4157.wav
2025-11-14 17:31:34 (IST) - 0:05:35 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:34 (IST) - 0:05:35 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:34 (IST) - 0:05:35 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:40 (IST) - 0:05:41 - train - INFO - [TTT] Step 51: grad_norm=1.000e+00, param_norm=125.9748, delta_norm=1.571e-01, relative_change=0.1247% (9 params)
2025-11-14 17:31:40 (IST) - 0:05:41 - train - INFO - step: 000051 - done (%): 2.5 - loss: 4.901 - lr: 5.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5322.8 - avg_words_per_second: 5253.6 - ETA: >2025-11-14 21:00:21
2025-11-14 17:31:40 (IST) - 0:05:42 - train - INFO - [DocStream] step=52 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=2-3]
2025-11-14 17:31:46 (IST) - 0:05:48 - train - INFO - [TTT] Step 52: grad_norm=1.000e+00, param_norm=125.9762, delta_norm=1.561e-01, relative_change=0.1239% (9 params)
2025-11-14 17:31:46 (IST) - 0:05:48 - train - INFO - step: 000052 - done (%): 2.6 - loss: 3.884 - lr: 5.4e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5310.9 - avg_words_per_second: 5254.7 - ETA: >2025-11-14 21:00:18
2025-11-14 17:31:47 (IST) - 0:05:48 - train - INFO - [DocStream] step=53 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=4-5]
2025-11-14 17:31:53 (IST) - 0:05:54 - train - INFO - [TTT] Step 53: grad_norm=1.000e+00, param_norm=125.9776, delta_norm=1.581e-01, relative_change=0.1255% (9 params)
2025-11-14 17:31:53 (IST) - 0:05:54 - train - INFO - step: 000053 - done (%): 2.6 - loss: 3.667 - lr: 5.6e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5426.4 - avg_words_per_second: 5257.9 - ETA: >2025-11-14 21:00:10
2025-11-14 17:31:53 (IST) - 0:05:54 - train - INFO - [DocStream] step=54 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=0-1]
2025-11-14 17:31:53 (IST) - 0:05:54 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4157.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4170.wav
2025-11-14 17:31:53 (IST) - 0:05:54 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:53 (IST) - 0:05:54 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:53 (IST) - 0:05:54 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:31:59 (IST) - 0:06:00 - train - INFO - [TTT] Step 54: grad_norm=1.000e+00, param_norm=125.9794, delta_norm=1.707e-01, relative_change=0.1355% (9 params)
2025-11-14 17:31:59 (IST) - 0:06:00 - train - INFO - step: 000054 - done (%): 2.7 - loss: 4.467 - lr: 5.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5316.7 - avg_words_per_second: 5258.9 - ETA: >2025-11-14 21:00:08
2025-11-14 17:31:59 (IST) - 0:06:01 - train - INFO - [DocStream] step=55 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=2-3]
2025-11-14 17:32:05 (IST) - 0:06:07 - train - INFO - [TTT] Step 55: grad_norm=1.000e+00, param_norm=125.9812, delta_norm=1.707e-01, relative_change=0.1355% (9 params)
2025-11-14 17:32:05 (IST) - 0:06:07 - train - INFO - step: 000055 - done (%): 2.8 - loss: 4.343 - lr: 5.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.7 - avg_words_per_second: 5259.9 - ETA: >2025-11-14 21:00:05
2025-11-14 17:32:06 (IST) - 0:06:07 - train - INFO - [DocStream] step=56 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=4-5]
2025-11-14 17:32:12 (IST) - 0:06:13 - train - INFO - [TTT] Step 56: grad_norm=1.000e+00, param_norm=125.9829, delta_norm=1.815e-01, relative_change=0.1441% (9 params)
2025-11-14 17:32:12 (IST) - 0:06:13 - train - INFO - step: 000056 - done (%): 2.8 - loss: 4.910 - lr: 6.0e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5323.0 - avg_words_per_second: 5261.0 - ETA: >2025-11-14 21:00:03
2025-11-14 17:32:12 (IST) - 0:06:13 - train - INFO - [DocStream] step=57 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=6-7]
2025-11-14 17:32:18 (IST) - 0:06:19 - train - INFO - [TTT] Step 57: grad_norm=1.000e+00, param_norm=125.9846, delta_norm=1.976e-01, relative_change=0.1569% (9 params)
2025-11-14 17:32:18 (IST) - 0:06:19 - train - INFO - step: 000057 - done (%): 2.9 - loss: 4.505 - lr: 6.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.6 - avg_words_per_second: 5261.9 - ETA: >2025-11-14 21:00:00
2025-11-14 17:32:18 (IST) - 0:06:20 - train - INFO - [DocStream] step=58 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=8-9]
2025-11-14 17:32:24 (IST) - 0:06:26 - train - INFO - [TTT] Step 58: grad_norm=1.000e+00, param_norm=125.9864, delta_norm=2.125e-01, relative_change=0.1687% (9 params)
2025-11-14 17:32:24 (IST) - 0:06:26 - train - INFO - step: 000058 - done (%): 2.9 - loss: 4.187 - lr: 6.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5307.7 - avg_words_per_second: 5262.7 - ETA: >2025-11-14 20:59:59
2025-11-14 17:32:25 (IST) - 0:06:26 - train - INFO - [DocStream] step=59 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=10-11]
2025-11-14 17:32:31 (IST) - 0:06:32 - train - INFO - [TTT] Step 59: grad_norm=1.000e+00, param_norm=125.9881, delta_norm=2.138e-01, relative_change=0.1697% (9 params)
2025-11-14 17:32:31 (IST) - 0:06:32 - train - INFO - step: 000059 - done (%): 3.0 - loss: 4.291 - lr: 6.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5303.9 - avg_words_per_second: 5263.4 - ETA: >2025-11-14 20:59:57
2025-11-14 17:32:31 (IST) - 0:06:32 - train - INFO - [DocStream] step=60 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=0-1]
2025-11-14 17:32:31 (IST) - 0:06:32 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4170.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4183.wav
2025-11-14 17:32:31 (IST) - 0:06:32 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:32:31 (IST) - 0:06:32 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:32:31 (IST) - 0:06:32 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:32:37 (IST) - 0:06:38 - train - INFO - [TTT] Step 60: grad_norm=1.000e+00, param_norm=125.9904, delta_norm=2.137e-01, relative_change=0.1697% (9 params)
2025-11-14 17:32:37 (IST) - 0:06:38 - train - INFO - step: 000060 - done (%): 3.0 - loss: 4.428 - lr: 6.6e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5303.0 - avg_words_per_second: 5264.0 - ETA: >2025-11-14 20:59:55
2025-11-14 17:32:37 (IST) - 0:06:39 - train - INFO - [DocStream] step=61 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=2-3]
2025-11-14 17:32:43 (IST) - 0:06:45 - train - INFO - [TTT] Step 61: grad_norm=1.000e+00, param_norm=125.9926, delta_norm=2.003e-01, relative_change=0.1590% (9 params)
2025-11-14 17:32:43 (IST) - 0:06:45 - train - INFO - step: 000061 - done (%): 3.0 - loss: 3.722 - lr: 6.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5309.5 - avg_words_per_second: 5264.8 - ETA: >2025-11-14 20:59:53
2025-11-14 17:32:44 (IST) - 0:06:45 - train - INFO - [DocStream] step=62 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=4-5]
2025-11-14 17:32:50 (IST) - 0:06:51 - train - INFO - [TTT] Step 62: grad_norm=1.000e+00, param_norm=125.9948, delta_norm=2.055e-01, relative_change=0.1631% (9 params)
2025-11-14 17:32:50 (IST) - 0:06:51 - train - INFO - step: 000062 - done (%): 3.1 - loss: 3.608 - lr: 6.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5307.5 - avg_words_per_second: 5265.4 - ETA: >2025-11-14 20:59:52
2025-11-14 17:32:50 (IST) - 0:06:51 - train - INFO - [DocStream] step=63 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=6-7]
2025-11-14 17:32:56 (IST) - 0:06:57 - train - INFO - [TTT] Step 63: grad_norm=1.000e+00, param_norm=125.9969, delta_norm=2.196e-01, relative_change=0.1743% (9 params)
2025-11-14 17:32:56 (IST) - 0:06:57 - train - INFO - step: 000063 - done (%): 3.1 - loss: 3.636 - lr: 7.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.3 - avg_words_per_second: 5266.2 - ETA: >2025-11-14 20:59:50
2025-11-14 17:32:56 (IST) - 0:06:58 - train - INFO - [DocStream] step=64 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=8-9]
2025-11-14 17:33:03 (IST) - 0:07:04 - train - INFO - [TTT] Step 64: grad_norm=1.000e+00, param_norm=125.9991, delta_norm=2.416e-01, relative_change=0.1918% (9 params)
2025-11-14 17:33:03 (IST) - 0:07:04 - train - INFO - step: 000064 - done (%): 3.2 - loss: 3.403 - lr: 7.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5312.2 - avg_words_per_second: 5266.9 - ETA: >2025-11-14 20:59:48
2025-11-14 17:33:03 (IST) - 0:07:04 - train - INFO - [DocStream] step=65 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=10-11]
2025-11-14 17:33:09 (IST) - 0:07:10 - train - INFO - [TTT] Step 65: grad_norm=1.000e+00, param_norm=126.0013, delta_norm=2.456e-01, relative_change=0.1949% (9 params)
2025-11-14 17:33:09 (IST) - 0:07:10 - train - INFO - step: 000065 - done (%): 3.2 - loss: 3.664 - lr: 7.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5274.0 - avg_words_per_second: 5267.0 - ETA: >2025-11-14 20:59:48
2025-11-14 17:33:09 (IST) - 0:07:11 - train - INFO - [DocStream] step=66 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=0-1]
2025-11-14 17:33:09 (IST) - 0:07:11 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4183.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4184.wav
2025-11-14 17:33:09 (IST) - 0:07:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:33:09 (IST) - 0:07:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:33:09 (IST) - 0:07:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:33:15 (IST) - 0:07:17 - train - INFO - [TTT] Step 66: grad_norm=1.000e+00, param_norm=126.0047, delta_norm=2.426e-01, relative_change=0.1925% (9 params)
2025-11-14 17:33:15 (IST) - 0:07:17 - train - INFO - step: 000066 - done (%): 3.3 - loss: 4.736 - lr: 7.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5301.9 - avg_words_per_second: 5267.5 - ETA: >2025-11-14 20:59:47
2025-11-14 17:33:16 (IST) - 0:07:17 - train - INFO - [DocStream] step=67 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=2-3]
2025-11-14 17:33:22 (IST) - 0:07:23 - train - INFO - [TTT] Step 67: grad_norm=1.000e+00, param_norm=126.0079, delta_norm=2.427e-01, relative_change=0.1926% (9 params)
2025-11-14 17:33:22 (IST) - 0:07:23 - train - INFO - step: 000067 - done (%): 3.4 - loss: 4.215 - lr: 7.6e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5306.1 - avg_words_per_second: 5268.1 - ETA: >2025-11-14 20:59:45
2025-11-14 17:33:22 (IST) - 0:07:23 - train - INFO - [DocStream] step=68 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=4-5]
2025-11-14 17:33:28 (IST) - 0:07:29 - train - INFO - [TTT] Step 68: grad_norm=1.000e+00, param_norm=126.0109, delta_norm=2.545e-01, relative_change=0.2020% (9 params)
2025-11-14 17:33:28 (IST) - 0:07:29 - train - INFO - step: 000068 - done (%): 3.4 - loss: 2.904 - lr: 7.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5308.8 - avg_words_per_second: 5268.7 - ETA: >2025-11-14 20:59:44
2025-11-14 17:33:28 (IST) - 0:07:30 - train - INFO - [DocStream] step=69 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=6-7]
2025-11-14 17:33:34 (IST) - 0:07:36 - train - INFO - [TTT] Step 69: grad_norm=9.683e-01, param_norm=126.0139, delta_norm=2.750e-01, relative_change=0.2183% (9 params)
2025-11-14 17:33:34 (IST) - 0:07:36 - train - INFO - step: 000069 - done (%): 3.5 - loss: 2.623 - lr: 7.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5307.6 - avg_words_per_second: 5269.2 - ETA: >2025-11-14 20:59:43
2025-11-14 17:33:35 (IST) - 0:07:36 - train - INFO - [DocStream] step=70 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=8-9]
2025-11-14 17:33:41 (IST) - 0:07:42 - train - INFO - [TTT] Step 70: grad_norm=1.000e+00, param_norm=126.0168, delta_norm=2.492e-01, relative_change=0.1978% (9 params)
2025-11-14 17:33:41 (IST) - 0:07:42 - train - INFO - step: 000070 - done (%): 3.5 - loss: 2.719 - lr: 8.0e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5291.3 - avg_words_per_second: 5269.6 - ETA: >2025-11-14 20:59:42
2025-11-14 17:33:41 (IST) - 0:07:42 - train - INFO - [DocStream] step=71 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=10-11]
2025-11-14 17:33:47 (IST) - 0:07:48 - train - INFO - [TTT] Step 71: grad_norm=1.000e+00, param_norm=126.0196, delta_norm=2.320e-01, relative_change=0.1841% (9 params)
2025-11-14 17:33:47 (IST) - 0:07:48 - train - INFO - step: 000071 - done (%): 3.5 - loss: 2.016 - lr: 8.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5309.8 - avg_words_per_second: 5270.1 - ETA: >2025-11-14 20:59:41
2025-11-14 17:33:47 (IST) - 0:07:49 - train - INFO - [DocStream] step=72 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=0-1]
2025-11-14 17:33:47 (IST) - 0:07:49 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4184.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4234.wav
2025-11-14 17:33:47 (IST) - 0:07:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:33:47 (IST) - 0:07:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:33:47 (IST) - 0:07:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:33:53 (IST) - 0:07:55 - train - INFO - [TTT] Step 72: grad_norm=1.000e+00, param_norm=126.0235, delta_norm=2.430e-01, relative_change=0.1928% (9 params)
2025-11-14 17:33:53 (IST) - 0:07:55 - train - INFO - step: 000072 - done (%): 3.6 - loss: 4.724 - lr: 8.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5307.9 - avg_words_per_second: 5270.6 - ETA: >2025-11-14 20:59:39
2025-11-14 17:33:54 (IST) - 0:07:55 - train - INFO - [DocStream] step=73 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=2-3]
2025-11-14 17:34:00 (IST) - 0:08:01 - train - INFO - [TTT] Step 73: grad_norm=1.000e+00, param_norm=126.0270, delta_norm=2.398e-01, relative_change=0.1902% (9 params)
2025-11-14 17:34:00 (IST) - 0:08:01 - train - INFO - step: 000073 - done (%): 3.6 - loss: 5.029 - lr: 8.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5310.1 - avg_words_per_second: 5271.2 - ETA: >2025-11-14 20:59:38
2025-11-14 17:34:00 (IST) - 0:08:01 - train - INFO - [DocStream] step=74 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=4-5]
2025-11-14 17:34:06 (IST) - 0:08:07 - train - INFO - [TTT] Step 74: grad_norm=1.000e+00, param_norm=126.0303, delta_norm=2.574e-01, relative_change=0.2043% (9 params)
2025-11-14 17:34:06 (IST) - 0:08:07 - train - INFO - step: 000074 - done (%): 3.7 - loss: 3.570 - lr: 8.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5309.3 - avg_words_per_second: 5271.7 - ETA: >2025-11-14 20:59:37
2025-11-14 17:34:07 (IST) - 0:08:08 - train - INFO - [DocStream] step=75 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=6-7]
2025-11-14 17:34:13 (IST) - 0:08:14 - train - INFO - [TTT] Step 75: grad_norm=1.000e+00, param_norm=126.0334, delta_norm=2.417e-01, relative_change=0.1918% (9 params)
2025-11-14 17:34:13 (IST) - 0:08:14 - train - INFO - step: 000075 - done (%): 3.8 - loss: 2.323 - lr: 8.6e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5313.1 - avg_words_per_second: 5272.2 - ETA: >2025-11-14 20:59:35
2025-11-14 17:34:13 (IST) - 0:08:14 - train - INFO - [DocStream] step=76 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=8-9]
2025-11-14 17:34:19 (IST) - 0:08:20 - train - INFO - [TTT] Step 76: grad_norm=1.000e+00, param_norm=126.0363, delta_norm=2.520e-01, relative_change=0.2000% (9 params)
2025-11-14 17:34:19 (IST) - 0:08:20 - train - INFO - step: 000076 - done (%): 3.8 - loss: 2.769 - lr: 8.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5308.9 - avg_words_per_second: 5272.7 - ETA: >2025-11-14 20:59:34
2025-11-14 17:34:19 (IST) - 0:08:21 - train - INFO - [DocStream] step=77 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=10-11]
2025-11-14 17:34:25 (IST) - 0:08:27 - train - INFO - [TTT] Step 77: grad_norm=1.000e+00, param_norm=126.0391, delta_norm=2.783e-01, relative_change=0.2208% (9 params)
2025-11-14 17:34:25 (IST) - 0:08:27 - train - INFO - step: 000077 - done (%): 3.9 - loss: 2.534 - lr: 8.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5324.2 - avg_words_per_second: 5273.4 - ETA: >2025-11-14 20:59:33
2025-11-14 17:34:26 (IST) - 0:08:27 - train - INFO - [DocStream] step=78 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=0-1]
2025-11-14 17:34:26 (IST) - 0:08:27 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4234.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4245.wav
2025-11-14 17:34:26 (IST) - 0:08:27 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:34:26 (IST) - 0:08:27 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:34:26 (IST) - 0:08:27 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:34:32 (IST) - 0:08:33 - train - INFO - [TTT] Step 78: grad_norm=1.000e+00, param_norm=126.0437, delta_norm=2.839e-01, relative_change=0.2252% (9 params)
2025-11-14 17:34:32 (IST) - 0:08:33 - train - INFO - step: 000078 - done (%): 3.9 - loss: 5.301 - lr: 8.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.9 - avg_words_per_second: 5273.9 - ETA: >2025-11-14 20:59:31
2025-11-14 17:34:32 (IST) - 0:08:33 - train - INFO - [DocStream] step=79 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=2-3]
2025-11-14 17:34:38 (IST) - 0:08:39 - train - INFO - [TTT] Step 79: grad_norm=1.000e+00, param_norm=126.0478, delta_norm=2.836e-01, relative_change=0.2250% (9 params)
2025-11-14 17:34:38 (IST) - 0:08:39 - train - INFO - step: 000079 - done (%): 4.0 - loss: 5.037 - lr: 9.0e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5308.5 - avg_words_per_second: 5274.3 - ETA: >2025-11-14 20:59:30
2025-11-14 17:34:38 (IST) - 0:08:40 - train - INFO - [DocStream] step=80 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=4-5]
2025-11-14 17:34:44 (IST) - 0:08:46 - train - INFO - [TTT] Step 80: grad_norm=1.000e+00, param_norm=126.0517, delta_norm=2.963e-01, relative_change=0.2351% (9 params)
2025-11-14 17:34:44 (IST) - 0:08:46 - train - INFO - step: 000080 - done (%): 4.0 - loss: 3.893 - lr: 9.1e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.0 - avg_words_per_second: 5274.8 - ETA: >2025-11-14 20:59:29
2025-11-14 17:34:45 (IST) - 0:08:46 - train - INFO - [DocStream] step=81 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=6-7]
2025-11-14 17:34:51 (IST) - 0:08:52 - train - INFO - [TTT] Step 81: grad_norm=1.000e+00, param_norm=126.0553, delta_norm=2.891e-01, relative_change=0.2294% (9 params)
2025-11-14 17:34:51 (IST) - 0:08:52 - train - INFO - step: 000081 - done (%): 4.0 - loss: 2.862 - lr: 9.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5314.3 - avg_words_per_second: 5275.2 - ETA: >2025-11-14 20:59:28
2025-11-14 17:34:51 (IST) - 0:08:52 - train - INFO - [DocStream] step=82 microbatch=0 samples=2 unique_docs=2 runs=4245.wav[segment=8], 4247.wav[segment=0]
2025-11-14 17:34:51 (IST) - 0:08:52 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4245.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4247.wav
2025-11-14 17:34:51 (IST) - 0:08:52 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:34:51 (IST) - 0:08:52 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:34:51 (IST) - 0:08:52 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:34:57 (IST) - 0:08:58 - train - INFO - [TTT] Step 82: grad_norm=1.000e+00, param_norm=126.0609, delta_norm=3.190e-01, relative_change=0.2531% (9 params)
2025-11-14 17:34:57 (IST) - 0:08:58 - train - INFO - step: 000082 - done (%): 4.1 - loss: 4.169 - lr: 9.2e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5361.3 - avg_words_per_second: 5276.3 - ETA: >2025-11-14 20:59:26
2025-11-14 17:34:57 (IST) - 0:08:59 - train - INFO - [DocStream] step=83 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=1-2]
2025-11-14 17:35:03 (IST) - 0:09:05 - train - INFO - [TTT] Step 83: grad_norm=1.000e+00, param_norm=126.0661, delta_norm=2.989e-01, relative_change=0.2371% (9 params)
2025-11-14 17:35:03 (IST) - 0:09:05 - train - INFO - step: 000083 - done (%): 4.2 - loss: 4.282 - lr: 9.3e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5308.0 - avg_words_per_second: 5276.7 - ETA: >2025-11-14 20:59:25
2025-11-14 17:35:04 (IST) - 0:09:05 - train - INFO - [DocStream] step=84 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=3-4]
2025-11-14 17:35:10 (IST) - 0:09:11 - train - INFO - [TTT] Step 84: grad_norm=1.000e+00, param_norm=126.0710, delta_norm=2.998e-01, relative_change=0.2378% (9 params)
2025-11-14 17:35:10 (IST) - 0:09:11 - train - INFO - step: 000084 - done (%): 4.2 - loss: 3.561 - lr: 9.4e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5310.8 - avg_words_per_second: 5277.1 - ETA: >2025-11-14 20:59:24
2025-11-14 17:35:10 (IST) - 0:09:11 - train - INFO - [DocStream] step=85 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=5-6]
2025-11-14 17:35:16 (IST) - 0:09:17 - train - INFO - [TTT] Step 85: grad_norm=9.561e-01, param_norm=126.0755, delta_norm=3.124e-01, relative_change=0.2478% (9 params)
2025-11-14 17:35:16 (IST) - 0:09:17 - train - INFO - step: 000085 - done (%): 4.2 - loss: 1.953 - lr: 9.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5308.9 - avg_words_per_second: 5277.4 - ETA: >2025-11-14 20:59:23
2025-11-14 17:35:16 (IST) - 0:09:18 - train - INFO - [DocStream] step=86 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=7-8]
2025-11-14 17:35:22 (IST) - 0:09:24 - train - INFO - [TTT] Step 86: grad_norm=6.997e-01, param_norm=126.0798, delta_norm=3.238e-01, relative_change=0.2568% (9 params)
2025-11-14 17:35:22 (IST) - 0:09:24 - train - INFO - step: 000086 - done (%): 4.3 - loss: 2.328 - lr: 9.5e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5325.0 - avg_words_per_second: 5278.0 - ETA: >2025-11-14 20:59:21
2025-11-14 17:35:23 (IST) - 0:09:24 - train - INFO - [DocStream] step=87 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=9-10]
2025-11-14 17:35:29 (IST) - 0:09:30 - train - INFO - [TTT] Step 87: grad_norm=7.056e-01, param_norm=126.0842, delta_norm=3.231e-01, relative_change=0.2563% (9 params)
2025-11-14 17:35:29 (IST) - 0:09:30 - train - INFO - step: 000087 - done (%): 4.3 - loss: 2.365 - lr: 9.6e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5314.8 - avg_words_per_second: 5278.4 - ETA: >2025-11-14 20:59:20
2025-11-14 17:35:29 (IST) - 0:09:30 - train - INFO - [DocStream] step=88 microbatch=0 samples=2 unique_docs=2 runs=4247.wav[segment=11], 4248.wav[segment=0]
2025-11-14 17:35:29 (IST) - 0:09:30 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4247.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4248.wav
2025-11-14 17:35:29 (IST) - 0:09:30 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:35:29 (IST) - 0:09:30 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:35:29 (IST) - 0:09:30 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:35:35 (IST) - 0:09:36 - train - INFO - [TTT] Step 88: grad_norm=1.000e+00, param_norm=126.0910, delta_norm=3.329e-01, relative_change=0.2640% (9 params)
2025-11-14 17:35:35 (IST) - 0:09:36 - train - INFO - step: 000088 - done (%): 4.4 - loss: 4.110 - lr: 9.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.4 - avg_words_per_second: 5278.8 - ETA: >2025-11-14 20:59:20
2025-11-14 17:35:35 (IST) - 0:09:37 - train - INFO - [DocStream] step=89 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=1-2]
2025-11-14 17:35:41 (IST) - 0:09:43 - train - INFO - [TTT] Step 89: grad_norm=1.000e+00, param_norm=126.0972, delta_norm=3.229e-01, relative_change=0.2561% (9 params)
2025-11-14 17:35:41 (IST) - 0:09:43 - train - INFO - step: 000089 - done (%): 4.5 - loss: 4.330 - lr: 9.7e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5323.4 - avg_words_per_second: 5279.3 - ETA: >2025-11-14 20:59:18
2025-11-14 17:35:42 (IST) - 0:09:43 - train - INFO - [DocStream] step=90 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=3-4]
2025-11-14 17:35:48 (IST) - 0:09:49 - train - INFO - [TTT] Step 90: grad_norm=1.000e+00, param_norm=126.1030, delta_norm=3.420e-01, relative_change=0.2712% (9 params)
2025-11-14 17:35:48 (IST) - 0:09:49 - train - INFO - step: 000090 - done (%): 4.5 - loss: 3.420 - lr: 9.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.0 - avg_words_per_second: 5279.6 - ETA: >2025-11-14 20:59:17
2025-11-14 17:35:48 (IST) - 0:09:49 - train - INFO - [DocStream] step=91 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=5-6]
2025-11-14 17:35:54 (IST) - 0:09:55 - train - INFO - [TTT] Step 91: grad_norm=8.752e-01, param_norm=126.1085, delta_norm=3.330e-01, relative_change=0.2641% (9 params)
2025-11-14 17:35:54 (IST) - 0:09:55 - train - INFO - step: 000091 - done (%): 4.5 - loss: 2.615 - lr: 9.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.8 - avg_words_per_second: 5280.0 - ETA: >2025-11-14 20:59:17
2025-11-14 17:35:54 (IST) - 0:09:56 - train - INFO - [DocStream] step=92 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=7-8]
2025-11-14 17:36:01 (IST) - 0:10:02 - train - INFO - [TTT] Step 92: grad_norm=6.894e-01, param_norm=126.1138, delta_norm=3.263e-01, relative_change=0.2588% (9 params)
2025-11-14 17:36:01 (IST) - 0:10:02 - train - INFO - step: 000092 - done (%): 4.6 - loss: 2.241 - lr: 9.8e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5322.2 - avg_words_per_second: 5280.4 - ETA: >2025-11-14 20:59:16
2025-11-14 17:36:01 (IST) - 0:10:02 - train - INFO - [DocStream] step=93 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=9-10]
2025-11-14 17:36:07 (IST) - 0:10:08 - train - INFO - [TTT] Step 93: grad_norm=8.231e-01, param_norm=126.1187, delta_norm=3.253e-01, relative_change=0.2579% (9 params)
2025-11-14 17:36:07 (IST) - 0:10:08 - train - INFO - step: 000093 - done (%): 4.7 - loss: 2.569 - lr: 9.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5314.5 - avg_words_per_second: 5280.8 - ETA: >2025-11-14 20:59:15
2025-11-14 17:36:07 (IST) - 0:10:08 - train - INFO - [DocStream] step=94 microbatch=0 samples=2 unique_docs=2 runs=4248.wav[segment=11], 4289.wav[segment=0]
2025-11-14 17:36:07 (IST) - 0:10:08 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4248.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4289.wav
2025-11-14 17:36:07 (IST) - 0:10:08 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:36:07 (IST) - 0:10:08 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:36:07 (IST) - 0:10:08 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:36:13 (IST) - 0:10:14 - train - INFO - [TTT] Step 94: grad_norm=1.000e+00, param_norm=126.1254, delta_norm=3.279e-01, relative_change=0.2600% (9 params)
2025-11-14 17:36:13 (IST) - 0:10:14 - train - INFO - step: 000094 - done (%): 4.7 - loss: 3.212 - lr: 9.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5326.4 - avg_words_per_second: 5281.3 - ETA: >2025-11-14 20:59:14
2025-11-14 17:36:14 (IST) - 0:10:15 - train - INFO - [DocStream] step=95 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=1-2]
2025-11-14 17:36:20 (IST) - 0:10:21 - train - INFO - [TTT] Step 95: grad_norm=1.000e+00, param_norm=126.1316, delta_norm=3.231e-01, relative_change=0.2562% (9 params)
2025-11-14 17:36:20 (IST) - 0:10:21 - train - INFO - step: 000095 - done (%): 4.8 - loss: 2.043 - lr: 9.9e-05 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5311.0 - avg_words_per_second: 5281.6 - ETA: >2025-11-14 20:59:13
2025-11-14 17:36:20 (IST) - 0:10:21 - train - INFO - [DocStream] step=96 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=3-4]
2025-11-14 17:36:26 (IST) - 0:10:27 - train - INFO - [TTT] Step 96: grad_norm=8.101e-01, param_norm=126.1373, delta_norm=3.183e-01, relative_change=0.2524% (9 params)
2025-11-14 17:36:26 (IST) - 0:10:27 - train - INFO - step: 000096 - done (%): 4.8 - loss: 0.972 - lr: 1.0e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5323.5 - avg_words_per_second: 5282.0 - ETA: >2025-11-14 20:59:12
2025-11-14 17:36:26 (IST) - 0:10:27 - train - INFO - [DocStream] step=97 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=5-6]
2025-11-14 17:36:32 (IST) - 0:10:33 - train - INFO - [TTT] Step 97: grad_norm=1.000e+00, param_norm=126.1426, delta_norm=2.987e-01, relative_change=0.2368% (9 params)
2025-11-14 17:36:32 (IST) - 0:10:33 - train - INFO - step: 000097 - done (%): 4.8 - loss: 0.717 - lr: 1.0e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5329.5 - avg_words_per_second: 5282.5 - ETA: >2025-11-14 20:59:11
2025-11-14 17:36:33 (IST) - 0:10:34 - train - INFO - [DocStream] step=98 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=7-8]
2025-11-14 17:36:39 (IST) - 0:10:40 - train - INFO - [TTT] Step 98: grad_norm=5.835e-01, param_norm=126.1477, delta_norm=2.879e-01, relative_change=0.2282% (9 params)
2025-11-14 17:36:39 (IST) - 0:10:40 - train - INFO - step: 000098 - done (%): 4.9 - loss: 1.193 - lr: 1.0e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5322.8 - avg_words_per_second: 5282.9 - ETA: >2025-11-14 20:59:10
2025-11-14 17:36:39 (IST) - 0:10:40 - train - INFO - [DocStream] step=99 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=9-10]
2025-11-14 17:36:45 (IST) - 0:10:46 - train - INFO - [TTT] Step 99: grad_norm=7.287e-01, param_norm=126.1526, delta_norm=2.780e-01, relative_change=0.2203% (9 params)
2025-11-14 17:36:45 (IST) - 0:10:46 - train - INFO - step: 000099 - done (%): 5.0 - loss: 1.228 - lr: 1.0e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5315.5 - avg_words_per_second: 5283.2 - ETA: >2025-11-14 20:59:09
2025-11-14 17:36:45 (IST) - 0:10:47 - train - INFO - [DocStream] step=100 microbatch=0 samples=2 unique_docs=2 runs=4289.wav[segment=11], 4290.wav[segment=0]
2025-11-14 17:36:45 (IST) - 0:10:47 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4289.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4290.wav
2025-11-14 17:36:45 (IST) - 0:10:47 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.500000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:36:45 (IST) - 0:10:47 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:36:45 (IST) - 0:10:47 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 74.000000 (dtype: torch.bfloat16); both learn via backprop during training
2025-11-14 17:36:51 (IST) - 0:10:53 - train - INFO - [TTT] Step 100: grad_norm=1.000e+00, param_norm=126.1607, delta_norm=3.183e-01, relative_change=0.2523% (9 params)
2025-11-14 17:36:51 (IST) - 0:10:53 - train - INFO - step: 000100 - done (%): 5.0 - loss: 3.247 - lr: 1.0e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5325.9 - avg_words_per_second: 5283.7 - ETA: >2025-11-14 20:59:08
2025-11-14 17:36:51 (IST) - 0:10:53 - checkpointing - INFO - Dumping checkpoint in /sise/eliyanac-group/ron_al/ttt_training_run2/checkpoints/checkpoint_000100/consolidated using tmp name: tmp.consolidated
2025-11-14 17:36:51 (IST) - 0:10:53 - utils - INFO - Closing: eval_logger
2025-11-14 17:36:52 (IST) - 0:10:54 - utils - ERROR - Error while closing eval_logger!
2025-11-14 17:36:52 (IST) - 0:10:54 - utils - INFO - Closing: metrics_logger
2025-11-14 17:36:52 (IST) - 0:10:54 - utils - INFO - Closed: metrics_logger
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mrun2[0m at: [34mhttps://wandb.ai/alufr-ben-gurion-university-of-the-negev/moshi_in_place/runs/frdlvb7i[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../sise/eliyanac-group/ron_al/ttt_training_run2/wandb/run-20251114_172604-frdlvb7i/logs[0m
==================================================
Job finished at: Fri 14 Nov 2025 17:36:55 IST
==================================================
