[rank0]:[W1114 17:00:43.602608894 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
wandb: Currently logged in as: alufr (alufr-ben-gurion-university-of-the-negev) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /sise/eliyanac-group/ron_al/ttt_training_run2/wandb/run-20251114_170043-y1jt5bmr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alufr-ben-gurion-university-of-the-negev/moshi_in_place
wandb: üöÄ View run at https://wandb.ai/alufr-ben-gurion-university-of-the-negev/moshi_in_place/runs/y1jt5bmr
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
/home/alufr/moshi_in_place_ttt/moshi_in_place/moshi/moshi/moshi/models/loaders.py:204: UserWarning: Repository kyutai/moshiko-pytorch-bf16 contains no config.json. Assuming this is a Moshi 7B. Support for such repository might be removed in the future.
  warnings.warn(
2025-11-14 17:00:48 (IST) - 0:00:11 - finetune.wrapped_model - WARNING - Buffer transformer.layers.10.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:00:49 (IST) - 0:00:12 - finetune.wrapped_model - WARNING - Buffer transformer.layers.20.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - WARNING - Buffer transformer.layers.30.gating.ttt_clip_event_counter still meta - initializing as zeros
