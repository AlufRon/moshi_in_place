==================================================
Job started at: Fri 14 Nov 2025 17:00:33 IST
Running on node: ise-6000-04.auth.ad.bgu.ac.il
GPU info:
Fri Nov 14 17:00:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:24:00.0 Off |                  Off |
| 30%   26C    P8             15W /  300W |       2MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==================================================
Set CUDA_VISIBLE_DEVICES=0
Starting YARN + TTT training...
Context Extension: 4x (3000 -> 12000 tokens)
TTT Layers: 3 (layers 10, 20, 30)
Base Model: Frozen (only TTT params trained)
Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.
2025-11-14 17:00:42 (IST) - 0:00:05 - distributed - INFO - torch.cuda.device_count: 1
2025-11-14 17:00:42 (IST) - 0:00:05 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0
2025-11-14 17:00:42 (IST) - 0:00:05 - distributed - INFO - local rank: 0
2025-11-14 17:00:42 (IST) - 0:00:05 - train - INFO - Going to init comms...
2025-11-14 17:00:42 (IST) - 0:00:05 - train - INFO - Run dir: /sise/eliyanac-group/ron_al/ttt_training_run2
2025-11-14 17:00:42 (IST) - 0:00:05 - train - INFO - Removing run dir /sise/eliyanac-group/ron_al/ttt_training_run2...
2025-11-14 17:00:43 (IST) - 0:00:06 - train - INFO - TrainArgs: {'batch_size': 2,
 'ckpt_freq': 100,
 'data': {'eval_data': '',
          'shuffle': False,
          'train_data': '/sise/eliyanac-group/ron_al/talkbank_callhome_english/talkbank.jsonl'},
 'do_ckpt': True,
 'do_eval': False,
 'duration_sec': 150.0,
 'eval_freq': 100,
 'first_codebook_weight_multiplier': 100.0,
 'full_finetuning': False,
 'gradient_checkpointing': True,
 'log_freq': 1,
 'lora': {'enable': False, 'ft_embed': False, 'rank': 64, 'scaling': 2.0},
 'max_norm': 1.0,
 'max_steps': 2000,
 'moshi_paths': {'config_path': None,
                 'hf_repo_id': 'kyutai/moshiko-pytorch-bf16',
                 'mimi_path': None,
                 'moshi_path': None,
                 'tokenizer_path': None},
 'num_ckpt_keep': 3,
 'num_microbatches': 1,
 'optim': {'lr': 0.01, 'pct_start': 0.05, 'weight_decay': 0.1},
 'overwrite_run_dir': True,
 'param_dtype': 'bfloat16',
 'run_dir': '/sise/eliyanac-group/ron_al/ttt_training_run2',
 'save_adapters': True,
 'seed': 0,
 'text_padding_weight': 0.5,
 'ttt': {'chunk_size': 256,
         'conv_kernel_size': 2,
         'delta_clip_fro_norm': 100.0,
         'enabled': True,
         'layer_frequency': 10,
         'learning_rate': 0.0001,
         'start_layer': 10,
         'unfreeze_ttt_layers': False},
 'wandb': {'key': '',
           'offline': False,
           'project': 'moshi_in_place',
           'run_name': 'run2'},
 'world_size': 1,
 'yarn': {'beta_fast': 32,
          'beta_slow': 1,
          'enabled': True,
          'mscale': 1.0,
          'mscale_all_dim': 0.0,
          'original_max_seq_len': 3000,
          'scale': 4.0}}
2025-11-14 17:00:43 (IST) - 0:00:06 - metrics_logger - INFO - initializing wandb
2025-11-14 17:00:45 (IST) - 0:00:08 - train - INFO - Loading Mimi and Moshi...
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - TTT (Test-Time Training) ENABLED
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Layer frequency: 10
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Start layer: 10
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Chunk size: 256
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Learning rate: 0.0001
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Conv kernel: 2
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - YaRN (Context Window Extension) ENABLED
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Scale: 4.0x
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Original max seq len: 3000
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Beta fast: 32
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   Beta slow: 1
2025-11-14 17:00:47 (IST) - 0:00:09 - finetune.wrapped_model - INFO - ======================================================================
[YaRN] Enabled with scale=4.0, original_len=3000
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[YaRN] Initializing RoPE buffers on device=meta
[YaRN] RoPE buffers initialized successfully
2025-11-14 17:00:47 (IST) - 0:00:10 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...
2025-11-14 17:00:47 (IST) - 0:00:10 - finetune.wrapped_model - INFO - Initializing TTT w_down from pretrained checkpoint...
2025-11-14 17:00:47 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ transformer.layers.10.gating.w_down <- transformer.layers.10.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:00:47 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ transformer.layers.20.gating.w_down <- transformer.layers.20.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:00:47 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ transformer.layers.30.gating.w_down <- transformer.layers.30.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:00:47 (IST) - 0:00:10 - finetune.wrapped_model - INFO - Initializing TTT layers ...
2025-11-14 17:00:48 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:00:48 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:00:48 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.10.gating.w_down_pretrained from w_down
2025-11-14 17:00:48 (IST) - 0:00:11 - finetune.wrapped_model - WARNING - Buffer transformer.layers.10.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:00:49 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:00:49 (IST) - 0:00:12 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:00:49 (IST) - 0:00:12 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.20.gating.w_down_pretrained from w_down
2025-11-14 17:00:49 (IST) - 0:00:12 - finetune.wrapped_model - WARNING - Buffer transformer.layers.20.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:00:50 (IST) - 0:00:12 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.30.gating.w_down_pretrained from w_down
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - WARNING - Buffer transformer.layers.30.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO - Initializing YaRN RoPE buffers ...
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.rope.inv_freq (shape: torch.Size([64]), scale: 4.0x)
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO - Finished initialization!
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO - TTT ACTIVE: 3 layers enabled
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO - TTT layer indices: [10, 20, 30]
2025-11-14 17:00:50 (IST) - 0:00:13 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:01:00 (IST) - 0:00:22 - train - INFO - [DocStream] step=1 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=0-1]
2025-11-14 17:01:00 (IST) - 0:00:22 - train - INFO - [TTT RESET] Document switch detected: None -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav
2025-11-14 17:01:00 (IST) - 0:00:22 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:00 (IST) - 0:00:22 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:00 (IST) - 0:00:22 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.367013
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.483802
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.794504
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   requires_grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   has grad: True
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - =========================

2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - [TTT] Step 1: grad_norm=1.000e+00, param_norm=126.0420, delta_norm=4.697e+00, relative_change=3.7264% (9 params)
2025-11-14 17:01:08 (IST) - 0:00:31 - train - INFO - step: 000001 - done (%): 0.1 - loss: 3.378 - lr: 4.0e-04 - peak_alloc_mem (GB): 36.0 - alloc_mem (GB): 21.0 - words_per_second: 2151.3 - avg_words_per_second: 2151.3 - ETA: >2025-11-15 01:43:49
2025-11-14 17:01:09 (IST) - 0:00:31 - train - INFO - [DocStream] step=2 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=2-3]
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.359873
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.551054
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.752882
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   requires_grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   has grad: True
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - =========================

2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - [TTT] Step 2: grad_norm=1.000e+00, param_norm=126.1541, delta_norm=3.238e+00, relative_change=2.5670% (9 params)
2025-11-14 17:01:14 (IST) - 0:00:37 - train - INFO - step: 000002 - done (%): 0.1 - loss: 15.218 - lr: 4.0e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5641.3 - avg_words_per_second: 3114.8 - ETA: >2025-11-14 23:02:03
2025-11-14 17:01:15 (IST) - 0:00:38 - train - INFO - [DocStream] step=3 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=4-5]
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.251355
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.422377
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.870872
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   requires_grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   has grad: True
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - =========================

2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - [TTT] Step 3: grad_norm=1.000e+00, param_norm=126.2753, delta_norm=2.637e+00, relative_change=2.0879% (9 params)
2025-11-14 17:01:21 (IST) - 0:00:43 - train - INFO - step: 000003 - done (%): 0.1 - loss: 8.474 - lr: 4.1e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5229.9 - avg_words_per_second: 3600.1 - ETA: >2025-11-14 22:13:22
2025-11-14 17:01:21 (IST) - 0:00:44 - train - INFO - [DocStream] step=4 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=0-1]
2025-11-14 17:01:21 (IST) - 0:00:44 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav
2025-11-14 17:01:21 (IST) - 0:00:44 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:21 (IST) - 0:00:44 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:21 (IST) - 0:00:44 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.406624
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.474242
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.780865
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   requires_grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   has grad: True
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO -   grad norm: 0.000000
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - =========================

2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - [TTT] Step 4: grad_norm=1.000e+00, param_norm=126.4255, delta_norm=2.536e+00, relative_change=2.0063% (9 params)
2025-11-14 17:01:27 (IST) - 0:00:49 - train - INFO - step: 000004 - done (%): 0.2 - loss: 4.810 - lr: 4.2e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5600.6 - avg_words_per_second: 3953.1 - ETA: >2025-11-14 21:45:28
2025-11-14 17:01:27 (IST) - 0:00:50 - train - INFO - [DocStream] step=5 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=2-3]
2025-11-14 17:01:33 (IST) - 0:00:55 - train - INFO - [TTT] Step 5: grad_norm=1.000e+00, param_norm=126.5876, delta_norm=2.321e+00, relative_change=1.8334% (9 params)
2025-11-14 17:01:33 (IST) - 0:00:55 - train - INFO - step: 000005 - done (%): 0.2 - loss: 10.633 - lr: 4.4e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5596.3 - avg_words_per_second: 4199.7 - ETA: >2025-11-14 21:28:45
2025-11-14 17:01:33 (IST) - 0:00:56 - train - INFO - [DocStream] step=6 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=4-5]
2025-11-14 17:01:39 (IST) - 0:01:01 - train - INFO - [TTT] Step 6: grad_norm=1.000e+00, param_norm=126.7573, delta_norm=2.276e+00, relative_change=1.7958% (9 params)
2025-11-14 17:01:39 (IST) - 0:01:01 - train - INFO - step: 000006 - done (%): 0.3 - loss: 5.650 - lr: 4.6e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5570.4 - avg_words_per_second: 4379.3 - ETA: >2025-11-14 21:17:46
2025-11-14 17:01:39 (IST) - 0:01:02 - train - INFO - [DocStream] step=7 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=6-7]
2025-11-14 17:01:45 (IST) - 0:01:08 - train - INFO - [TTT] Step 7: grad_norm=1.000e+00, param_norm=126.9434, delta_norm=2.310e+00, relative_change=1.8194% (9 params)
2025-11-14 17:01:45 (IST) - 0:01:08 - train - INFO - step: 000007 - done (%): 0.3 - loss: 4.174 - lr: 4.9e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5567.8 - avg_words_per_second: 4517.1 - ETA: >2025-11-14 21:09:56
2025-11-14 17:01:45 (IST) - 0:01:08 - train - INFO - [DocStream] step=8 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=8-9]
2025-11-14 17:01:51 (IST) - 0:01:14 - train - INFO - [TTT] Step 8: grad_norm=1.000e+00, param_norm=127.1371, delta_norm=2.254e+00, relative_change=1.7732% (9 params)
2025-11-14 17:01:51 (IST) - 0:01:14 - train - INFO - step: 000008 - done (%): 0.4 - loss: 3.308 - lr: 5.2e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5554.5 - avg_words_per_second: 4625.1 - ETA: >2025-11-14 21:04:07
2025-11-14 17:01:51 (IST) - 0:01:14 - train - INFO - [DocStream] step=9 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=10-11]
2025-11-14 17:01:57 (IST) - 0:01:20 - train - INFO - [TTT] Step 9: grad_norm=1.000e+00, param_norm=127.3446, delta_norm=2.289e+00, relative_change=1.7977% (9 params)
2025-11-14 17:01:57 (IST) - 0:01:20 - train - INFO - step: 000009 - done (%): 0.5 - loss: 2.852 - lr: 5.5e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5536.5 - avg_words_per_second: 4711.2 - ETA: >2025-11-14 20:59:40
2025-11-14 17:01:57 (IST) - 0:01:20 - train - INFO - [DocStream] step=10 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=0-1]
2025-11-14 17:01:57 (IST) - 0:01:20 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav
2025-11-14 17:01:57 (IST) - 0:01:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:57 (IST) - 0:01:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:01:57 (IST) - 0:01:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.986328; w_down is frozen during training so its norm stays constant
2025-11-14 17:02:03 (IST) - 0:01:26 - train - INFO - [TTT] Step 10: grad_norm=1.000e+00, param_norm=127.5962, delta_norm=2.395e+00, relative_change=1.8774% (9 params)
2025-11-14 17:02:03 (IST) - 0:01:26 - train - INFO - step: 000010 - done (%): 0.5 - loss: 5.038 - lr: 5.9e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5529.9 - avg_words_per_second: 4782.0 - ETA: >2025-11-14 20:56:08
2025-11-14 17:02:03 (IST) - 0:01:26 - train - INFO - [DocStream] step=11 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=2-3]
2025-11-14 17:02:09 (IST) - 0:01:32 - train - INFO - [TTT] Step 11: grad_norm=1.000e+00, param_norm=127.8711, delta_norm=2.406e+00, relative_change=1.8818% (9 params)
2025-11-14 17:02:09 (IST) - 0:01:32 - train - INFO - step: 000011 - done (%): 0.6 - loss: 10.351 - lr: 6.4e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5516.0 - avg_words_per_second: 4840.6 - ETA: >2025-11-14 20:53:17
2025-11-14 17:02:10 (IST) - 0:01:32 - train - INFO - [DocStream] step=12 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=4-5]
2025-11-14 17:02:15 (IST) - 0:01:38 - train - INFO - [TTT] Step 12: grad_norm=1.000e+00, param_norm=128.1700, delta_norm=2.411e+00, relative_change=1.8810% (9 params)
2025-11-14 17:02:15 (IST) - 0:01:38 - train - INFO - step: 000012 - done (%): 0.6 - loss: 3.835 - lr: 6.9e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5507.5 - avg_words_per_second: 4889.9 - ETA: >2025-11-14 20:50:56
2025-11-14 17:02:16 (IST) - 0:01:38 - train - INFO - [DocStream] step=13 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=6-7]
2025-11-14 17:02:22 (IST) - 0:01:44 - train - INFO - [TTT] Step 13: grad_norm=1.000e+00, param_norm=128.4958, delta_norm=2.580e+00, relative_change=2.0075% (9 params)
2025-11-14 17:02:22 (IST) - 0:01:44 - train - INFO - step: 000013 - done (%): 0.7 - loss: 3.661 - lr: 7.4e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5499.8 - avg_words_per_second: 4932.0 - ETA: >2025-11-14 20:48:59
2025-11-14 17:02:22 (IST) - 0:01:45 - train - INFO - [DocStream] step=14 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=8-9]
2025-11-14 17:02:28 (IST) - 0:01:50 - train - INFO - [TTT] Step 14: grad_norm=1.000e+00, param_norm=128.8494, delta_norm=2.588e+00, relative_change=2.0088% (9 params)
2025-11-14 17:02:28 (IST) - 0:01:50 - train - INFO - step: 000014 - done (%): 0.7 - loss: 3.332 - lr: 8.0e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5487.0 - avg_words_per_second: 4967.9 - ETA: >2025-11-14 20:47:20
2025-11-14 17:02:28 (IST) - 0:01:51 - train - INFO - [DocStream] step=15 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=10-11]
2025-11-14 17:02:34 (IST) - 0:01:56 - train - INFO - [TTT] Step 15: grad_norm=1.000e+00, param_norm=129.2316, delta_norm=2.662e+00, relative_change=2.0596% (9 params)
2025-11-14 17:02:34 (IST) - 0:01:57 - train - INFO - step: 000015 - done (%): 0.8 - loss: 3.256 - lr: 8.7e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5479.7 - avg_words_per_second: 4999.0 - ETA: >2025-11-14 20:45:55
2025-11-14 17:02:34 (IST) - 0:01:57 - train - INFO - [DocStream] step=16 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=0-1]
2025-11-14 17:02:34 (IST) - 0:01:57 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav
2025-11-14 17:02:34 (IST) - 0:01:57 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:02:34 (IST) - 0:01:57 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:02:34 (IST) - 0:01:57 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.988281; w_down is frozen during training so its norm stays constant
2025-11-14 17:02:40 (IST) - 0:02:03 - train - INFO - [TTT] Step 16: grad_norm=1.000e+00, param_norm=129.6967, delta_norm=2.822e+00, relative_change=2.1761% (9 params)
2025-11-14 17:02:40 (IST) - 0:02:03 - train - INFO - step: 000016 - done (%): 0.8 - loss: 4.238 - lr: 9.3e-04 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5426.2 - avg_words_per_second: 5023.7 - ETA: >2025-11-14 20:44:49
2025-11-14 17:02:40 (IST) - 0:02:03 - train - INFO - [DocStream] step=17 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=2-3]
2025-11-14 17:02:46 (IST) - 0:02:09 - train - INFO - [TTT] Step 17: grad_norm=1.000e+00, param_norm=130.2027, delta_norm=2.919e+00, relative_change=2.2420% (9 params)
2025-11-14 17:02:46 (IST) - 0:02:09 - train - INFO - step: 000017 - done (%): 0.8 - loss: 9.765 - lr: 1.0e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5477.7 - avg_words_per_second: 5048.3 - ETA: >2025-11-14 20:43:43
2025-11-14 17:02:47 (IST) - 0:02:09 - train - INFO - [DocStream] step=18 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=4-5]
2025-11-14 17:02:52 (IST) - 0:02:15 - train - INFO - [TTT] Step 18: grad_norm=1.000e+00, param_norm=130.7469, delta_norm=3.075e+00, relative_change=2.3519% (9 params)
2025-11-14 17:02:52 (IST) - 0:02:15 - train - INFO - step: 000018 - done (%): 0.9 - loss: 3.444 - lr: 1.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5469.1 - avg_words_per_second: 5070.0 - ETA: >2025-11-14 20:42:46
2025-11-14 17:02:53 (IST) - 0:02:15 - train - INFO - [DocStream] step=19 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=6-7]
2025-11-14 17:02:59 (IST) - 0:02:21 - train - INFO - [TTT] Step 19: grad_norm=1.000e+00, param_norm=131.3375, delta_norm=3.185e+00, relative_change=2.4250% (9 params)
2025-11-14 17:02:59 (IST) - 0:02:21 - train - INFO - step: 000019 - done (%): 0.9 - loss: 4.729 - lr: 1.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5476.8 - avg_words_per_second: 5089.9 - ETA: >2025-11-14 20:41:54
2025-11-14 17:02:59 (IST) - 0:02:22 - train - INFO - [DocStream] step=20 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=8-9]
2025-11-14 17:03:05 (IST) - 0:02:27 - train - INFO - [TTT] Step 20: grad_norm=1.000e+00, param_norm=131.9777, delta_norm=3.222e+00, relative_change=2.4415% (9 params)
2025-11-14 17:03:05 (IST) - 0:02:27 - train - INFO - step: 000020 - done (%): 1.0 - loss: 4.308 - lr: 1.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5456.9 - avg_words_per_second: 5107.1 - ETA: >2025-11-14 20:41:10
2025-11-14 17:03:05 (IST) - 0:02:28 - train - INFO - [DocStream] step=21 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=10-11]
2025-11-14 17:03:11 (IST) - 0:02:34 - train - INFO - [TTT] Step 21: grad_norm=1.000e+00, param_norm=132.6728, delta_norm=3.386e+00, relative_change=2.5523% (9 params)
2025-11-14 17:03:11 (IST) - 0:02:34 - train - INFO - step: 000021 - done (%): 1.1 - loss: 2.833 - lr: 1.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5462.5 - avg_words_per_second: 5123.0 - ETA: >2025-11-14 20:40:29
2025-11-14 17:03:11 (IST) - 0:02:34 - train - INFO - [DocStream] step=22 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=0-1]
2025-11-14 17:03:11 (IST) - 0:02:34 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav
2025-11-14 17:03:11 (IST) - 0:02:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:03:11 (IST) - 0:02:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:03:11 (IST) - 0:02:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.988281; w_down is frozen during training so its norm stays constant
2025-11-14 17:03:17 (IST) - 0:02:40 - train - INFO - [TTT] Step 22: grad_norm=1.000e+00, param_norm=133.5616, delta_norm=3.743e+00, relative_change=2.8022% (9 params)
2025-11-14 17:03:17 (IST) - 0:02:40 - train - INFO - step: 000022 - done (%): 1.1 - loss: 3.864 - lr: 1.4e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5445.8 - avg_words_per_second: 5136.8 - ETA: >2025-11-14 20:39:53
2025-11-14 17:03:17 (IST) - 0:02:40 - train - INFO - [DocStream] step=23 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=2-3]
2025-11-14 17:03:23 (IST) - 0:02:46 - train - INFO - [TTT] Step 23: grad_norm=1.000e+00, param_norm=134.5279, delta_norm=3.825e+00, relative_change=2.8432% (9 params)
2025-11-14 17:03:23 (IST) - 0:02:46 - train - INFO - step: 000023 - done (%): 1.1 - loss: 7.852 - lr: 1.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5462.1 - avg_words_per_second: 5150.1 - ETA: >2025-11-14 20:39:19
2025-11-14 17:03:24 (IST) - 0:02:46 - train - INFO - [DocStream] step=24 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=4-5]
2025-11-14 17:03:29 (IST) - 0:02:52 - train - INFO - [TTT] Step 24: grad_norm=1.000e+00, param_norm=135.5611, delta_norm=4.014e+00, relative_change=2.9608% (9 params)
2025-11-14 17:03:29 (IST) - 0:02:52 - train - INFO - step: 000024 - done (%): 1.2 - loss: 4.713 - lr: 1.6e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5451.0 - avg_words_per_second: 5162.0 - ETA: >2025-11-14 20:38:49
2025-11-14 17:03:30 (IST) - 0:02:52 - train - INFO - [DocStream] step=25 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=6-7]
2025-11-14 17:03:36 (IST) - 0:02:58 - train - INFO - [TTT] Step 25: grad_norm=1.000e+00, param_norm=136.6608, delta_norm=4.315e+00, relative_change=3.1575% (9 params)
2025-11-14 17:03:36 (IST) - 0:02:58 - train - INFO - step: 000025 - done (%): 1.2 - loss: 3.825 - lr: 1.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5460.3 - avg_words_per_second: 5173.3 - ETA: >2025-11-14 20:38:20
2025-11-14 17:03:36 (IST) - 0:02:59 - train - INFO - [DocStream] step=26 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=8-9]
2025-11-14 17:03:42 (IST) - 0:03:05 - train - INFO - [TTT] Step 26: grad_norm=1.000e+00, param_norm=137.8401, delta_norm=4.371e+00, relative_change=3.1713% (9 params)
2025-11-14 17:03:42 (IST) - 0:03:05 - train - INFO - step: 000026 - done (%): 1.3 - loss: 2.841 - lr: 1.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5446.6 - avg_words_per_second: 5183.3 - ETA: >2025-11-14 20:37:55
2025-11-14 17:03:42 (IST) - 0:03:05 - train - INFO - [DocStream] step=27 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=10-11]
2025-11-14 17:03:48 (IST) - 0:03:11 - train - INFO - [TTT] Step 27: grad_norm=1.000e+00, param_norm=139.0894, delta_norm=4.242e+00, relative_change=3.0501% (9 params)
2025-11-14 17:03:48 (IST) - 0:03:11 - train - INFO - step: 000027 - done (%): 1.4 - loss: 4.062 - lr: 1.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5445.7 - avg_words_per_second: 5192.6 - ETA: >2025-11-14 20:37:32
2025-11-14 17:03:48 (IST) - 0:03:11 - train - INFO - [DocStream] step=28 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=0-1]
2025-11-14 17:03:48 (IST) - 0:03:11 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav
2025-11-14 17:03:48 (IST) - 0:03:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:03:48 (IST) - 0:03:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:03:48 (IST) - 0:03:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.990234; w_down is frozen during training so its norm stays constant
2025-11-14 17:03:54 (IST) - 0:03:17 - train - INFO - [TTT] Step 28: grad_norm=1.000e+00, param_norm=140.6781, delta_norm=4.868e+00, relative_change=3.4602% (9 params)
2025-11-14 17:03:54 (IST) - 0:03:17 - train - INFO - step: 000028 - done (%): 1.4 - loss: 3.957 - lr: 2.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5422.6 - avg_words_per_second: 5200.5 - ETA: >2025-11-14 20:37:12
2025-11-14 17:03:55 (IST) - 0:03:17 - train - INFO - [DocStream] step=29 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=2-3]
2025-11-14 17:04:00 (IST) - 0:03:23 - train - INFO - [TTT] Step 29: grad_norm=1.000e+00, param_norm=142.3713, delta_norm=5.042e+00, relative_change=3.5414% (9 params)
2025-11-14 17:04:01 (IST) - 0:03:23 - train - INFO - step: 000029 - done (%): 1.4 - loss: 8.232 - lr: 2.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5444.8 - avg_words_per_second: 5208.5 - ETA: >2025-11-14 20:36:52
2025-11-14 17:04:01 (IST) - 0:03:23 - train - INFO - [DocStream] step=30 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=4-5]
2025-11-14 17:04:07 (IST) - 0:03:29 - train - INFO - [TTT] Step 30: grad_norm=1.000e+00, param_norm=144.1696, delta_norm=5.346e+00, relative_change=3.7079% (9 params)
2025-11-14 17:04:07 (IST) - 0:03:29 - train - INFO - step: 000030 - done (%): 1.5 - loss: 3.493 - lr: 2.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5462.6 - avg_words_per_second: 5216.6 - ETA: >2025-11-14 20:36:32
2025-11-14 17:04:07 (IST) - 0:03:30 - train - INFO - [DocStream] step=31 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=6-7]
2025-11-14 17:04:13 (IST) - 0:03:36 - train - INFO - [TTT] Step 31: grad_norm=7.202e-01, param_norm=146.0759, delta_norm=5.441e+00, relative_change=3.7245% (9 params)
2025-11-14 17:04:13 (IST) - 0:03:36 - train - INFO - step: 000031 - done (%): 1.6 - loss: 3.312 - lr: 2.4e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5445.4 - avg_words_per_second: 5223.7 - ETA: >2025-11-14 20:36:15
2025-11-14 17:04:13 (IST) - 0:03:36 - train - INFO - [DocStream] step=32 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=8-9]
2025-11-14 17:04:19 (IST) - 0:03:42 - train - INFO - [TTT] Step 32: grad_norm=1.000e+00, param_norm=148.0651, delta_norm=5.515e+00, relative_change=3.7244% (9 params)
2025-11-14 17:04:19 (IST) - 0:03:42 - train - INFO - step: 000032 - done (%): 1.6 - loss: 3.170 - lr: 2.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5458.1 - avg_words_per_second: 5230.7 - ETA: >2025-11-14 20:35:57
2025-11-14 17:04:19 (IST) - 0:03:42 - train - INFO - [DocStream] step=33 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=10-11]
2025-11-14 17:04:25 (IST) - 0:03:48 - train - INFO - [TTT] Step 33: grad_norm=6.150e-01, param_norm=150.1707, delta_norm=5.521e+00, relative_change=3.6766% (9 params)
2025-11-14 17:04:25 (IST) - 0:03:48 - train - INFO - step: 000033 - done (%): 1.6 - loss: 2.589 - lr: 2.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5438.9 - avg_words_per_second: 5236.8 - ETA: >2025-11-14 20:35:42
2025-11-14 17:04:26 (IST) - 0:03:48 - train - INFO - [DocStream] step=34 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=0-1]
2025-11-14 17:04:26 (IST) - 0:03:48 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav
2025-11-14 17:04:26 (IST) - 0:03:48 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:04:26 (IST) - 0:03:48 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:04:26 (IST) - 0:03:48 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.990234; w_down is frozen during training so its norm stays constant
2025-11-14 17:04:31 (IST) - 0:03:54 - train - INFO - [TTT] Step 34: grad_norm=1.000e+00, param_norm=152.8392, delta_norm=6.394e+00, relative_change=4.1836% (9 params)
2025-11-14 17:04:32 (IST) - 0:03:54 - train - INFO - step: 000034 - done (%): 1.7 - loss: 4.183 - lr: 2.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5423.5 - avg_words_per_second: 5242.1 - ETA: >2025-11-14 20:35:29
2025-11-14 17:04:32 (IST) - 0:03:54 - train - INFO - [DocStream] step=35 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=2-3]
2025-11-14 17:04:38 (IST) - 0:04:00 - train - INFO - [TTT] Step 35: grad_norm=1.000e+00, param_norm=155.6087, delta_norm=6.658e+00, relative_change=4.2787% (9 params)
2025-11-14 17:04:38 (IST) - 0:04:00 - train - INFO - step: 000035 - done (%): 1.8 - loss: 6.882 - lr: 2.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5450.9 - avg_words_per_second: 5247.8 - ETA: >2025-11-14 20:35:15
2025-11-14 17:04:38 (IST) - 0:04:01 - train - INFO - [DocStream] step=36 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=4-5]
2025-11-14 17:04:44 (IST) - 0:04:07 - train - INFO - [TTT] Step 36: grad_norm=8.279e-01, param_norm=158.4737, delta_norm=7.038e+00, relative_change=4.4410% (9 params)
2025-11-14 17:04:44 (IST) - 0:04:07 - train - INFO - step: 000036 - done (%): 1.8 - loss: 4.036 - lr: 3.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5435.4 - avg_words_per_second: 5252.9 - ETA: >2025-11-14 20:35:03
2025-11-14 17:04:44 (IST) - 0:04:07 - train - INFO - [DocStream] step=37 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=6-7]
2025-11-14 17:04:50 (IST) - 0:04:13 - train - INFO - [TTT] Step 37: grad_norm=8.120e-01, param_norm=161.4248, delta_norm=6.850e+00, relative_change=4.2434% (9 params)
2025-11-14 17:04:50 (IST) - 0:04:13 - train - INFO - step: 000037 - done (%): 1.9 - loss: 3.700 - lr: 3.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5436.4 - avg_words_per_second: 5257.7 - ETA: >2025-11-14 20:34:51
2025-11-14 17:04:50 (IST) - 0:04:13 - train - INFO - [DocStream] step=38 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=8-9]
2025-11-14 17:04:56 (IST) - 0:04:19 - train - INFO - [TTT] Step 38: grad_norm=1.000e+00, param_norm=164.4259, delta_norm=7.230e+00, relative_change=4.3970% (9 params)
2025-11-14 17:04:56 (IST) - 0:04:19 - train - INFO - step: 000038 - done (%): 1.9 - loss: 3.628 - lr: 3.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5445.8 - avg_words_per_second: 5262.4 - ETA: >2025-11-14 20:34:39
2025-11-14 17:04:57 (IST) - 0:04:19 - train - INFO - [DocStream] step=39 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=10-11]
2025-11-14 17:05:03 (IST) - 0:04:25 - train - INFO - [TTT] Step 39: grad_norm=7.058e-01, param_norm=167.5314, delta_norm=7.326e+00, relative_change=4.3730% (9 params)
2025-11-14 17:05:03 (IST) - 0:04:25 - train - INFO - step: 000039 - done (%): 1.9 - loss: 2.856 - lr: 3.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5436.5 - avg_words_per_second: 5266.8 - ETA: >2025-11-14 20:34:29
2025-11-14 17:05:03 (IST) - 0:04:26 - train - INFO - [DocStream] step=40 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=0-1]
2025-11-14 17:05:03 (IST) - 0:04:26 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav
2025-11-14 17:05:03 (IST) - 0:04:26 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.980469; w_down is frozen during training so its norm stays constant
2025-11-14 17:05:03 (IST) - 0:04:26 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:05:03 (IST) - 0:04:26 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.988281; w_down is frozen during training so its norm stays constant
2025-11-14 17:05:09 (IST) - 0:04:31 - train - INFO - [TTT] Step 40: grad_norm=1.000e+00, param_norm=171.5895, delta_norm=8.535e+00, relative_change=4.9739% (9 params)
2025-11-14 17:05:09 (IST) - 0:04:31 - train - INFO - step: 000040 - done (%): 2.0 - loss: 4.980 - lr: 3.6e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5427.9 - avg_words_per_second: 5270.7 - ETA: >2025-11-14 20:34:19
2025-11-14 17:05:09 (IST) - 0:04:32 - train - INFO - [DocStream] step=41 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=2-3]
2025-11-14 17:05:15 (IST) - 0:04:38 - train - INFO - [TTT] Step 41: grad_norm=1.000e+00, param_norm=175.7523, delta_norm=8.604e+00, relative_change=4.8955% (9 params)
2025-11-14 17:05:15 (IST) - 0:04:38 - train - INFO - step: 000041 - done (%): 2.0 - loss: 8.164 - lr: 3.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5432.2 - avg_words_per_second: 5274.5 - ETA: >2025-11-14 20:34:10
2025-11-14 17:05:15 (IST) - 0:04:38 - train - INFO - [DocStream] step=42 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=4-5]
2025-11-14 17:05:21 (IST) - 0:04:44 - train - INFO - [TTT] Step 42: grad_norm=1.000e+00, param_norm=179.9559, delta_norm=8.771e+00, relative_change=4.8739% (9 params)
2025-11-14 17:05:21 (IST) - 0:04:44 - train - INFO - step: 000042 - done (%): 2.1 - loss: 7.354 - lr: 3.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5418.5 - avg_words_per_second: 5277.8 - ETA: >2025-11-14 20:34:02
2025-11-14 17:05:22 (IST) - 0:04:44 - train - INFO - [DocStream] step=43 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=6-7]
2025-11-14 17:05:27 (IST) - 0:04:50 - train - INFO - [TTT] Step 43: grad_norm=5.341e-01, param_norm=184.2576, delta_norm=8.633e+00, relative_change=4.6851% (9 params)
2025-11-14 17:05:27 (IST) - 0:04:50 - train - INFO - step: 000043 - done (%): 2.1 - loss: 2.689 - lr: 4.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5439.4 - avg_words_per_second: 5281.5 - ETA: >2025-11-14 20:33:53
2025-11-14 17:05:28 (IST) - 0:04:50 - train - INFO - [DocStream] step=44 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=8-9]
2025-11-14 17:05:34 (IST) - 0:04:56 - train - INFO - [TTT] Step 44: grad_norm=4.274e-01, param_norm=188.6385, delta_norm=8.518e+00, relative_change=4.5156% (9 params)
2025-11-14 17:05:34 (IST) - 0:04:56 - train - INFO - step: 000044 - done (%): 2.2 - loss: 3.525 - lr: 4.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5440.8 - avg_words_per_second: 5285.0 - ETA: >2025-11-14 20:33:45
2025-11-14 17:05:34 (IST) - 0:04:57 - train - INFO - [DocStream] step=45 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=10-11]
2025-11-14 17:05:40 (IST) - 0:05:02 - train - INFO - [TTT] Step 45: grad_norm=3.418e-01, param_norm=193.0770, delta_norm=8.335e+00, relative_change=4.3170% (9 params)
2025-11-14 17:05:40 (IST) - 0:05:02 - train - INFO - step: 000045 - done (%): 2.2 - loss: 2.958 - lr: 4.4e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5437.3 - avg_words_per_second: 5288.3 - ETA: >2025-11-14 20:33:37
2025-11-14 17:05:40 (IST) - 0:05:03 - train - INFO - [DocStream] step=46 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=0-1]
2025-11-14 17:05:40 (IST) - 0:05:03 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4156.wav
2025-11-14 17:05:40 (IST) - 0:05:03 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.976562; w_down is frozen during training so its norm stays constant
2025-11-14 17:05:40 (IST) - 0:05:03 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:05:40 (IST) - 0:05:03 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.988281; w_down is frozen during training so its norm stays constant
2025-11-14 17:05:46 (IST) - 0:05:09 - train - INFO - [TTT] Step 46: grad_norm=1.000e+00, param_norm=198.9306, delta_norm=1.049e+01, relative_change=5.2730% (9 params)
2025-11-14 17:05:46 (IST) - 0:05:09 - train - INFO - step: 000046 - done (%): 2.3 - loss: 4.402 - lr: 4.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5368.8 - avg_words_per_second: 5290.0 - ETA: >2025-11-14 20:33:32
2025-11-14 17:05:46 (IST) - 0:05:09 - train - INFO - [DocStream] step=47 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=2-3]
2025-11-14 17:05:52 (IST) - 0:05:15 - train - INFO - [TTT] Step 47: grad_norm=1.000e+00, param_norm=204.8005, delta_norm=1.046e+01, relative_change=5.1075% (9 params)
2025-11-14 17:05:52 (IST) - 0:05:15 - train - INFO - step: 000047 - done (%): 2.4 - loss: 12.657 - lr: 4.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5434.6 - avg_words_per_second: 5293.0 - ETA: >2025-11-14 20:33:25
2025-11-14 17:05:53 (IST) - 0:05:15 - train - INFO - [DocStream] step=48 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=4-5]
2025-11-14 17:05:59 (IST) - 0:05:21 - train - INFO - [TTT] Step 48: grad_norm=1.000e+00, param_norm=210.6219, delta_norm=1.089e+01, relative_change=5.1691% (9 params)
2025-11-14 17:05:59 (IST) - 0:05:21 - train - INFO - step: 000048 - done (%): 2.4 - loss: 8.801 - lr: 4.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5429.4 - avg_words_per_second: 5295.8 - ETA: >2025-11-14 20:33:19
2025-11-14 17:05:59 (IST) - 0:05:22 - train - INFO - [DocStream] step=49 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=6-7]
2025-11-14 17:06:05 (IST) - 0:05:27 - train - INFO - [TTT] Step 49: grad_norm=1.000e+00, param_norm=216.4421, delta_norm=1.114e+01, relative_change=5.1473% (9 params)
2025-11-14 17:06:05 (IST) - 0:05:27 - train - INFO - step: 000049 - done (%): 2.5 - loss: 8.001 - lr: 5.0e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5422.1 - avg_words_per_second: 5298.3 - ETA: >2025-11-14 20:33:13
2025-11-14 17:06:05 (IST) - 0:05:28 - train - INFO - [DocStream] step=50 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=8-9]
2025-11-14 17:06:11 (IST) - 0:05:34 - train - INFO - [TTT] Step 50: grad_norm=1.000e+00, param_norm=222.1426, delta_norm=1.103e+01, relative_change=4.9675% (9 params)
2025-11-14 17:06:11 (IST) - 0:05:34 - train - INFO - step: 000050 - done (%): 2.5 - loss: 7.881 - lr: 5.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5515.7 - avg_words_per_second: 5302.5 - ETA: >2025-11-14 20:33:02
2025-11-14 17:06:11 (IST) - 0:05:34 - train - INFO - [DocStream] step=51 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=0-1]
2025-11-14 17:06:11 (IST) - 0:05:34 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4156.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4157.wav
2025-11-14 17:06:11 (IST) - 0:05:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.976562; w_down is frozen during training so its norm stays constant
2025-11-14 17:06:11 (IST) - 0:05:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.976562; w_down is frozen during training so its norm stays constant
2025-11-14 17:06:11 (IST) - 0:05:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.988281; w_down is frozen during training so its norm stays constant
2025-11-14 17:06:17 (IST) - 0:05:40 - train - INFO - [TTT] Step 51: grad_norm=1.000e+00, param_norm=230.0285, delta_norm=1.360e+01, relative_change=5.9129% (9 params)
2025-11-14 17:06:17 (IST) - 0:05:40 - train - INFO - step: 000051 - done (%): 2.5 - loss: 4.900 - lr: 5.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5415.5 - avg_words_per_second: 5304.7 - ETA: >2025-11-14 20:32:57
2025-11-14 17:06:17 (IST) - 0:05:40 - train - INFO - [DocStream] step=52 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=2-3]
2025-11-14 17:06:23 (IST) - 0:05:46 - train - INFO - [TTT] Step 52: grad_norm=1.000e+00, param_norm=237.8170, delta_norm=1.333e+01, relative_change=5.6058% (9 params)
2025-11-14 17:06:23 (IST) - 0:05:46 - train - INFO - step: 000052 - done (%): 2.6 - loss: 12.894 - lr: 5.4e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5437.2 - avg_words_per_second: 5307.2 - ETA: >2025-11-14 20:32:51
2025-11-14 17:06:24 (IST) - 0:05:46 - train - INFO - [DocStream] step=53 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=4-5]
2025-11-14 17:06:29 (IST) - 0:05:52 - train - INFO - [TTT] Step 53: grad_norm=1.000e+00, param_norm=245.3519, delta_norm=1.373e+01, relative_change=5.5948% (9 params)
2025-11-14 17:06:29 (IST) - 0:05:52 - train - INFO - step: 000053 - done (%): 2.6 - loss: 13.424 - lr: 5.6e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5550.9 - avg_words_per_second: 5311.6 - ETA: >2025-11-14 20:32:41
2025-11-14 17:06:30 (IST) - 0:05:52 - train - INFO - [DocStream] step=54 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=0-1]
2025-11-14 17:06:30 (IST) - 0:05:52 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4157.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4170.wav
2025-11-14 17:06:30 (IST) - 0:05:52 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.974609; w_down is frozen during training so its norm stays constant
2025-11-14 17:06:30 (IST) - 0:05:52 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.976562; w_down is frozen during training so its norm stays constant
2025-11-14 17:06:30 (IST) - 0:05:52 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.988281; w_down is frozen during training so its norm stays constant
2025-11-14 17:06:36 (IST) - 0:05:58 - train - INFO - [TTT] Step 54: grad_norm=1.000e+00, param_norm=255.0258, delta_norm=1.656e+01, relative_change=6.4944% (9 params)
2025-11-14 17:06:36 (IST) - 0:05:58 - train - INFO - step: 000054 - done (%): 2.7 - loss: 4.466 - lr: 5.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5432.5 - avg_words_per_second: 5313.7 - ETA: >2025-11-14 20:32:36
2025-11-14 17:06:36 (IST) - 0:05:59 - train - INFO - [DocStream] step=55 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=2-3]
2025-11-14 17:06:42 (IST) - 0:06:05 - train - INFO - [TTT] Step 55: grad_norm=1.000e+00, param_norm=264.5419, delta_norm=1.673e+01, relative_change=6.3237% (9 params)
2025-11-14 17:06:42 (IST) - 0:06:05 - train - INFO - step: 000055 - done (%): 2.8 - loss: 10.674 - lr: 5.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5433.1 - avg_words_per_second: 5315.9 - ETA: >2025-11-14 20:32:30
2025-11-14 17:06:42 (IST) - 0:06:05 - train - INFO - [DocStream] step=56 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=4-5]
2025-11-14 17:06:48 (IST) - 0:06:11 - train - INFO - [TTT] Step 56: grad_norm=1.000e+00, param_norm=273.9029, delta_norm=1.578e+01, relative_change=5.7627% (9 params)
2025-11-14 17:06:48 (IST) - 0:06:11 - train - INFO - step: 000056 - done (%): 2.8 - loss: 10.695 - lr: 6.0e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5436.8 - avg_words_per_second: 5318.0 - ETA: >2025-11-14 20:32:25
2025-11-14 17:06:48 (IST) - 0:06:11 - train - INFO - [DocStream] step=57 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=6-7]
2025-11-14 17:06:54 (IST) - 0:06:17 - train - INFO - [TTT] Step 57: grad_norm=1.000e+00, param_norm=283.0170, delta_norm=1.573e+01, relative_change=5.5588% (9 params)
2025-11-14 17:06:54 (IST) - 0:06:17 - train - INFO - step: 000057 - done (%): 2.9 - loss: 8.665 - lr: 6.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5430.0 - avg_words_per_second: 5319.9 - ETA: >2025-11-14 20:32:21
2025-11-14 17:06:55 (IST) - 0:06:17 - train - INFO - [DocStream] step=58 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=8-9]
2025-11-14 17:07:01 (IST) - 0:06:23 - train - INFO - [TTT] Step 58: grad_norm=1.000e+00, param_norm=291.7615, delta_norm=1.447e+01, relative_change=4.9603% (9 params)
2025-11-14 17:07:01 (IST) - 0:06:23 - train - INFO - step: 000058 - done (%): 2.9 - loss: 9.159 - lr: 6.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5411.5 - avg_words_per_second: 5321.5 - ETA: >2025-11-14 20:32:17
2025-11-14 17:07:01 (IST) - 0:06:23 - train - INFO - [DocStream] step=59 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=10-11]
2025-11-14 17:07:07 (IST) - 0:06:29 - train - INFO - [TTT] Step 59: grad_norm=8.445e-01, param_norm=300.2182, delta_norm=1.478e+01, relative_change=4.9241% (9 params)
2025-11-14 17:07:07 (IST) - 0:06:29 - train - INFO - step: 000059 - done (%): 3.0 - loss: 8.339 - lr: 6.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5432.0 - avg_words_per_second: 5323.3 - ETA: >2025-11-14 20:32:13
2025-11-14 17:07:07 (IST) - 0:06:30 - train - INFO - [DocStream] step=60 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=0-1]
2025-11-14 17:07:07 (IST) - 0:06:30 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4170.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4183.wav
2025-11-14 17:07:07 (IST) - 0:06:30 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.970703; w_down is frozen during training so its norm stays constant
2025-11-14 17:07:07 (IST) - 0:06:30 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.972656; w_down is frozen during training so its norm stays constant
2025-11-14 17:07:07 (IST) - 0:06:30 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:07:13 (IST) - 0:06:36 - train - INFO - [TTT] Step 60: grad_norm=1.000e+00, param_norm=311.4358, delta_norm=1.763e+01, relative_change=5.6613% (9 params)
2025-11-14 17:07:13 (IST) - 0:06:36 - train - INFO - step: 000060 - done (%): 3.0 - loss: 4.430 - lr: 6.6e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5425.1 - avg_words_per_second: 5325.0 - ETA: >2025-11-14 20:32:09
2025-11-14 17:07:13 (IST) - 0:06:36 - train - INFO - [DocStream] step=61 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=2-3]
2025-11-14 17:07:19 (IST) - 0:06:42 - train - INFO - [TTT] Step 61: grad_norm=1.000e+00, param_norm=322.0825, delta_norm=1.688e+01, relative_change=5.2410% (9 params)
2025-11-14 17:07:19 (IST) - 0:06:42 - train - INFO - step: 000061 - done (%): 3.0 - loss: 15.539 - lr: 6.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5433.4 - avg_words_per_second: 5326.7 - ETA: >2025-11-14 20:32:05
2025-11-14 17:07:19 (IST) - 0:06:42 - train - INFO - [DocStream] step=62 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=4-5]
2025-11-14 17:07:25 (IST) - 0:06:48 - train - INFO - [TTT] Step 62: grad_norm=8.063e-01, param_norm=332.3367, delta_norm=1.730e+01, relative_change=5.2044% (9 params)
2025-11-14 17:07:25 (IST) - 0:06:48 - train - INFO - step: 000062 - done (%): 3.1 - loss: 9.610 - lr: 6.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5427.7 - avg_words_per_second: 5328.3 - ETA: >2025-11-14 20:32:01
2025-11-14 17:07:26 (IST) - 0:06:48 - train - INFO - [DocStream] step=63 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=6-7]
2025-11-14 17:07:32 (IST) - 0:06:54 - train - INFO - [TTT] Step 63: grad_norm=1.000e+00, param_norm=342.1136, delta_norm=1.593e+01, relative_change=4.6568% (9 params)
2025-11-14 17:07:32 (IST) - 0:06:54 - train - INFO - step: 000063 - done (%): 3.1 - loss: 9.795 - lr: 7.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5429.1 - avg_words_per_second: 5329.9 - ETA: >2025-11-14 20:31:57
2025-11-14 17:07:32 (IST) - 0:06:55 - train - INFO - [DocStream] step=64 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=8-9]
2025-11-14 17:07:38 (IST) - 0:07:00 - train - INFO - [TTT] Step 64: grad_norm=5.270e-01, param_norm=351.4240, delta_norm=1.556e+01, relative_change=4.4265% (9 params)
2025-11-14 17:07:38 (IST) - 0:07:00 - train - INFO - step: 000064 - done (%): 3.2 - loss: 7.668 - lr: 7.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5429.6 - avg_words_per_second: 5331.4 - ETA: >2025-11-14 20:31:54
2025-11-14 17:07:38 (IST) - 0:07:01 - train - INFO - [DocStream] step=65 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=10-11]
2025-11-14 17:07:44 (IST) - 0:07:07 - train - INFO - [TTT] Step 65: grad_norm=1.000e+00, param_norm=359.9974, delta_norm=1.531e+01, relative_change=4.2537% (9 params)
2025-11-14 17:07:44 (IST) - 0:07:07 - train - INFO - step: 000065 - done (%): 3.2 - loss: 9.460 - lr: 7.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5431.9 - avg_words_per_second: 5332.9 - ETA: >2025-11-14 20:31:50
2025-11-14 17:07:44 (IST) - 0:07:07 - train - INFO - [DocStream] step=66 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=0-1]
2025-11-14 17:07:44 (IST) - 0:07:07 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4183.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4184.wav
2025-11-14 17:07:44 (IST) - 0:07:07 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.968750; w_down is frozen during training so its norm stays constant
2025-11-14 17:07:44 (IST) - 0:07:07 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.970703; w_down is frozen during training so its norm stays constant
2025-11-14 17:07:44 (IST) - 0:07:07 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.982422; w_down is frozen during training so its norm stays constant
2025-11-14 17:07:50 (IST) - 0:07:13 - train - INFO - [TTT] Step 66: grad_norm=1.000e+00, param_norm=373.0470, delta_norm=2.028e+01, relative_change=5.4357% (9 params)
2025-11-14 17:07:50 (IST) - 0:07:13 - train - INFO - step: 000066 - done (%): 3.3 - loss: 4.736 - lr: 7.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5425.0 - avg_words_per_second: 5334.3 - ETA: >2025-11-14 20:31:47
2025-11-14 17:07:51 (IST) - 0:07:13 - train - INFO - [DocStream] step=67 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=2-3]
2025-11-14 17:07:56 (IST) - 0:07:19 - train - INFO - [TTT] Step 67: grad_norm=9.662e-01, param_norm=385.3469, delta_norm=1.917e+01, relative_change=4.9742% (9 params)
2025-11-14 17:07:56 (IST) - 0:07:19 - train - INFO - step: 000067 - done (%): 3.4 - loss: 10.739 - lr: 7.6e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5431.9 - avg_words_per_second: 5335.7 - ETA: >2025-11-14 20:31:43
2025-11-14 17:07:57 (IST) - 0:07:19 - train - INFO - [DocStream] step=68 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=4-5]
2025-11-14 17:08:03 (IST) - 0:07:25 - train - INFO - [TTT] Step 68: grad_norm=3.024e-01, param_norm=397.0131, delta_norm=1.817e+01, relative_change=4.5757% (9 params)
2025-11-14 17:08:03 (IST) - 0:07:25 - train - INFO - step: 000068 - done (%): 3.4 - loss: 6.328 - lr: 7.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5442.0 - avg_words_per_second: 5337.3 - ETA: >2025-11-14 20:31:40
2025-11-14 17:08:03 (IST) - 0:07:26 - train - INFO - [DocStream] step=69 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=6-7]
2025-11-14 17:08:09 (IST) - 0:07:32 - train - INFO - [TTT] Step 69: grad_norm=3.810e-01, param_norm=407.9449, delta_norm=1.707e+01, relative_change=4.1833% (9 params)
2025-11-14 17:08:09 (IST) - 0:07:32 - train - INFO - step: 000069 - done (%): 3.5 - loss: 5.289 - lr: 7.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5432.0 - avg_words_per_second: 5338.6 - ETA: >2025-11-14 20:31:36
2025-11-14 17:08:09 (IST) - 0:07:32 - train - INFO - [DocStream] step=70 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=8-9]
2025-11-14 17:08:15 (IST) - 0:07:38 - train - INFO - [TTT] Step 70: grad_norm=2.035e-01, param_norm=418.1712, delta_norm=1.576e+01, relative_change=3.7686% (9 params)
2025-11-14 17:08:15 (IST) - 0:07:38 - train - INFO - step: 000070 - done (%): 3.5 - loss: 4.905 - lr: 8.0e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5423.3 - avg_words_per_second: 5339.8 - ETA: >2025-11-14 20:31:34
2025-11-14 17:08:15 (IST) - 0:07:38 - train - INFO - [DocStream] step=71 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=10-11]
2025-11-14 17:08:21 (IST) - 0:07:44 - train - INFO - [TTT] Step 71: grad_norm=3.105e-01, param_norm=427.6413, delta_norm=1.497e+01, relative_change=3.5006% (9 params)
2025-11-14 17:08:21 (IST) - 0:07:44 - train - INFO - step: 000071 - done (%): 3.5 - loss: 3.754 - lr: 8.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5435.5 - avg_words_per_second: 5341.1 - ETA: >2025-11-14 20:31:30
2025-11-14 17:08:22 (IST) - 0:07:44 - train - INFO - [DocStream] step=72 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=0-1]
2025-11-14 17:08:22 (IST) - 0:07:44 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4184.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4234.wav
2025-11-14 17:08:22 (IST) - 0:07:44 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.964844; w_down is frozen during training so its norm stays constant
2025-11-14 17:08:22 (IST) - 0:07:44 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.966797; w_down is frozen during training so its norm stays constant
2025-11-14 17:08:22 (IST) - 0:07:44 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.976562; w_down is frozen during training so its norm stays constant
2025-11-14 17:08:28 (IST) - 0:07:50 - train - INFO - [TTT] Step 72: grad_norm=1.000e+00, param_norm=443.0276, delta_norm=2.216e+01, relative_change=5.0025% (9 params)
2025-11-14 17:08:28 (IST) - 0:07:50 - train - INFO - step: 000072 - done (%): 3.6 - loss: 4.725 - lr: 8.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5390.8 - avg_words_per_second: 5341.8 - ETA: >2025-11-14 20:31:29
2025-11-14 17:08:28 (IST) - 0:07:51 - train - INFO - [DocStream] step=73 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=2-3]
2025-11-14 17:08:34 (IST) - 0:07:56 - train - INFO - [TTT] Step 73: grad_norm=1.000e+00, param_norm=457.0699, delta_norm=2.224e+01, relative_change=4.8652% (9 params)
2025-11-14 17:08:34 (IST) - 0:07:56 - train - INFO - step: 000073 - done (%): 3.6 - loss: 14.820 - lr: 8.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5429.6 - avg_words_per_second: 5343.0 - ETA: >2025-11-14 20:31:26
2025-11-14 17:08:34 (IST) - 0:07:57 - train - INFO - [DocStream] step=74 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=4-5]
2025-11-14 17:08:40 (IST) - 0:08:03 - train - INFO - [TTT] Step 74: grad_norm=4.365e-01, param_norm=470.1163, delta_norm=2.018e+01, relative_change=4.2926% (9 params)
2025-11-14 17:08:40 (IST) - 0:08:03 - train - INFO - step: 000074 - done (%): 3.7 - loss: 7.326 - lr: 8.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5439.5 - avg_words_per_second: 5344.3 - ETA: >2025-11-14 20:31:23
2025-11-14 17:08:40 (IST) - 0:08:03 - train - INFO - [DocStream] step=75 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=6-7]
2025-11-14 17:08:46 (IST) - 0:08:09 - train - INFO - [TTT] Step 75: grad_norm=2.216e-01, param_norm=482.2125, delta_norm=1.845e+01, relative_change=3.8265% (9 params)
2025-11-14 17:08:46 (IST) - 0:08:09 - train - INFO - step: 000075 - done (%): 3.8 - loss: 4.446 - lr: 8.6e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5440.5 - avg_words_per_second: 5345.5 - ETA: >2025-11-14 20:31:20
2025-11-14 17:08:47 (IST) - 0:08:09 - train - INFO - [DocStream] step=76 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=8-9]
2025-11-14 17:08:52 (IST) - 0:08:15 - train - INFO - [TTT] Step 76: grad_norm=2.739e-01, param_norm=493.3121, delta_norm=1.718e+01, relative_change=3.4818% (9 params)
2025-11-14 17:08:52 (IST) - 0:08:15 - train - INFO - step: 000076 - done (%): 3.8 - loss: 5.125 - lr: 8.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5432.7 - avg_words_per_second: 5346.7 - ETA: >2025-11-14 20:31:17
2025-11-14 17:08:53 (IST) - 0:08:15 - train - INFO - [DocStream] step=77 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=10-11]
2025-11-14 17:08:59 (IST) - 0:08:21 - train - INFO - [TTT] Step 77: grad_norm=1.620e-01, param_norm=503.4900, delta_norm=1.573e+01, relative_change=3.1232% (9 params)
2025-11-14 17:08:59 (IST) - 0:08:21 - train - INFO - step: 000077 - done (%): 3.9 - loss: 4.341 - lr: 8.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5442.3 - avg_words_per_second: 5347.9 - ETA: >2025-11-14 20:31:15
2025-11-14 17:08:59 (IST) - 0:08:22 - train - INFO - [DocStream] step=78 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=0-1]
2025-11-14 17:08:59 (IST) - 0:08:22 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4234.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4245.wav
2025-11-14 17:08:59 (IST) - 0:08:22 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.958984; w_down is frozen during training so its norm stays constant
2025-11-14 17:08:59 (IST) - 0:08:22 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.960938; w_down is frozen during training so its norm stays constant
2025-11-14 17:08:59 (IST) - 0:08:22 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.972656; w_down is frozen during training so its norm stays constant
2025-11-14 17:09:05 (IST) - 0:08:28 - train - INFO - [TTT] Step 78: grad_norm=1.000e+00, param_norm=520.7322, delta_norm=2.577e+01, relative_change=4.9487% (9 params)
2025-11-14 17:09:05 (IST) - 0:08:28 - train - INFO - step: 000078 - done (%): 3.9 - loss: 5.301 - lr: 8.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5415.0 - avg_words_per_second: 5348.7 - ETA: >2025-11-14 20:31:13
2025-11-14 17:09:05 (IST) - 0:08:28 - train - INFO - [DocStream] step=79 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=2-3]
2025-11-14 17:09:11 (IST) - 0:08:34 - train - INFO - [TTT] Step 79: grad_norm=1.000e+00, param_norm=536.5318, delta_norm=2.593e+01, relative_change=4.8321% (9 params)
2025-11-14 17:09:11 (IST) - 0:08:34 - train - INFO - step: 000079 - done (%): 4.0 - loss: 13.224 - lr: 9.0e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5430.1 - avg_words_per_second: 5349.7 - ETA: >2025-11-14 20:31:10
2025-11-14 17:09:11 (IST) - 0:08:34 - train - INFO - [DocStream] step=80 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=4-5]
2025-11-14 17:09:17 (IST) - 0:08:40 - train - INFO - [TTT] Step 80: grad_norm=1.000e+00, param_norm=550.7594, delta_norm=2.377e+01, relative_change=4.3156% (9 params)
2025-11-14 17:09:17 (IST) - 0:08:40 - train - INFO - step: 000080 - done (%): 4.0 - loss: 8.462 - lr: 9.1e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5436.0 - avg_words_per_second: 5350.8 - ETA: >2025-11-14 20:31:08
2025-11-14 17:09:18 (IST) - 0:08:40 - train - INFO - [DocStream] step=81 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=6-7]
2025-11-14 17:09:24 (IST) - 0:08:46 - train - INFO - [TTT] Step 81: grad_norm=1.632e-01, param_norm=563.8717, delta_norm=2.129e+01, relative_change=3.7766% (9 params)
2025-11-14 17:09:24 (IST) - 0:08:46 - train - INFO - step: 000081 - done (%): 4.0 - loss: 4.756 - lr: 9.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5433.8 - avg_words_per_second: 5351.8 - ETA: >2025-11-14 20:31:05
2025-11-14 17:09:24 (IST) - 0:08:46 - train - INFO - [DocStream] step=82 microbatch=0 samples=2 unique_docs=2 runs=4245.wav[segment=8], 4247.wav[segment=0]
2025-11-14 17:09:24 (IST) - 0:08:46 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4245.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4247.wav
2025-11-14 17:09:24 (IST) - 0:08:46 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.957031; w_down is frozen during training so its norm stays constant
2025-11-14 17:09:24 (IST) - 0:08:46 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.960938; w_down is frozen during training so its norm stays constant
2025-11-14 17:09:24 (IST) - 0:08:46 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.970703; w_down is frozen during training so its norm stays constant
2025-11-14 17:09:30 (IST) - 0:08:52 - train - INFO - [TTT] Step 82: grad_norm=1.000e+00, param_norm=582.3226, delta_norm=3.146e+01, relative_change=5.4030% (9 params)
2025-11-14 17:09:30 (IST) - 0:08:52 - train - INFO - step: 000082 - done (%): 4.1 - loss: 4.169 - lr: 9.2e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5470.5 - avg_words_per_second: 5353.2 - ETA: >2025-11-14 20:31:02
2025-11-14 17:09:30 (IST) - 0:08:53 - train - INFO - [DocStream] step=83 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=1-2]
2025-11-14 17:09:36 (IST) - 0:08:59 - train - INFO - [TTT] Step 83: grad_norm=4.132e-01, param_norm=599.4607, delta_norm=2.847e+01, relative_change=4.7498% (9 params)
2025-11-14 17:09:36 (IST) - 0:08:59 - train - INFO - step: 000083 - done (%): 4.2 - loss: 9.821 - lr: 9.3e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5439.6 - avg_words_per_second: 5354.3 - ETA: >2025-11-14 20:31:00
2025-11-14 17:09:36 (IST) - 0:08:59 - train - INFO - [DocStream] step=84 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=3-4]
2025-11-14 17:09:42 (IST) - 0:09:05 - train - INFO - [TTT] Step 84: grad_norm=2.716e-01, param_norm=615.2916, delta_norm=2.551e+01, relative_change=4.1466% (9 params)
2025-11-14 17:09:42 (IST) - 0:09:05 - train - INFO - step: 000084 - done (%): 4.2 - loss: 7.578 - lr: 9.4e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5434.4 - avg_words_per_second: 5355.2 - ETA: >2025-11-14 20:30:57
2025-11-14 17:09:42 (IST) - 0:09:05 - train - INFO - [DocStream] step=85 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=5-6]
2025-11-14 17:09:48 (IST) - 0:09:11 - train - INFO - [TTT] Step 85: grad_norm=1.843e-01, param_norm=629.7472, delta_norm=2.314e+01, relative_change=3.6746% (9 params)
2025-11-14 17:09:48 (IST) - 0:09:11 - train - INFO - step: 000085 - done (%): 4.2 - loss: 4.251 - lr: 9.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5432.6 - avg_words_per_second: 5356.1 - ETA: >2025-11-14 20:30:55
2025-11-14 17:09:49 (IST) - 0:09:11 - train - INFO - [DocStream] step=86 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=7-8]
2025-11-14 17:09:55 (IST) - 0:09:17 - train - INFO - [TTT] Step 86: grad_norm=3.484e-01, param_norm=642.7195, delta_norm=2.145e+01, relative_change=3.3373% (9 params)
2025-11-14 17:09:55 (IST) - 0:09:17 - train - INFO - step: 000086 - done (%): 4.3 - loss: 4.890 - lr: 9.5e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5430.0 - avg_words_per_second: 5356.9 - ETA: >2025-11-14 20:30:53
2025-11-14 17:09:55 (IST) - 0:09:18 - train - INFO - [DocStream] step=87 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=9-10]
2025-11-14 17:10:01 (IST) - 0:09:23 - train - INFO - [TTT] Step 87: grad_norm=2.496e-01, param_norm=654.4667, delta_norm=1.859e+01, relative_change=2.8408% (9 params)
2025-11-14 17:10:01 (IST) - 0:09:23 - train - INFO - step: 000087 - done (%): 4.3 - loss: 4.812 - lr: 9.6e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5437.7 - avg_words_per_second: 5357.9 - ETA: >2025-11-14 20:30:51
2025-11-14 17:10:01 (IST) - 0:09:24 - train - INFO - [DocStream] step=88 microbatch=0 samples=2 unique_docs=2 runs=4247.wav[segment=11], 4248.wav[segment=0]
2025-11-14 17:10:01 (IST) - 0:09:24 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4247.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4248.wav
2025-11-14 17:10:01 (IST) - 0:09:24 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.951172; w_down is frozen during training so its norm stays constant
2025-11-14 17:10:01 (IST) - 0:09:24 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.955078; w_down is frozen during training so its norm stays constant
2025-11-14 17:10:01 (IST) - 0:09:24 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator total norm 0.964844; w_down is frozen during training so its norm stays constant
2025-11-14 17:10:07 (IST) - 0:09:30 - train - INFO - [TTT] Step 88: grad_norm=1.000e+00, param_norm=675.1536, delta_norm=3.160e+01, relative_change=4.6801% (9 params)
2025-11-14 17:10:07 (IST) - 0:09:30 - train - INFO - step: 000088 - done (%): 4.4 - loss: 4.109 - lr: 9.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5426.0 - avg_words_per_second: 5358.6 - ETA: >2025-11-14 20:30:49
2025-11-14 17:10:07 (IST) - 0:09:30 - train - INFO - [DocStream] step=89 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=1-2]
2025-11-14 17:10:13 (IST) - 0:09:36 - train - INFO - [TTT] Step 89: grad_norm=4.788e-01, param_norm=693.8538, delta_norm=2.851e+01, relative_change=4.1092% (9 params)
2025-11-14 17:10:13 (IST) - 0:09:36 - train - INFO - step: 000089 - done (%): 4.5 - loss: 8.820 - lr: 9.7e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5439.5 - avg_words_per_second: 5359.5 - ETA: >2025-11-14 20:30:47
2025-11-14 17:10:14 (IST) - 0:09:36 - train - INFO - [DocStream] step=90 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=3-4]
2025-11-14 17:10:19 (IST) - 0:09:42 - train - INFO - [TTT] Step 90: grad_norm=2.022e-01, param_norm=710.7263, delta_norm=2.548e+01, relative_change=3.5850% (9 params)
2025-11-14 17:10:19 (IST) - 0:09:42 - train - INFO - step: 000090 - done (%): 4.5 - loss: 7.192 - lr: 9.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5435.8 - avg_words_per_second: 5360.4 - ETA: >2025-11-14 20:30:45
2025-11-14 17:10:20 (IST) - 0:09:42 - train - INFO - [DocStream] step=91 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=5-6]
2025-11-14 17:10:26 (IST) - 0:09:48 - train - INFO - [TTT] Step 91: grad_norm=1.997e-01, param_norm=725.8419, delta_norm=2.268e+01, relative_change=3.1244% (9 params)
2025-11-14 17:10:26 (IST) - 0:09:48 - train - INFO - step: 000091 - done (%): 4.5 - loss: 4.414 - lr: 9.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5446.0 - avg_words_per_second: 5361.3 - ETA: >2025-11-14 20:30:43
2025-11-14 17:10:26 (IST) - 0:09:49 - train - INFO - [DocStream] step=92 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=7-8]
2025-11-14 17:10:32 (IST) - 0:09:54 - train - INFO - [TTT] Step 92: grad_norm=1.722e-01, param_norm=739.3476, delta_norm=2.019e+01, relative_change=2.7302% (9 params)
2025-11-14 17:10:32 (IST) - 0:09:54 - train - INFO - step: 000092 - done (%): 4.6 - loss: 3.779 - lr: 9.8e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5447.2 - avg_words_per_second: 5362.2 - ETA: >2025-11-14 20:30:41
2025-11-14 17:10:32 (IST) - 0:09:55 - train - INFO - [DocStream] step=93 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=9-10]
2025-11-14 17:10:38 (IST) - 0:10:01 - train - INFO - [TTT] Step 93: grad_norm=1.390e-01, param_norm=751.3444, delta_norm=1.787e+01, relative_change=2.3790% (9 params)
2025-11-14 17:10:38 (IST) - 0:10:01 - train - INFO - step: 000093 - done (%): 4.7 - loss: 4.363 - lr: 9.9e-03 - peak_alloc_mem (GB): 39.3 - alloc_mem (GB): 21.0 - words_per_second: 5447.8 - avg_words_per_second: 5363.1 - ETA: >2025-11-14 20:30:39
2025-11-14 17:10:38 (IST) - 0:10:01 - utils - INFO - Closing: eval_logger
2025-11-14 17:10:39 (IST) - 0:10:02 - utils - ERROR - Error while closing eval_logger!
2025-11-14 17:10:39 (IST) - 0:10:02 - utils - INFO - Closing: metrics_logger
2025-11-14 17:10:39 (IST) - 0:10:02 - utils - ERROR - Error while closing metrics_logger!
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mrun2[0m at: [34mhttps://wandb.ai/alufr-ben-gurion-university-of-the-negev/moshi_in_place/runs/y1jt5bmr[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../sise/eliyanac-group/ron_al/ttt_training_run2/wandb/run-20251114_170043-y1jt5bmr/logs[0m
==================================================
Job finished at: Fri 14 Nov 2025 17:10:42 IST
==================================================
