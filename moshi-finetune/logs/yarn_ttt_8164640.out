==================================================
Job started at: Fri 14 Nov 2025 17:31:56 IST
Running on node: ise-6000-06.auth.ad.bgu.ac.il
GPU info:
Fri Nov 14 17:31:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:24:00.0 Off |                  Off |
| 30%   27C    P8             19W /  300W |       2MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==================================================
Set CUDA_VISIBLE_DEVICES=0
Starting YARN + TTT training...
Context Extension: 4x (3000 -> 12000 tokens)
TTT Layers: 3 (layers 10, 20, 30)
Base Model: Frozen (only TTT params trained)
Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.
2025-11-14 17:32:02 (IST) - 0:00:03 - distributed - INFO - torch.cuda.device_count: 1
2025-11-14 17:32:02 (IST) - 0:00:03 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0
2025-11-14 17:32:02 (IST) - 0:00:03 - distributed - INFO - local rank: 0
2025-11-14 17:32:02 (IST) - 0:00:03 - train - INFO - Going to init comms...
2025-11-14 17:32:02 (IST) - 0:00:03 - train - INFO - Run dir: /sise/eliyanac-group/ron_al/ttt_training_run2
2025-11-14 17:32:02 (IST) - 0:00:03 - train - INFO - Removing run dir /sise/eliyanac-group/ron_al/ttt_training_run2...
2025-11-14 17:32:03 (IST) - 0:00:04 - train - INFO - TrainArgs: {'batch_size': 2,
 'ckpt_freq': 100,
 'data': {'eval_data': '',
          'shuffle': False,
          'train_data': '/sise/eliyanac-group/ron_al/talkbank_callhome_english/talkbank.jsonl'},
 'do_ckpt': True,
 'do_eval': False,
 'duration_sec': 150.0,
 'eval_freq': 100,
 'first_codebook_weight_multiplier': 100.0,
 'full_finetuning': False,
 'gradient_checkpointing': True,
 'log_freq': 1,
 'lora': {'enable': False, 'ft_embed': False, 'rank': 64, 'scaling': 2.0},
 'max_norm': 1.0,
 'max_steps': 2000,
 'moshi_paths': {'config_path': None,
                 'hf_repo_id': 'kyutai/moshiko-pytorch-bf16',
                 'mimi_path': None,
                 'moshi_path': None,
                 'tokenizer_path': None},
 'num_ckpt_keep': 3,
 'num_microbatches': 1,
 'optim': {'lr': 0.0001, 'pct_start': 0.05, 'weight_decay': 0.1},
 'overwrite_run_dir': True,
 'param_dtype': 'bfloat16',
 'run_dir': '/sise/eliyanac-group/ron_al/ttt_training_run2',
 'save_adapters': True,
 'seed': 0,
 'text_padding_weight': 0.5,
 'ttt': {'chunk_size': 256,
         'conv_kernel_size': 2,
         'delta_clip_fro_norm': 100.0,
         'enabled': True,
         'layer_frequency': 10,
         'learning_rate': 0.0001,
         'start_layer': 10,
         'unfreeze_ttt_layers': False},
 'wandb': {'key': '',
           'offline': False,
           'project': 'moshi_in_place',
           'run_name': 'run2'},
 'world_size': 1,
 'yarn': {'beta_fast': 32,
          'beta_slow': 1,
          'enabled': True,
          'mscale': 1.0,
          'mscale_all_dim': 0.0,
          'original_max_seq_len': 3000,
          'scale': 4.0}}
2025-11-14 17:32:03 (IST) - 0:00:04 - metrics_logger - INFO - initializing wandb
2025-11-14 17:32:05 (IST) - 0:00:06 - train - INFO - Loading Mimi and Moshi...
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - TTT (Test-Time Training) ENABLED
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Layer frequency: 10
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Start layer: 10
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Chunk size: 256
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Learning rate: 0.0001
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Conv kernel: 2
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - YaRN (Context Window Extension) ENABLED
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Scale: 4.0x
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Original max seq len: 3000
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Beta fast: 32
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   Beta slow: 1
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - ======================================================================
[YaRN] Enabled with scale=4.0, original_len=3000
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[YaRN] Initializing RoPE buffers on device=meta
[YaRN] RoPE buffers initialized successfully
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - Initializing TTT w_down from pretrained checkpoint...
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   ✓ transformer.layers.10.gating.w_down <- transformer.layers.10.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   ✓ transformer.layers.20.gating.w_down <- transformer.layers.20.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO -   ✓ transformer.layers.30.gating.w_down <- transformer.layers.30.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:32:06 (IST) - 0:00:07 - finetune.wrapped_model - INFO - Initializing TTT layers ...
2025-11-14 17:32:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:32:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:32:07 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.10.gating.w_down_pretrained from w_down
2025-11-14 17:32:07 (IST) - 0:00:08 - finetune.wrapped_model - WARNING - Buffer transformer.layers.10.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:32:08 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:32:08 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:32:08 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.20.gating.w_down_pretrained from w_down
2025-11-14 17:32:08 (IST) - 0:00:09 - finetune.wrapped_model - WARNING - Buffer transformer.layers.20.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.30.gating.w_down_pretrained from w_down
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - WARNING - Buffer transformer.layers.30.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO - Initializing YaRN RoPE buffers ...
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.rope.inv_freq (shape: torch.Size([64]), scale: 4.0x)
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO - Pinned 3 w_down parameters to float32 for TTT precision
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO - Finished initialization!
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO - TTT ACTIVE: 3 layers enabled
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO - TTT layer indices: [10, 20, 30]
2025-11-14 17:32:09 (IST) - 0:00:10 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:32:14 (IST) - 0:00:15 - train - INFO - [DocStream] step=1 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=0-1]
2025-11-14 17:32:14 (IST) - 0:00:15 - train - INFO - [TTT RESET] Document switch detected: None -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav
2025-11-14 17:32:14 (IST) - 0:00:15 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:32:14 (IST) - 0:00:15 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:32:14 (IST) - 0:00:15 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.366987
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.483812
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.794510
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   requires_grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   has grad: True
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:22 (IST) - 0:00:23 - train - INFO - =========================

2025-11-14 17:32:23 (IST) - 0:00:23 - train - INFO - [TTT] Step 1: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.697e-02, relative_change=0.0373% (9 params)
2025-11-14 17:32:23 (IST) - 0:00:23 - train - INFO - step: 000001 - done (%): 0.1 - loss: 3.378 - lr: 4.0e-06 - peak_alloc_mem (GB): 35.6 - alloc_mem (GB): 21.0 - words_per_second: 2981.7 - avg_words_per_second: 2981.7 - ETA: >2025-11-14 23:49:30
2025-11-14 17:32:23 (IST) - 0:00:24 - train - INFO - [DocStream] step=2 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=2-3]
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.383147
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.496660
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.778798
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   requires_grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   has grad: True
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:28 (IST) - 0:00:29 - train - INFO - =========================

2025-11-14 17:32:29 (IST) - 0:00:29 - train - INFO - [TTT] Step 2: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.879e-02, relative_change=0.0308% (9 params)
2025-11-14 17:32:29 (IST) - 0:00:29 - train - INFO - step: 000002 - done (%): 0.1 - loss: 3.897 - lr: 4.0e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5652.3 - avg_words_per_second: 3903.9 - ETA: >2025-11-14 22:20:21
2025-11-14 17:32:29 (IST) - 0:00:30 - train - INFO - [DocStream] step=3 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=4-5]
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.423969
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.515728
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.744496
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   requires_grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   has grad: True
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - =========================

2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - [TTT] Step 3: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.488e-02, relative_change=0.0277% (9 params)
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - step: 000003 - done (%): 0.1 - loss: 3.316 - lr: 4.1e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5277.7 - avg_words_per_second: 4274.8 - ETA: >2025-11-14 21:55:21
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - [DocStream] step=4 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=0-1]
2025-11-14 17:32:35 (IST) - 0:00:36 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav
2025-11-14 17:32:35 (IST) - 0:00:36 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:32:35 (IST) - 0:00:36 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:32:35 (IST) - 0:00:36 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.406496
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.474143
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.780992
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   requires_grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   has grad: True
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO -   grad norm: 0.000000
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - =========================

2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - [TTT] Step 4: grad_norm=1.000e+00, param_norm=125.9593, delta_norm=3.275e-02, relative_change=0.0260% (9 params)
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - step: 000004 - done (%): 0.2 - loss: 4.809 - lr: 4.2e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5631.9 - avg_words_per_second: 4548.9 - ETA: >2025-11-14 21:39:30
2025-11-14 17:32:41 (IST) - 0:00:42 - train - INFO - [DocStream] step=5 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=2-3]
2025-11-14 17:32:47 (IST) - 0:00:48 - train - INFO - [TTT] Step 5: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.221e-02, relative_change=0.0256% (9 params)
2025-11-14 17:32:47 (IST) - 0:00:48 - train - INFO - step: 000005 - done (%): 0.2 - loss: 4.922 - lr: 4.4e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5619.5 - avg_words_per_second: 4729.1 - ETA: >2025-11-14 21:30:05
2025-11-14 17:32:47 (IST) - 0:00:48 - train - INFO - [DocStream] step=6 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=4-5]
2025-11-14 17:32:53 (IST) - 0:00:54 - train - INFO - [TTT] Step 6: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.271e-02, relative_change=0.0260% (9 params)
2025-11-14 17:32:53 (IST) - 0:00:54 - train - INFO - step: 000006 - done (%): 0.3 - loss: 3.317 - lr: 4.6e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5593.0 - avg_words_per_second: 4854.0 - ETA: >2025-11-14 21:23:57
2025-11-14 17:32:53 (IST) - 0:00:54 - train - INFO - [DocStream] step=7 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=6-7]
2025-11-14 17:32:59 (IST) - 0:01:00 - train - INFO - [TTT] Step 7: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.191e-02, relative_change=0.0253% (9 params)
2025-11-14 17:32:59 (IST) - 0:01:00 - train - INFO - step: 000007 - done (%): 0.3 - loss: 2.587 - lr: 4.9e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5585.1 - avg_words_per_second: 4946.5 - ETA: >2025-11-14 21:19:37
2025-11-14 17:32:59 (IST) - 0:01:00 - train - INFO - [DocStream] step=8 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=8-9]
2025-11-14 17:33:05 (IST) - 0:01:06 - train - INFO - [TTT] Step 8: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.219e-02, relative_change=0.0256% (9 params)
2025-11-14 17:33:05 (IST) - 0:01:06 - train - INFO - step: 000008 - done (%): 0.4 - loss: 2.286 - lr: 5.2e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5554.8 - avg_words_per_second: 5015.2 - ETA: >2025-11-14 21:16:30
2025-11-14 17:33:05 (IST) - 0:01:06 - train - INFO - [DocStream] step=9 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=10-11]
2025-11-14 17:33:11 (IST) - 0:01:12 - train - INFO - [TTT] Step 9: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.274e-02, relative_change=0.0260% (9 params)
2025-11-14 17:33:11 (IST) - 0:01:12 - train - INFO - step: 000009 - done (%): 0.5 - loss: 2.288 - lr: 5.5e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5534.6 - avg_words_per_second: 5068.0 - ETA: >2025-11-14 21:14:10
2025-11-14 17:33:11 (IST) - 0:01:12 - train - INFO - [DocStream] step=10 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=0-1]
2025-11-14 17:33:11 (IST) - 0:01:12 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav
2025-11-14 17:33:11 (IST) - 0:01:12 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:33:11 (IST) - 0:01:12 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:33:11 (IST) - 0:01:12 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:33:17 (IST) - 0:01:18 - train - INFO - [TTT] Step 10: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.359e-02, relative_change=0.0267% (9 params)
2025-11-14 17:33:17 (IST) - 0:01:18 - train - INFO - step: 000010 - done (%): 0.5 - loss: 5.038 - lr: 5.9e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5514.3 - avg_words_per_second: 5109.4 - ETA: >2025-11-14 21:12:22
2025-11-14 17:33:18 (IST) - 0:01:18 - train - INFO - [DocStream] step=11 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=2-3]
2025-11-14 17:33:23 (IST) - 0:01:24 - train - INFO - [TTT] Step 11: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.495e-02, relative_change=0.0277% (9 params)
2025-11-14 17:33:23 (IST) - 0:01:24 - train - INFO - step: 000011 - done (%): 0.6 - loss: 4.570 - lr: 6.4e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5492.5 - avg_words_per_second: 5142.0 - ETA: >2025-11-14 21:10:59
2025-11-14 17:33:24 (IST) - 0:01:25 - train - INFO - [DocStream] step=12 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=4-5]
2025-11-14 17:33:30 (IST) - 0:01:30 - train - INFO - [TTT] Step 12: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.656e-02, relative_change=0.0290% (9 params)
2025-11-14 17:33:30 (IST) - 0:01:30 - train - INFO - step: 000012 - done (%): 0.6 - loss: 2.697 - lr: 6.9e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5480.4 - avg_words_per_second: 5168.6 - ETA: >2025-11-14 21:09:51
2025-11-14 17:33:30 (IST) - 0:01:31 - train - INFO - [DocStream] step=13 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=6-7]
2025-11-14 17:33:36 (IST) - 0:01:37 - train - INFO - [TTT] Step 13: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.768e-02, relative_change=0.0299% (9 params)
2025-11-14 17:33:36 (IST) - 0:01:37 - train - INFO - step: 000013 - done (%): 0.7 - loss: 3.039 - lr: 7.4e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5469.5 - avg_words_per_second: 5190.5 - ETA: >2025-11-14 21:08:56
2025-11-14 17:33:36 (IST) - 0:01:37 - train - INFO - [DocStream] step=14 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=8-9]
2025-11-14 17:33:42 (IST) - 0:01:43 - train - INFO - [TTT] Step 14: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=3.951e-02, relative_change=0.0314% (9 params)
2025-11-14 17:33:42 (IST) - 0:01:43 - train - INFO - step: 000014 - done (%): 0.7 - loss: 2.849 - lr: 8.0e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5459.9 - avg_words_per_second: 5208.9 - ETA: >2025-11-14 21:08:10
2025-11-14 17:33:42 (IST) - 0:01:43 - train - INFO - [DocStream] step=15 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=10-11]
2025-11-14 17:33:48 (IST) - 0:01:49 - train - INFO - [TTT] Step 15: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=4.244e-02, relative_change=0.0337% (9 params)
2025-11-14 17:33:48 (IST) - 0:01:49 - train - INFO - step: 000015 - done (%): 0.8 - loss: 2.746 - lr: 8.7e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5441.7 - avg_words_per_second: 5223.8 - ETA: >2025-11-14 21:07:33
2025-11-14 17:33:48 (IST) - 0:01:49 - train - INFO - [DocStream] step=16 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=0-1]
2025-11-14 17:33:48 (IST) - 0:01:49 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav
2025-11-14 17:33:48 (IST) - 0:01:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:33:48 (IST) - 0:01:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:33:48 (IST) - 0:01:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:33:54 (IST) - 0:01:55 - train - INFO - [TTT] Step 16: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.425e-02, relative_change=0.0351% (9 params)
2025-11-14 17:33:54 (IST) - 0:01:55 - train - INFO - step: 000016 - done (%): 0.8 - loss: 4.238 - lr: 9.3e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5432.4 - avg_words_per_second: 5236.4 - ETA: >2025-11-14 21:07:02
2025-11-14 17:33:55 (IST) - 0:01:55 - train - INFO - [DocStream] step=17 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=2-3]
2025-11-14 17:34:01 (IST) - 0:02:01 - train - INFO - [TTT] Step 17: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=4.679e-02, relative_change=0.0371% (9 params)
2025-11-14 17:34:01 (IST) - 0:02:01 - train - INFO - step: 000017 - done (%): 0.8 - loss: 3.980 - lr: 1.0e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5424.1 - avg_words_per_second: 5247.0 - ETA: >2025-11-14 21:06:36
2025-11-14 17:34:01 (IST) - 0:02:02 - train - INFO - [DocStream] step=18 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=4-5]
2025-11-14 17:34:07 (IST) - 0:02:08 - train - INFO - [TTT] Step 18: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=4.925e-02, relative_change=0.0391% (9 params)
2025-11-14 17:34:07 (IST) - 0:02:08 - train - INFO - step: 000018 - done (%): 0.9 - loss: 2.508 - lr: 1.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5418.0 - avg_words_per_second: 5256.3 - ETA: >2025-11-14 21:06:13
2025-11-14 17:34:07 (IST) - 0:02:08 - train - INFO - [DocStream] step=19 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=6-7]
2025-11-14 17:34:13 (IST) - 0:02:14 - train - INFO - [TTT] Step 19: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=5.191e-02, relative_change=0.0412% (9 params)
2025-11-14 17:34:13 (IST) - 0:02:14 - train - INFO - step: 000019 - done (%): 0.9 - loss: 3.824 - lr: 1.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5421.3 - avg_words_per_second: 5264.7 - ETA: >2025-11-14 21:05:53
2025-11-14 17:34:13 (IST) - 0:02:14 - train - INFO - [DocStream] step=20 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=8-9]
2025-11-14 17:34:19 (IST) - 0:02:20 - train - INFO - [TTT] Step 20: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=5.547e-02, relative_change=0.0440% (9 params)
2025-11-14 17:34:19 (IST) - 0:02:20 - train - INFO - step: 000020 - done (%): 1.0 - loss: 3.386 - lr: 1.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5411.4 - avg_words_per_second: 5271.8 - ETA: >2025-11-14 21:05:35
2025-11-14 17:34:20 (IST) - 0:02:20 - train - INFO - [DocStream] step=21 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=10-11]
2025-11-14 17:34:26 (IST) - 0:02:26 - train - INFO - [TTT] Step 21: grad_norm=9.065e-01, param_norm=125.9588, delta_norm=5.798e-02, relative_change=0.0460% (9 params)
2025-11-14 17:34:26 (IST) - 0:02:26 - train - INFO - step: 000021 - done (%): 1.1 - loss: 1.924 - lr: 1.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5399.5 - avg_words_per_second: 5277.8 - ETA: >2025-11-14 21:05:21
2025-11-14 17:34:26 (IST) - 0:02:27 - train - INFO - [DocStream] step=22 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=0-1]
2025-11-14 17:34:26 (IST) - 0:02:27 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav
2025-11-14 17:34:26 (IST) - 0:02:27 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:34:26 (IST) - 0:02:27 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:34:26 (IST) - 0:02:27 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:34:32 (IST) - 0:02:33 - train - INFO - [TTT] Step 22: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=6.221e-02, relative_change=0.0494% (9 params)
2025-11-14 17:34:32 (IST) - 0:02:33 - train - INFO - step: 000022 - done (%): 1.1 - loss: 3.864 - lr: 1.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5392.7 - avg_words_per_second: 5282.9 - ETA: >2025-11-14 21:05:08
2025-11-14 17:34:32 (IST) - 0:02:33 - train - INFO - [DocStream] step=23 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=2-3]
2025-11-14 17:34:38 (IST) - 0:02:39 - train - INFO - [TTT] Step 23: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=6.682e-02, relative_change=0.0530% (9 params)
2025-11-14 17:34:38 (IST) - 0:02:39 - train - INFO - step: 000023 - done (%): 1.1 - loss: 3.712 - lr: 1.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5385.5 - avg_words_per_second: 5287.3 - ETA: >2025-11-14 21:04:58
2025-11-14 17:34:38 (IST) - 0:02:39 - train - INFO - [DocStream] step=24 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=4-5]
2025-11-14 17:34:44 (IST) - 0:02:45 - train - INFO - [TTT] Step 24: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=7.160e-02, relative_change=0.0568% (9 params)
2025-11-14 17:34:44 (IST) - 0:02:45 - train - INFO - step: 000024 - done (%): 1.2 - loss: 2.494 - lr: 1.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5385.0 - avg_words_per_second: 5291.3 - ETA: >2025-11-14 21:04:48
2025-11-14 17:34:45 (IST) - 0:02:45 - train - INFO - [DocStream] step=25 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=6-7]
2025-11-14 17:34:51 (IST) - 0:02:51 - train - INFO - [TTT] Step 25: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=7.577e-02, relative_change=0.0602% (9 params)
2025-11-14 17:34:51 (IST) - 0:02:51 - train - INFO - step: 000025 - done (%): 1.2 - loss: 2.505 - lr: 1.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5372.0 - avg_words_per_second: 5294.5 - ETA: >2025-11-14 21:04:40
2025-11-14 17:34:51 (IST) - 0:02:52 - train - INFO - [DocStream] step=26 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=8-9]
2025-11-14 17:34:57 (IST) - 0:02:58 - train - INFO - [TTT] Step 26: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=7.803e-02, relative_change=0.0619% (9 params)
2025-11-14 17:34:57 (IST) - 0:02:58 - train - INFO - step: 000026 - done (%): 1.3 - loss: 1.751 - lr: 1.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5379.3 - avg_words_per_second: 5297.7 - ETA: >2025-11-14 21:04:33
2025-11-14 17:34:57 (IST) - 0:02:58 - train - INFO - [DocStream] step=27 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=10-11]
2025-11-14 17:35:03 (IST) - 0:03:04 - train - INFO - [TTT] Step 27: grad_norm=8.358e-01, param_norm=125.9588, delta_norm=8.101e-02, relative_change=0.0643% (9 params)
2025-11-14 17:35:03 (IST) - 0:03:04 - train - INFO - step: 000027 - done (%): 1.4 - loss: 2.361 - lr: 1.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5373.5 - avg_words_per_second: 5300.4 - ETA: >2025-11-14 21:04:26
2025-11-14 17:35:04 (IST) - 0:03:04 - train - INFO - [DocStream] step=28 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=0-1]
2025-11-14 17:35:04 (IST) - 0:03:04 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav
2025-11-14 17:35:04 (IST) - 0:03:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:35:04 (IST) - 0:03:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:35:04 (IST) - 0:03:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:35:09 (IST) - 0:03:10 - train - INFO - [TTT] Step 28: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=8.629e-02, relative_change=0.0685% (9 params)
2025-11-14 17:35:09 (IST) - 0:03:10 - train - INFO - step: 000028 - done (%): 1.4 - loss: 3.956 - lr: 2.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5376.6 - avg_words_per_second: 5303.1 - ETA: >2025-11-14 21:04:20
2025-11-14 17:35:10 (IST) - 0:03:11 - train - INFO - [DocStream] step=29 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=2-3]
2025-11-14 17:35:16 (IST) - 0:03:17 - train - INFO - [TTT] Step 29: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=9.261e-02, relative_change=0.0735% (9 params)
2025-11-14 17:35:16 (IST) - 0:03:17 - train - INFO - step: 000029 - done (%): 1.4 - loss: 3.923 - lr: 2.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5361.6 - avg_words_per_second: 5305.1 - ETA: >2025-11-14 21:04:15
2025-11-14 17:35:16 (IST) - 0:03:17 - train - INFO - [DocStream] step=30 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=4-5]
2025-11-14 17:35:22 (IST) - 0:03:23 - train - INFO - [TTT] Step 30: grad_norm=9.204e-01, param_norm=125.9588, delta_norm=9.694e-02, relative_change=0.0770% (9 params)
2025-11-14 17:35:22 (IST) - 0:03:23 - train - INFO - step: 000030 - done (%): 1.5 - loss: 1.921 - lr: 2.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5362.4 - avg_words_per_second: 5307.0 - ETA: >2025-11-14 21:04:10
2025-11-14 17:35:22 (IST) - 0:03:23 - train - INFO - [DocStream] step=31 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=6-7]
2025-11-14 17:35:28 (IST) - 0:03:29 - train - INFO - [TTT] Step 31: grad_norm=8.400e-01, param_norm=125.9588, delta_norm=1.019e-01, relative_change=0.0809% (9 params)
2025-11-14 17:35:28 (IST) - 0:03:29 - train - INFO - step: 000031 - done (%): 1.6 - loss: 2.197 - lr: 2.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.9 - avg_words_per_second: 5308.7 - ETA: >2025-11-14 21:04:06
2025-11-14 17:35:29 (IST) - 0:03:30 - train - INFO - [DocStream] step=32 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=8-9]
2025-11-14 17:35:35 (IST) - 0:03:35 - train - INFO - [TTT] Step 32: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.034e-01, relative_change=0.0821% (9 params)
2025-11-14 17:35:35 (IST) - 0:03:35 - train - INFO - step: 000032 - done (%): 1.6 - loss: 1.998 - lr: 2.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5370.4 - avg_words_per_second: 5310.6 - ETA: >2025-11-14 21:04:02
2025-11-14 17:35:35 (IST) - 0:03:36 - train - INFO - [DocStream] step=33 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=10-11]
2025-11-14 17:35:41 (IST) - 0:03:42 - train - INFO - [TTT] Step 33: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=1.070e-01, relative_change=0.0850% (9 params)
2025-11-14 17:35:41 (IST) - 0:03:42 - train - INFO - step: 000033 - done (%): 1.6 - loss: 1.840 - lr: 2.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5363.6 - avg_words_per_second: 5312.2 - ETA: >2025-11-14 21:03:58
2025-11-14 17:35:41 (IST) - 0:03:42 - train - INFO - [DocStream] step=34 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=0-1]
2025-11-14 17:35:41 (IST) - 0:03:42 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav
2025-11-14 17:35:41 (IST) - 0:03:42 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:35:41 (IST) - 0:03:42 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:35:41 (IST) - 0:03:42 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:35:47 (IST) - 0:03:48 - train - INFO - [TTT] Step 34: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=1.114e-01, relative_change=0.0885% (9 params)
2025-11-14 17:35:47 (IST) - 0:03:48 - train - INFO - step: 000034 - done (%): 1.7 - loss: 4.182 - lr: 2.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.6 - avg_words_per_second: 5313.6 - ETA: >2025-11-14 21:03:55
2025-11-14 17:35:48 (IST) - 0:03:48 - train - INFO - [DocStream] step=35 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=2-3]
2025-11-14 17:35:54 (IST) - 0:03:54 - train - INFO - [TTT] Step 35: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.184e-01, relative_change=0.0940% (9 params)
2025-11-14 17:35:54 (IST) - 0:03:54 - train - INFO - step: 000035 - done (%): 1.8 - loss: 3.352 - lr: 2.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5361.2 - avg_words_per_second: 5314.9 - ETA: >2025-11-14 21:03:51
2025-11-14 17:35:54 (IST) - 0:03:55 - train - INFO - [DocStream] step=36 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=4-5]
2025-11-14 17:36:00 (IST) - 0:04:01 - train - INFO - [TTT] Step 36: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=1.235e-01, relative_change=0.0981% (9 params)
2025-11-14 17:36:00 (IST) - 0:04:01 - train - INFO - step: 000036 - done (%): 1.8 - loss: 2.836 - lr: 3.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5360.4 - avg_words_per_second: 5316.2 - ETA: >2025-11-14 21:03:48
2025-11-14 17:36:00 (IST) - 0:04:01 - train - INFO - [DocStream] step=37 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=6-7]
2025-11-14 17:36:06 (IST) - 0:04:07 - train - INFO - [TTT] Step 37: grad_norm=9.856e-01, param_norm=125.9588, delta_norm=1.305e-01, relative_change=0.1036% (9 params)
2025-11-14 17:36:06 (IST) - 0:04:07 - train - INFO - step: 000037 - done (%): 1.9 - loss: 2.548 - lr: 3.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5361.4 - avg_words_per_second: 5317.4 - ETA: >2025-11-14 21:03:46
2025-11-14 17:36:06 (IST) - 0:04:07 - train - INFO - [DocStream] step=38 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=8-9]
2025-11-14 17:36:12 (IST) - 0:04:13 - train - INFO - [TTT] Step 38: grad_norm=7.915e-01, param_norm=125.9589, delta_norm=1.349e-01, relative_change=0.1071% (9 params)
2025-11-14 17:36:12 (IST) - 0:04:13 - train - INFO - step: 000038 - done (%): 1.9 - loss: 2.290 - lr: 3.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.2 - avg_words_per_second: 5318.5 - ETA: >2025-11-14 21:03:43
2025-11-14 17:36:13 (IST) - 0:04:14 - train - INFO - [DocStream] step=39 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=10-11]
2025-11-14 17:36:19 (IST) - 0:04:20 - train - INFO - [TTT] Step 39: grad_norm=7.949e-01, param_norm=125.9592, delta_norm=1.387e-01, relative_change=0.1101% (9 params)
2025-11-14 17:36:19 (IST) - 0:04:20 - train - INFO - step: 000039 - done (%): 1.9 - loss: 1.915 - lr: 3.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5360.1 - avg_words_per_second: 5319.5 - ETA: >2025-11-14 21:03:40
2025-11-14 17:36:19 (IST) - 0:04:20 - train - INFO - [DocStream] step=40 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=0-1]
2025-11-14 17:36:19 (IST) - 0:04:20 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav
2025-11-14 17:36:19 (IST) - 0:04:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:36:19 (IST) - 0:04:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:36:19 (IST) - 0:04:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:36:25 (IST) - 0:04:26 - train - INFO - [TTT] Step 40: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=1.450e-01, relative_change=0.1151% (9 params)
2025-11-14 17:36:25 (IST) - 0:04:26 - train - INFO - step: 000040 - done (%): 2.0 - loss: 4.980 - lr: 3.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5358.2 - avg_words_per_second: 5320.5 - ETA: >2025-11-14 21:03:38
2025-11-14 17:36:25 (IST) - 0:04:26 - train - INFO - [DocStream] step=41 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=2-3]
2025-11-14 17:36:31 (IST) - 0:04:32 - train - INFO - [TTT] Step 41: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.515e-01, relative_change=0.1202% (9 params)
2025-11-14 17:36:31 (IST) - 0:04:32 - train - INFO - step: 000041 - done (%): 2.0 - loss: 3.760 - lr: 3.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5347.9 - avg_words_per_second: 5321.2 - ETA: >2025-11-14 21:03:37
2025-11-14 17:36:32 (IST) - 0:04:33 - train - INFO - [DocStream] step=42 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=4-5]
2025-11-14 17:36:38 (IST) - 0:04:39 - train - INFO - [TTT] Step 42: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=1.577e-01, relative_change=0.1252% (9 params)
2025-11-14 17:36:38 (IST) - 0:04:39 - train - INFO - step: 000042 - done (%): 2.1 - loss: 3.611 - lr: 3.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5344.1 - avg_words_per_second: 5321.7 - ETA: >2025-11-14 21:03:35
2025-11-14 17:36:38 (IST) - 0:04:39 - train - INFO - [DocStream] step=43 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=6-7]
2025-11-14 17:36:44 (IST) - 0:04:45 - train - INFO - [TTT] Step 43: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=1.632e-01, relative_change=0.1296% (9 params)
2025-11-14 17:36:44 (IST) - 0:04:45 - train - INFO - step: 000043 - done (%): 2.1 - loss: 1.782 - lr: 4.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5355.3 - avg_words_per_second: 5322.5 - ETA: >2025-11-14 21:03:33
2025-11-14 17:36:44 (IST) - 0:04:45 - train - INFO - [DocStream] step=44 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=8-9]
2025-11-14 17:36:50 (IST) - 0:04:51 - train - INFO - [TTT] Step 44: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=1.644e-01, relative_change=0.1305% (9 params)
2025-11-14 17:36:50 (IST) - 0:04:51 - train - INFO - step: 000044 - done (%): 2.2 - loss: 2.462 - lr: 4.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5348.8 - avg_words_per_second: 5323.1 - ETA: >2025-11-14 21:03:32
2025-11-14 17:36:51 (IST) - 0:04:51 - train - INFO - [DocStream] step=45 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=10-11]
2025-11-14 17:36:57 (IST) - 0:04:57 - train - INFO - [TTT] Step 45: grad_norm=9.013e-01, param_norm=125.9597, delta_norm=1.690e-01, relative_change=0.1342% (9 params)
2025-11-14 17:36:57 (IST) - 0:04:57 - train - INFO - step: 000045 - done (%): 2.2 - loss: 2.108 - lr: 4.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5353.2 - avg_words_per_second: 5323.8 - ETA: >2025-11-14 21:03:30
2025-11-14 17:36:57 (IST) - 0:04:58 - train - INFO - [DocStream] step=46 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=0-1]
2025-11-14 17:36:57 (IST) - 0:04:58 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4156.wav
2025-11-14 17:36:57 (IST) - 0:04:58 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:36:57 (IST) - 0:04:58 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:36:57 (IST) - 0:04:58 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:37:03 (IST) - 0:05:04 - train - INFO - [TTT] Step 46: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.758e-01, relative_change=0.1395% (9 params)
2025-11-14 17:37:03 (IST) - 0:05:04 - train - INFO - step: 000046 - done (%): 2.3 - loss: 4.402 - lr: 4.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5302.2 - avg_words_per_second: 5323.3 - ETA: >2025-11-14 21:03:32
2025-11-14 17:37:03 (IST) - 0:05:04 - train - INFO - [DocStream] step=47 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=2-3]
2025-11-14 17:37:09 (IST) - 0:05:10 - train - INFO - [TTT] Step 47: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=1.864e-01, relative_change=0.1480% (9 params)
2025-11-14 17:37:09 (IST) - 0:05:10 - train - INFO - step: 000047 - done (%): 2.4 - loss: 4.594 - lr: 4.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5355.1 - avg_words_per_second: 5324.0 - ETA: >2025-11-14 21:03:30
2025-11-14 17:37:10 (IST) - 0:05:10 - train - INFO - [DocStream] step=48 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=4-5]
2025-11-14 17:37:16 (IST) - 0:05:16 - train - INFO - [TTT] Step 48: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.941e-01, relative_change=0.1541% (9 params)
2025-11-14 17:37:16 (IST) - 0:05:16 - train - INFO - step: 000048 - done (%): 2.4 - loss: 4.697 - lr: 4.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5348.7 - avg_words_per_second: 5324.5 - ETA: >2025-11-14 21:03:29
2025-11-14 17:37:16 (IST) - 0:05:17 - train - INFO - [DocStream] step=49 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=6-7]
2025-11-14 17:37:22 (IST) - 0:05:23 - train - INFO - [TTT] Step 49: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=1.947e-01, relative_change=0.1546% (9 params)
2025-11-14 17:37:22 (IST) - 0:05:23 - train - INFO - step: 000049 - done (%): 2.5 - loss: 4.831 - lr: 5.0e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.4 - avg_words_per_second: 5324.8 - ETA: >2025-11-14 21:03:28
2025-11-14 17:37:22 (IST) - 0:05:23 - train - INFO - [DocStream] step=50 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=8-9]
2025-11-14 17:37:28 (IST) - 0:05:29 - train - INFO - [TTT] Step 50: grad_norm=1.000e+00, param_norm=125.9597, delta_norm=1.911e-01, relative_change=0.1517% (9 params)
2025-11-14 17:37:28 (IST) - 0:05:29 - train - INFO - step: 000050 - done (%): 2.5 - loss: 4.406 - lr: 5.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5430.9 - avg_words_per_second: 5326.9 - ETA: >2025-11-14 21:03:23
2025-11-14 17:37:28 (IST) - 0:05:29 - train - INFO - [DocStream] step=51 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=0-1]
2025-11-14 17:37:28 (IST) - 0:05:29 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4156.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4157.wav
2025-11-14 17:37:28 (IST) - 0:05:29 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:37:28 (IST) - 0:05:29 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:37:28 (IST) - 0:05:29 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:37:34 (IST) - 0:05:35 - train - INFO - [TTT] Step 51: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=2.028e-01, relative_change=0.1610% (9 params)
2025-11-14 17:37:34 (IST) - 0:05:35 - train - INFO - step: 000051 - done (%): 2.5 - loss: 4.901 - lr: 5.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5356.2 - avg_words_per_second: 5327.5 - ETA: >2025-11-14 21:03:22
2025-11-14 17:37:35 (IST) - 0:05:36 - train - INFO - [DocStream] step=52 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=2-3]
2025-11-14 17:37:41 (IST) - 0:05:42 - train - INFO - [TTT] Step 52: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=2.176e-01, relative_change=0.1728% (9 params)
2025-11-14 17:37:41 (IST) - 0:05:42 - train - INFO - step: 000052 - done (%): 2.6 - loss: 4.192 - lr: 5.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5353.0 - avg_words_per_second: 5328.0 - ETA: >2025-11-14 21:03:20
2025-11-14 17:37:41 (IST) - 0:05:42 - train - INFO - [DocStream] step=53 microbatch=0 samples=2 unique_docs=1 runs=4157.wav[segments=4-5]
2025-11-14 17:37:47 (IST) - 0:05:48 - train - INFO - [TTT] Step 53: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=2.239e-01, relative_change=0.1777% (9 params)
2025-11-14 17:37:47 (IST) - 0:05:48 - train - INFO - step: 000053 - done (%): 2.6 - loss: 3.762 - lr: 5.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5465.0 - avg_words_per_second: 5330.5 - ETA: >2025-11-14 21:03:14
2025-11-14 17:37:47 (IST) - 0:05:48 - train - INFO - [DocStream] step=54 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=0-1]
2025-11-14 17:37:47 (IST) - 0:05:48 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4157.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4170.wav
2025-11-14 17:37:47 (IST) - 0:05:48 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:37:47 (IST) - 0:05:48 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:37:47 (IST) - 0:05:48 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:37:53 (IST) - 0:05:54 - train - INFO - [TTT] Step 54: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=2.384e-01, relative_change=0.1892% (9 params)
2025-11-14 17:37:53 (IST) - 0:05:54 - train - INFO - step: 000054 - done (%): 2.7 - loss: 4.466 - lr: 5.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5354.2 - avg_words_per_second: 5330.9 - ETA: >2025-11-14 21:03:13
2025-11-14 17:37:54 (IST) - 0:05:54 - train - INFO - [DocStream] step=55 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=2-3]
2025-11-14 17:38:00 (IST) - 0:06:00 - train - INFO - [TTT] Step 55: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=2.544e-01, relative_change=0.2020% (9 params)
2025-11-14 17:38:00 (IST) - 0:06:00 - train - INFO - step: 000055 - done (%): 2.8 - loss: 4.660 - lr: 5.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5351.5 - avg_words_per_second: 5331.3 - ETA: >2025-11-14 21:03:12
2025-11-14 17:38:00 (IST) - 0:06:01 - train - INFO - [DocStream] step=56 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=4-5]
2025-11-14 17:38:06 (IST) - 0:06:07 - train - INFO - [TTT] Step 56: grad_norm=1.000e+00, param_norm=125.9595, delta_norm=2.593e-01, relative_change=0.2059% (9 params)
2025-11-14 17:38:06 (IST) - 0:06:07 - train - INFO - step: 000056 - done (%): 2.8 - loss: 5.084 - lr: 6.0e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5321.9 - avg_words_per_second: 5331.1 - ETA: >2025-11-14 21:03:13
2025-11-14 17:38:06 (IST) - 0:06:07 - train - INFO - [DocStream] step=57 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=6-7]
2025-11-14 17:38:12 (IST) - 0:06:13 - train - INFO - [TTT] Step 57: grad_norm=1.000e+00, param_norm=125.9604, delta_norm=2.556e-01, relative_change=0.2029% (9 params)
2025-11-14 17:38:12 (IST) - 0:06:13 - train - INFO - step: 000057 - done (%): 2.9 - loss: 4.686 - lr: 6.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5347.0 - avg_words_per_second: 5331.4 - ETA: >2025-11-14 21:03:12
2025-11-14 17:38:13 (IST) - 0:06:13 - train - INFO - [DocStream] step=58 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=8-9]
2025-11-14 17:38:19 (IST) - 0:06:19 - train - INFO - [TTT] Step 58: grad_norm=1.000e+00, param_norm=125.9617, delta_norm=2.516e-01, relative_change=0.1997% (9 params)
2025-11-14 17:38:19 (IST) - 0:06:19 - train - INFO - step: 000058 - done (%): 2.9 - loss: 4.358 - lr: 6.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5355.0 - avg_words_per_second: 5331.8 - ETA: >2025-11-14 21:03:11
2025-11-14 17:38:19 (IST) - 0:06:20 - train - INFO - [DocStream] step=59 microbatch=0 samples=2 unique_docs=1 runs=4170.wav[segments=10-11]
2025-11-14 17:38:25 (IST) - 0:06:26 - train - INFO - [TTT] Step 59: grad_norm=1.000e+00, param_norm=125.9632, delta_norm=2.498e-01, relative_change=0.1984% (9 params)
2025-11-14 17:38:25 (IST) - 0:06:26 - train - INFO - step: 000059 - done (%): 3.0 - loss: 4.411 - lr: 6.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5344.7 - avg_words_per_second: 5332.0 - ETA: >2025-11-14 21:03:11
2025-11-14 17:38:25 (IST) - 0:06:26 - train - INFO - [DocStream] step=60 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=0-1]
2025-11-14 17:38:25 (IST) - 0:06:26 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4170.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4183.wav
2025-11-14 17:38:25 (IST) - 0:06:26 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:38:25 (IST) - 0:06:26 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:38:25 (IST) - 0:06:26 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:38:31 (IST) - 0:06:32 - train - INFO - [TTT] Step 60: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=2.435e-01, relative_change=0.1933% (9 params)
2025-11-14 17:38:31 (IST) - 0:06:32 - train - INFO - step: 000060 - done (%): 3.0 - loss: 4.429 - lr: 6.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5350.7 - avg_words_per_second: 5332.3 - ETA: >2025-11-14 21:03:10
2025-11-14 17:38:31 (IST) - 0:06:32 - train - INFO - [DocStream] step=61 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=2-3]
2025-11-14 17:38:37 (IST) - 0:06:38 - train - INFO - [TTT] Step 61: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=2.569e-01, relative_change=0.2039% (9 params)
2025-11-14 17:38:37 (IST) - 0:06:38 - train - INFO - step: 000061 - done (%): 3.0 - loss: 4.160 - lr: 6.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5353.2 - avg_words_per_second: 5332.7 - ETA: >2025-11-14 21:03:09
2025-11-14 17:38:38 (IST) - 0:06:39 - train - INFO - [DocStream] step=62 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=4-5]
2025-11-14 17:38:44 (IST) - 0:06:45 - train - INFO - [TTT] Step 62: grad_norm=1.000e+00, param_norm=125.9593, delta_norm=2.682e-01, relative_change=0.2129% (9 params)
2025-11-14 17:38:44 (IST) - 0:06:45 - train - INFO - step: 000062 - done (%): 3.1 - loss: 3.925 - lr: 6.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.9 - avg_words_per_second: 5332.9 - ETA: >2025-11-14 21:03:09
2025-11-14 17:38:44 (IST) - 0:06:45 - train - INFO - [DocStream] step=63 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=6-7]
2025-11-14 17:38:50 (IST) - 0:06:51 - train - INFO - [TTT] Step 63: grad_norm=1.000e+00, param_norm=125.9603, delta_norm=2.815e-01, relative_change=0.2235% (9 params)
2025-11-14 17:38:50 (IST) - 0:06:51 - train - INFO - step: 000063 - done (%): 3.1 - loss: 3.806 - lr: 7.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5343.5 - avg_words_per_second: 5333.0 - ETA: >2025-11-14 21:03:08
2025-11-14 17:38:50 (IST) - 0:06:51 - train - INFO - [DocStream] step=64 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=8-9]
2025-11-14 17:38:56 (IST) - 0:06:57 - train - INFO - [TTT] Step 64: grad_norm=1.000e+00, param_norm=125.9617, delta_norm=2.803e-01, relative_change=0.2225% (9 params)
2025-11-14 17:38:56 (IST) - 0:06:57 - train - INFO - step: 000064 - done (%): 3.2 - loss: 3.560 - lr: 7.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5328.3 - avg_words_per_second: 5333.0 - ETA: >2025-11-14 21:03:09
2025-11-14 17:38:57 (IST) - 0:06:58 - train - INFO - [DocStream] step=65 microbatch=0 samples=2 unique_docs=1 runs=4183.wav[segments=10-11]
2025-11-14 17:39:03 (IST) - 0:07:04 - train - INFO - [TTT] Step 65: grad_norm=1.000e+00, param_norm=125.9636, delta_norm=2.782e-01, relative_change=0.2209% (9 params)
2025-11-14 17:39:03 (IST) - 0:07:04 - train - INFO - step: 000065 - done (%): 3.2 - loss: 3.810 - lr: 7.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5344.7 - avg_words_per_second: 5333.1 - ETA: >2025-11-14 21:03:08
2025-11-14 17:39:03 (IST) - 0:07:04 - train - INFO - [DocStream] step=66 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=0-1]
2025-11-14 17:39:03 (IST) - 0:07:04 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4183.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4184.wav
2025-11-14 17:39:03 (IST) - 0:07:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:39:03 (IST) - 0:07:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:39:03 (IST) - 0:07:04 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:39:09 (IST) - 0:07:10 - train - INFO - [TTT] Step 66: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=2.875e-01, relative_change=0.2283% (9 params)
2025-11-14 17:39:09 (IST) - 0:07:10 - train - INFO - step: 000066 - done (%): 3.3 - loss: 4.736 - lr: 7.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5358.7 - avg_words_per_second: 5333.5 - ETA: >2025-11-14 21:03:07
2025-11-14 17:39:09 (IST) - 0:07:10 - train - INFO - [DocStream] step=67 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=2-3]
2025-11-14 17:39:15 (IST) - 0:07:16 - train - INFO - [TTT] Step 67: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=2.989e-01, relative_change=0.2373% (9 params)
2025-11-14 17:39:15 (IST) - 0:07:16 - train - INFO - step: 000067 - done (%): 3.4 - loss: 4.492 - lr: 7.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5344.9 - avg_words_per_second: 5333.7 - ETA: >2025-11-14 21:03:07
2025-11-14 17:39:16 (IST) - 0:07:16 - train - INFO - [DocStream] step=68 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=4-5]
2025-11-14 17:39:22 (IST) - 0:07:22 - train - INFO - [TTT] Step 68: grad_norm=1.000e+00, param_norm=125.9598, delta_norm=3.134e-01, relative_change=0.2488% (9 params)
2025-11-14 17:39:22 (IST) - 0:07:22 - train - INFO - step: 000068 - done (%): 3.4 - loss: 2.974 - lr: 7.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5352.3 - avg_words_per_second: 5334.0 - ETA: >2025-11-14 21:03:06
2025-11-14 17:39:22 (IST) - 0:07:23 - train - INFO - [DocStream] step=69 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=6-7]
2025-11-14 17:39:28 (IST) - 0:07:29 - train - INFO - [TTT] Step 69: grad_norm=8.484e-01, param_norm=125.9611, delta_norm=3.118e-01, relative_change=0.2475% (9 params)
2025-11-14 17:39:28 (IST) - 0:07:29 - train - INFO - step: 000069 - done (%): 3.5 - loss: 2.680 - lr: 7.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5347.9 - avg_words_per_second: 5334.2 - ETA: >2025-11-14 21:03:06
2025-11-14 17:39:28 (IST) - 0:07:29 - train - INFO - [DocStream] step=70 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=8-9]
2025-11-14 17:39:34 (IST) - 0:07:35 - train - INFO - [TTT] Step 70: grad_norm=1.000e+00, param_norm=125.9629, delta_norm=3.069e-01, relative_change=0.2436% (9 params)
2025-11-14 17:39:34 (IST) - 0:07:35 - train - INFO - step: 000070 - done (%): 3.5 - loss: 2.747 - lr: 8.0e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5352.7 - avg_words_per_second: 5334.4 - ETA: >2025-11-14 21:03:05
2025-11-14 17:39:35 (IST) - 0:07:35 - train - INFO - [DocStream] step=71 microbatch=0 samples=2 unique_docs=1 runs=4184.wav[segments=10-11]
2025-11-14 17:39:41 (IST) - 0:07:41 - train - INFO - [TTT] Step 71: grad_norm=6.840e-01, param_norm=125.9651, delta_norm=3.035e-01, relative_change=0.2409% (9 params)
2025-11-14 17:39:41 (IST) - 0:07:41 - train - INFO - step: 000071 - done (%): 3.5 - loss: 1.978 - lr: 8.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5344.1 - avg_words_per_second: 5334.6 - ETA: >2025-11-14 21:03:05
2025-11-14 17:39:41 (IST) - 0:07:42 - train - INFO - [DocStream] step=72 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=0-1]
2025-11-14 17:39:41 (IST) - 0:07:42 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4184.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4234.wav
2025-11-14 17:39:41 (IST) - 0:07:42 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:39:41 (IST) - 0:07:42 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:39:41 (IST) - 0:07:42 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:39:47 (IST) - 0:07:48 - train - INFO - [TTT] Step 72: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=3.102e-01, relative_change=0.2463% (9 params)
2025-11-14 17:39:47 (IST) - 0:07:48 - train - INFO - step: 000072 - done (%): 3.6 - loss: 4.724 - lr: 8.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.6 - avg_words_per_second: 5334.9 - ETA: >2025-11-14 21:03:04
2025-11-14 17:39:47 (IST) - 0:07:48 - train - INFO - [DocStream] step=73 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=2-3]
2025-11-14 17:39:53 (IST) - 0:07:54 - train - INFO - [TTT] Step 73: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=3.326e-01, relative_change=0.2640% (9 params)
2025-11-14 17:39:53 (IST) - 0:07:54 - train - INFO - step: 000073 - done (%): 3.6 - loss: 5.019 - lr: 8.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5354.3 - avg_words_per_second: 5335.2 - ETA: >2025-11-14 21:03:03
2025-11-14 17:39:54 (IST) - 0:07:54 - train - INFO - [DocStream] step=74 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=4-5]
2025-11-14 17:40:00 (IST) - 0:08:00 - train - INFO - [TTT] Step 74: grad_norm=1.000e+00, param_norm=125.9600, delta_norm=3.406e-01, relative_change=0.2704% (9 params)
2025-11-14 17:40:00 (IST) - 0:08:00 - train - INFO - step: 000074 - done (%): 3.7 - loss: 3.643 - lr: 8.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5356.8 - avg_words_per_second: 5335.5 - ETA: >2025-11-14 21:03:03
2025-11-14 17:40:00 (IST) - 0:08:01 - train - INFO - [DocStream] step=75 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=6-7]
2025-11-14 17:40:06 (IST) - 0:08:07 - train - INFO - [TTT] Step 75: grad_norm=7.708e-01, param_norm=125.9617, delta_norm=3.381e-01, relative_change=0.2684% (9 params)
2025-11-14 17:40:06 (IST) - 0:08:07 - train - INFO - step: 000075 - done (%): 3.8 - loss: 2.296 - lr: 8.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.8 - avg_words_per_second: 5335.6 - ETA: >2025-11-14 21:03:02
2025-11-14 17:40:06 (IST) - 0:08:07 - train - INFO - [DocStream] step=76 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=8-9]
2025-11-14 17:40:12 (IST) - 0:08:13 - train - INFO - [TTT] Step 76: grad_norm=1.000e+00, param_norm=125.9640, delta_norm=3.307e-01, relative_change=0.2625% (9 params)
2025-11-14 17:40:12 (IST) - 0:08:13 - train - INFO - step: 000076 - done (%): 3.8 - loss: 2.763 - lr: 8.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5343.0 - avg_words_per_second: 5335.7 - ETA: >2025-11-14 21:03:02
2025-11-14 17:40:12 (IST) - 0:08:13 - train - INFO - [DocStream] step=77 microbatch=0 samples=2 unique_docs=1 runs=4234.wav[segments=10-11]
2025-11-14 17:40:18 (IST) - 0:08:19 - train - INFO - [TTT] Step 77: grad_norm=1.000e+00, param_norm=125.9665, delta_norm=3.219e-01, relative_change=0.2556% (9 params)
2025-11-14 17:40:18 (IST) - 0:08:19 - train - INFO - step: 000077 - done (%): 3.9 - loss: 2.585 - lr: 8.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.6 - avg_words_per_second: 5336.0 - ETA: >2025-11-14 21:03:01
2025-11-14 17:40:19 (IST) - 0:08:20 - train - INFO - [DocStream] step=78 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=0-1]
2025-11-14 17:40:19 (IST) - 0:08:20 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4234.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4245.wav
2025-11-14 17:40:19 (IST) - 0:08:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:19 (IST) - 0:08:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:19 (IST) - 0:08:20 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:25 (IST) - 0:08:26 - train - INFO - [TTT] Step 78: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=3.346e-01, relative_change=0.2657% (9 params)
2025-11-14 17:40:25 (IST) - 0:08:26 - train - INFO - step: 000078 - done (%): 3.9 - loss: 5.301 - lr: 8.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5346.3 - avg_words_per_second: 5336.1 - ETA: >2025-11-14 21:03:01
2025-11-14 17:40:25 (IST) - 0:08:26 - train - INFO - [DocStream] step=79 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=2-3]
2025-11-14 17:40:31 (IST) - 0:08:32 - train - INFO - [TTT] Step 79: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.623e-01, relative_change=0.2876% (9 params)
2025-11-14 17:40:31 (IST) - 0:08:32 - train - INFO - step: 000079 - done (%): 4.0 - loss: 5.273 - lr: 9.0e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5341.7 - avg_words_per_second: 5336.2 - ETA: >2025-11-14 21:03:01
2025-11-14 17:40:31 (IST) - 0:08:32 - train - INFO - [DocStream] step=80 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=4-5]
2025-11-14 17:40:37 (IST) - 0:08:38 - train - INFO - [TTT] Step 80: grad_norm=1.000e+00, param_norm=125.9604, delta_norm=3.664e-01, relative_change=0.2909% (9 params)
2025-11-14 17:40:37 (IST) - 0:08:38 - train - INFO - step: 000080 - done (%): 4.0 - loss: 4.044 - lr: 9.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5347.0 - avg_words_per_second: 5336.3 - ETA: >2025-11-14 21:03:01
2025-11-14 17:40:38 (IST) - 0:08:39 - train - INFO - [DocStream] step=81 microbatch=0 samples=2 unique_docs=1 runs=4245.wav[segments=6-7]
2025-11-14 17:40:44 (IST) - 0:08:45 - train - INFO - [TTT] Step 81: grad_norm=9.655e-01, param_norm=125.9625, delta_norm=3.731e-01, relative_change=0.2962% (9 params)
2025-11-14 17:40:44 (IST) - 0:08:45 - train - INFO - step: 000081 - done (%): 4.0 - loss: 2.886 - lr: 9.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5345.5 - avg_words_per_second: 5336.4 - ETA: >2025-11-14 21:03:00
2025-11-14 17:40:44 (IST) - 0:08:45 - train - INFO - [DocStream] step=82 microbatch=0 samples=2 unique_docs=2 runs=4245.wav[segment=8], 4247.wav[segment=0]
2025-11-14 17:40:44 (IST) - 0:08:45 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4245.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4247.wav
2025-11-14 17:40:44 (IST) - 0:08:45 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:44 (IST) - 0:08:45 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:44 (IST) - 0:08:45 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:50 (IST) - 0:08:51 - train - INFO - [TTT] Step 82: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=3.928e-01, relative_change=0.3118% (9 params)
2025-11-14 17:40:50 (IST) - 0:08:51 - train - INFO - step: 000082 - done (%): 4.1 - loss: 4.169 - lr: 9.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5396.5 - avg_words_per_second: 5337.1 - ETA: >2025-11-14 21:02:59
2025-11-14 17:40:50 (IST) - 0:08:51 - train - INFO - [DocStream] step=83 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=1-2]
2025-11-14 17:40:56 (IST) - 0:08:57 - train - INFO - [TTT] Step 83: grad_norm=1.000e+00, param_norm=125.9596, delta_norm=3.895e-01, relative_change=0.3092% (9 params)
2025-11-14 17:40:56 (IST) - 0:08:57 - train - INFO - step: 000083 - done (%): 4.2 - loss: 4.531 - lr: 9.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5343.4 - avg_words_per_second: 5337.2 - ETA: >2025-11-14 21:02:58
2025-11-14 17:40:57 (IST) - 0:08:57 - train - INFO - [DocStream] step=84 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=3-4]
2025-11-14 17:41:03 (IST) - 0:09:03 - train - INFO - [TTT] Step 84: grad_norm=1.000e+00, param_norm=125.9612, delta_norm=3.891e-01, relative_change=0.3089% (9 params)
2025-11-14 17:41:03 (IST) - 0:09:03 - train - INFO - step: 000084 - done (%): 4.2 - loss: 3.685 - lr: 9.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.2 - avg_words_per_second: 5337.3 - ETA: >2025-11-14 21:02:58
2025-11-14 17:41:03 (IST) - 0:09:04 - train - INFO - [DocStream] step=85 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=5-6]
2025-11-14 17:41:09 (IST) - 0:09:10 - train - INFO - [TTT] Step 85: grad_norm=1.000e+00, param_norm=125.9636, delta_norm=3.803e-01, relative_change=0.3019% (9 params)
2025-11-14 17:41:09 (IST) - 0:09:10 - train - INFO - step: 000085 - done (%): 4.2 - loss: 1.976 - lr: 9.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5357.6 - avg_words_per_second: 5337.5 - ETA: >2025-11-14 21:02:58
2025-11-14 17:41:09 (IST) - 0:09:10 - train - INFO - [DocStream] step=86 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=7-8]
2025-11-14 17:41:15 (IST) - 0:09:16 - train - INFO - [TTT] Step 86: grad_norm=1.000e+00, param_norm=125.9665, delta_norm=3.717e-01, relative_change=0.2951% (9 params)
2025-11-14 17:41:15 (IST) - 0:09:16 - train - INFO - step: 000086 - done (%): 4.3 - loss: 2.373 - lr: 9.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5346.4 - avg_words_per_second: 5337.6 - ETA: >2025-11-14 21:02:58
2025-11-14 17:41:16 (IST) - 0:09:16 - train - INFO - [DocStream] step=87 microbatch=0 samples=2 unique_docs=1 runs=4247.wav[segments=9-10]
2025-11-14 17:41:22 (IST) - 0:09:22 - train - INFO - [TTT] Step 87: grad_norm=7.754e-01, param_norm=125.9700, delta_norm=3.536e-01, relative_change=0.2807% (9 params)
2025-11-14 17:41:22 (IST) - 0:09:22 - train - INFO - step: 000087 - done (%): 4.3 - loss: 2.399 - lr: 9.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5355.2 - avg_words_per_second: 5337.8 - ETA: >2025-11-14 21:02:57
2025-11-14 17:41:22 (IST) - 0:09:23 - train - INFO - [DocStream] step=88 microbatch=0 samples=2 unique_docs=2 runs=4247.wav[segment=11], 4248.wav[segment=0]
2025-11-14 17:41:22 (IST) - 0:09:23 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4247.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4248.wav
2025-11-14 17:41:22 (IST) - 0:09:23 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:22 (IST) - 0:09:23 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:22 (IST) - 0:09:23 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:28 (IST) - 0:09:29 - train - INFO - [TTT] Step 88: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=3.656e-01, relative_change=0.2902% (9 params)
2025-11-14 17:41:28 (IST) - 0:09:29 - train - INFO - step: 000088 - done (%): 4.4 - loss: 4.109 - lr: 9.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5349.2 - avg_words_per_second: 5337.9 - ETA: >2025-11-14 21:02:57
2025-11-14 17:41:28 (IST) - 0:09:29 - train - INFO - [DocStream] step=89 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=1-2]
2025-11-14 17:41:34 (IST) - 0:09:35 - train - INFO - [TTT] Step 89: grad_norm=1.000e+00, param_norm=125.9594, delta_norm=3.878e-01, relative_change=0.3078% (9 params)
2025-11-14 17:41:34 (IST) - 0:09:35 - train - INFO - step: 000089 - done (%): 4.5 - loss: 4.611 - lr: 9.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5341.8 - avg_words_per_second: 5338.0 - ETA: >2025-11-14 21:02:57
2025-11-14 17:41:35 (IST) - 0:09:35 - train - INFO - [DocStream] step=90 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=3-4]
2025-11-14 17:41:40 (IST) - 0:09:41 - train - INFO - [TTT] Step 90: grad_norm=1.000e+00, param_norm=125.9610, delta_norm=3.954e-01, relative_change=0.3139% (9 params)
2025-11-14 17:41:40 (IST) - 0:09:41 - train - INFO - step: 000090 - done (%): 4.5 - loss: 3.697 - lr: 9.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5353.0 - avg_words_per_second: 5338.2 - ETA: >2025-11-14 21:02:56
2025-11-14 17:41:41 (IST) - 0:09:42 - train - INFO - [DocStream] step=91 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=5-6]
2025-11-14 17:41:47 (IST) - 0:09:48 - train - INFO - [TTT] Step 91: grad_norm=8.183e-01, param_norm=125.9634, delta_norm=3.851e-01, relative_change=0.3057% (9 params)
2025-11-14 17:41:47 (IST) - 0:09:48 - train - INFO - step: 000091 - done (%): 4.5 - loss: 2.657 - lr: 9.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5355.6 - avg_words_per_second: 5338.4 - ETA: >2025-11-14 21:02:56
2025-11-14 17:41:47 (IST) - 0:09:48 - train - INFO - [DocStream] step=92 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=7-8]
2025-11-14 17:41:53 (IST) - 0:09:54 - train - INFO - [TTT] Step 92: grad_norm=7.085e-01, param_norm=125.9665, delta_norm=3.738e-01, relative_change=0.2967% (9 params)
2025-11-14 17:41:53 (IST) - 0:09:54 - train - INFO - step: 000092 - done (%): 4.6 - loss: 2.283 - lr: 9.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5358.3 - avg_words_per_second: 5338.6 - ETA: >2025-11-14 21:02:55
2025-11-14 17:41:53 (IST) - 0:09:54 - train - INFO - [DocStream] step=93 microbatch=0 samples=2 unique_docs=1 runs=4248.wav[segments=9-10]
2025-11-14 17:41:59 (IST) - 0:10:00 - train - INFO - [TTT] Step 93: grad_norm=8.108e-01, param_norm=125.9699, delta_norm=3.621e-01, relative_change=0.2875% (9 params)
2025-11-14 17:41:59 (IST) - 0:10:00 - train - INFO - step: 000093 - done (%): 4.7 - loss: 2.572 - lr: 9.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5348.6 - avg_words_per_second: 5338.7 - ETA: >2025-11-14 21:02:55
2025-11-14 17:42:00 (IST) - 0:10:01 - train - INFO - [DocStream] step=94 microbatch=0 samples=2 unique_docs=2 runs=4248.wav[segment=11], 4289.wav[segment=0]
2025-11-14 17:42:00 (IST) - 0:10:01 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4248.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4289.wav
2025-11-14 17:42:00 (IST) - 0:10:01 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:00 (IST) - 0:10:01 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:00 (IST) - 0:10:01 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:06 (IST) - 0:10:07 - train - INFO - [TTT] Step 94: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=3.586e-01, relative_change=0.2847% (9 params)
2025-11-14 17:42:06 (IST) - 0:10:07 - train - INFO - step: 000094 - done (%): 4.7 - loss: 3.212 - lr: 9.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5356.5 - avg_words_per_second: 5338.9 - ETA: >2025-11-14 21:02:55
2025-11-14 17:42:06 (IST) - 0:10:07 - train - INFO - [DocStream] step=95 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=1-2]
2025-11-14 17:42:12 (IST) - 0:10:13 - train - INFO - [TTT] Step 95: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.795e-01, relative_change=0.3013% (9 params)
2025-11-14 17:42:12 (IST) - 0:10:13 - train - INFO - step: 000095 - done (%): 4.8 - loss: 2.207 - lr: 9.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5356.1 - avg_words_per_second: 5339.0 - ETA: >2025-11-14 21:02:54
2025-11-14 17:42:12 (IST) - 0:10:13 - train - INFO - [DocStream] step=96 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=3-4]
2025-11-14 17:42:18 (IST) - 0:10:19 - train - INFO - [TTT] Step 96: grad_norm=1.000e+00, param_norm=125.9603, delta_norm=3.679e-01, relative_change=0.2921% (9 params)
2025-11-14 17:42:18 (IST) - 0:10:19 - train - INFO - step: 000096 - done (%): 4.8 - loss: 1.055 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5352.6 - avg_words_per_second: 5339.2 - ETA: >2025-11-14 21:02:54
2025-11-14 17:42:19 (IST) - 0:10:19 - train - INFO - [DocStream] step=97 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=5-6]
2025-11-14 17:42:25 (IST) - 0:10:25 - train - INFO - [TTT] Step 97: grad_norm=5.891e-01, param_norm=125.9623, delta_norm=3.701e-01, relative_change=0.2938% (9 params)
2025-11-14 17:42:25 (IST) - 0:10:25 - train - INFO - step: 000097 - done (%): 4.8 - loss: 0.761 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5355.6 - avg_words_per_second: 5339.4 - ETA: >2025-11-14 21:02:53
2025-11-14 17:42:25 (IST) - 0:10:26 - train - INFO - [DocStream] step=98 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=7-8]
2025-11-14 17:42:31 (IST) - 0:10:32 - train - INFO - [TTT] Step 98: grad_norm=7.275e-01, param_norm=125.9648, delta_norm=3.486e-01, relative_change=0.2767% (9 params)
2025-11-14 17:42:31 (IST) - 0:10:32 - train - INFO - step: 000098 - done (%): 4.9 - loss: 1.245 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5357.7 - avg_words_per_second: 5339.5 - ETA: >2025-11-14 21:02:53
2025-11-14 17:42:31 (IST) - 0:10:32 - train - INFO - [DocStream] step=99 microbatch=0 samples=2 unique_docs=1 runs=4289.wav[segments=9-10]
2025-11-14 17:42:37 (IST) - 0:10:38 - train - INFO - [TTT] Step 99: grad_norm=6.749e-01, param_norm=125.9677, delta_norm=3.319e-01, relative_change=0.2635% (9 params)
2025-11-14 17:42:37 (IST) - 0:10:38 - train - INFO - step: 000099 - done (%): 5.0 - loss: 1.262 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5348.9 - avg_words_per_second: 5339.6 - ETA: >2025-11-14 21:02:53
2025-11-14 17:42:38 (IST) - 0:10:38 - train - INFO - [DocStream] step=100 microbatch=0 samples=2 unique_docs=2 runs=4289.wav[segment=11], 4290.wav[segment=0]
2025-11-14 17:42:38 (IST) - 0:10:38 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4289.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4290.wav
2025-11-14 17:42:38 (IST) - 0:10:38 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:38 (IST) - 0:10:38 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:38 (IST) - 0:10:38 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:44 (IST) - 0:10:44 - train - INFO - [TTT] Step 100: grad_norm=1.000e+00, param_norm=125.9586, delta_norm=3.703e-01, relative_change=0.2940% (9 params)
2025-11-14 17:42:44 (IST) - 0:10:44 - train - INFO - step: 000100 - done (%): 5.0 - loss: 3.247 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.5 - avg_words_per_second: 5339.7 - ETA: >2025-11-14 21:02:53
2025-11-14 17:42:44 (IST) - 0:10:44 - checkpointing - INFO - Dumping checkpoint in /sise/eliyanac-group/ron_al/ttt_training_run2/checkpoints/checkpoint_000100/consolidated using tmp name: tmp.consolidated
2025-11-14 17:42:45 (IST) - 0:10:46 - checkpointing - INFO - Done dumping checkpoint in /sise/eliyanac-group/ron_al/ttt_training_run2/checkpoints/checkpoint_000100/consolidated for step: 100
2025-11-14 17:42:45 (IST) - 0:10:46 - checkpointing - INFO - Done deleting checkpoints 
2025-11-14 17:42:45 (IST) - 0:10:46 - checkpointing - INFO - Done!
2025-11-14 17:42:45 (IST) - 0:10:46 - train - INFO - [DocStream] step=101 microbatch=0 samples=2 unique_docs=1 runs=4290.wav[segments=1-2]
2025-11-14 17:42:51 (IST) - 0:10:52 - train - INFO - [TTT] Step 101: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=4.012e-01, relative_change=0.3185% (9 params)
2025-11-14 17:42:51 (IST) - 0:10:52 - train - INFO - step: 000101 - done (%): 5.0 - loss: 3.184 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5368.1 - avg_words_per_second: 5339.9 - ETA: >2025-11-14 21:02:53
2025-11-14 17:42:51 (IST) - 0:10:52 - train - INFO - [DocStream] step=102 microbatch=0 samples=2 unique_docs=1 runs=4290.wav[segments=3-4]
2025-11-14 17:42:57 (IST) - 0:10:58 - train - INFO - [TTT] Step 102: grad_norm=9.910e-01, param_norm=125.9606, delta_norm=4.018e-01, relative_change=0.3190% (9 params)
2025-11-14 17:42:57 (IST) - 0:10:58 - train - INFO - step: 000102 - done (%): 5.1 - loss: 1.708 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5361.2 - avg_words_per_second: 5340.2 - ETA: >2025-11-14 21:02:53
2025-11-14 17:42:58 (IST) - 0:10:59 - train - INFO - [DocStream] step=103 microbatch=0 samples=2 unique_docs=1 runs=4290.wav[segments=5-6]
2025-11-14 17:43:04 (IST) - 0:11:05 - train - INFO - [TTT] Step 103: grad_norm=9.114e-01, param_norm=125.9630, delta_norm=3.842e-01, relative_change=0.3050% (9 params)
2025-11-14 17:43:04 (IST) - 0:11:05 - train - INFO - step: 000103 - done (%): 5.2 - loss: 1.837 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5337.4 - avg_words_per_second: 5340.1 - ETA: >2025-11-14 21:02:53
2025-11-14 17:43:04 (IST) - 0:11:05 - train - INFO - [DocStream] step=104 microbatch=0 samples=2 unique_docs=1 runs=4290.wav[segments=7-8]
2025-11-14 17:43:10 (IST) - 0:11:11 - train - INFO - [TTT] Step 104: grad_norm=1.000e+00, param_norm=125.9658, delta_norm=3.799e-01, relative_change=0.3016% (9 params)
2025-11-14 17:43:10 (IST) - 0:11:11 - train - INFO - step: 000104 - done (%): 5.2 - loss: 2.706 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5364.7 - avg_words_per_second: 5340.4 - ETA: >2025-11-14 21:02:52
2025-11-14 17:43:10 (IST) - 0:11:11 - train - INFO - [DocStream] step=105 microbatch=0 samples=2 unique_docs=1 runs=4290.wav[segments=9-10]
2025-11-14 17:43:16 (IST) - 0:11:17 - train - INFO - [TTT] Step 105: grad_norm=1.000e+00, param_norm=125.9690, delta_norm=3.578e-01, relative_change=0.2841% (9 params)
2025-11-14 17:43:16 (IST) - 0:11:17 - train - INFO - step: 000105 - done (%): 5.2 - loss: 2.681 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.9 - avg_words_per_second: 5340.5 - ETA: >2025-11-14 21:02:52
2025-11-14 17:43:17 (IST) - 0:11:17 - train - INFO - [DocStream] step=106 microbatch=0 samples=2 unique_docs=2 runs=4290.wav[segment=11], 4310.wav[segment=0]
2025-11-14 17:43:17 (IST) - 0:11:17 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4290.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4310.wav
2025-11-14 17:43:17 (IST) - 0:11:17 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:17 (IST) - 0:11:17 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:17 (IST) - 0:11:17 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:23 (IST) - 0:11:23 - train - INFO - [TTT] Step 106: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=3.733e-01, relative_change=0.2964% (9 params)
2025-11-14 17:43:23 (IST) - 0:11:23 - train - INFO - step: 000106 - done (%): 5.3 - loss: 4.148 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5360.6 - avg_words_per_second: 5340.7 - ETA: >2025-11-14 21:02:52
2025-11-14 17:43:23 (IST) - 0:11:24 - train - INFO - [DocStream] step=107 microbatch=0 samples=2 unique_docs=1 runs=4310.wav[segments=1-2]
2025-11-14 17:43:29 (IST) - 0:11:30 - train - INFO - [TTT] Step 107: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.987e-01, relative_change=0.3165% (9 params)
2025-11-14 17:43:29 (IST) - 0:11:30 - train - INFO - step: 000107 - done (%): 5.3 - loss: 5.073 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5347.3 - avg_words_per_second: 5340.8 - ETA: >2025-11-14 21:02:51
2025-11-14 17:43:29 (IST) - 0:11:30 - train - INFO - [DocStream] step=108 microbatch=0 samples=2 unique_docs=1 runs=4310.wav[segments=3-4]
2025-11-14 17:43:35 (IST) - 0:11:36 - train - INFO - [TTT] Step 108: grad_norm=1.000e+00, param_norm=125.9609, delta_norm=4.195e-01, relative_change=0.3331% (9 params)
2025-11-14 17:43:35 (IST) - 0:11:36 - train - INFO - step: 000108 - done (%): 5.4 - loss: 4.610 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.4 - avg_words_per_second: 5340.8 - ETA: >2025-11-14 21:02:51
2025-11-14 17:43:36 (IST) - 0:11:36 - train - INFO - [DocStream] step=109 microbatch=0 samples=2 unique_docs=1 runs=4310.wav[segments=5-6]
2025-11-14 17:43:42 (IST) - 0:11:42 - train - INFO - [TTT] Step 109: grad_norm=1.000e+00, param_norm=125.9637, delta_norm=4.232e-01, relative_change=0.3360% (9 params)
2025-11-14 17:43:42 (IST) - 0:11:42 - train - INFO - step: 000109 - done (%): 5.5 - loss: 3.752 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.6 - avg_words_per_second: 5340.8 - ETA: >2025-11-14 21:02:51
2025-11-14 17:43:42 (IST) - 0:11:43 - train - INFO - [DocStream] step=110 microbatch=0 samples=2 unique_docs=1 runs=4310.wav[segments=7-8]
2025-11-14 17:43:48 (IST) - 0:11:49 - train - INFO - [TTT] Step 110: grad_norm=7.554e-01, param_norm=125.9672, delta_norm=4.102e-01, relative_change=0.3256% (9 params)
2025-11-14 17:43:48 (IST) - 0:11:49 - train - INFO - step: 000110 - done (%): 5.5 - loss: 2.723 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5344.4 - avg_words_per_second: 5340.9 - ETA: >2025-11-14 21:02:51
2025-11-14 17:43:48 (IST) - 0:11:49 - train - INFO - [DocStream] step=111 microbatch=0 samples=2 unique_docs=1 runs=4310.wav[segments=9-10]
2025-11-14 17:43:54 (IST) - 0:11:55 - train - INFO - [TTT] Step 111: grad_norm=8.547e-01, param_norm=125.9714, delta_norm=4.093e-01, relative_change=0.3249% (9 params)
2025-11-14 17:43:54 (IST) - 0:11:55 - train - INFO - step: 000111 - done (%): 5.5 - loss: 2.759 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5489.5 - avg_words_per_second: 5342.2 - ETA: >2025-11-14 21:02:48
2025-11-14 17:43:54 (IST) - 0:11:55 - train - INFO - [DocStream] step=112 microbatch=0 samples=2 unique_docs=1 runs=4315.wav[segments=0-1]
2025-11-14 17:43:54 (IST) - 0:11:55 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4310.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4315.wav
2025-11-14 17:43:54 (IST) - 0:11:55 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:54 (IST) - 0:11:55 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:54 (IST) - 0:11:55 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:00 (IST) - 0:12:01 - train - INFO - [TTT] Step 112: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=4.058e-01, relative_change=0.3221% (9 params)
2025-11-14 17:44:00 (IST) - 0:12:01 - train - INFO - step: 000112 - done (%): 5.6 - loss: 4.802 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5353.6 - avg_words_per_second: 5342.3 - ETA: >2025-11-14 21:02:48
2025-11-14 17:44:01 (IST) - 0:12:01 - train - INFO - [DocStream] step=113 microbatch=0 samples=2 unique_docs=1 runs=4315.wav[segments=2-3]
2025-11-14 17:44:07 (IST) - 0:12:07 - train - INFO - [TTT] Step 113: grad_norm=1.000e+00, param_norm=125.9596, delta_norm=4.148e-01, relative_change=0.3293% (9 params)
2025-11-14 17:44:07 (IST) - 0:12:07 - train - INFO - step: 000113 - done (%): 5.7 - loss: 4.093 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5345.6 - avg_words_per_second: 5342.3 - ETA: >2025-11-14 21:02:48
2025-11-14 17:44:07 (IST) - 0:12:08 - train - INFO - [DocStream] step=114 microbatch=0 samples=2 unique_docs=1 runs=4315.wav[segments=4-5]
2025-11-14 17:44:13 (IST) - 0:12:14 - train - INFO - [TTT] Step 114: grad_norm=1.000e+00, param_norm=125.9614, delta_norm=3.997e-01, relative_change=0.3174% (9 params)
2025-11-14 17:44:13 (IST) - 0:12:14 - train - INFO - step: 000114 - done (%): 5.7 - loss: 3.802 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5354.6 - avg_words_per_second: 5342.4 - ETA: >2025-11-14 21:02:48
2025-11-14 17:44:13 (IST) - 0:12:14 - train - INFO - [DocStream] step=115 microbatch=0 samples=2 unique_docs=1 runs=4315.wav[segments=6-7]
2025-11-14 17:44:19 (IST) - 0:12:20 - train - INFO - [TTT] Step 115: grad_norm=1.000e+00, param_norm=125.9639, delta_norm=3.994e-01, relative_change=0.3171% (9 params)
2025-11-14 17:44:19 (IST) - 0:12:20 - train - INFO - step: 000115 - done (%): 5.8 - loss: 2.178 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5348.4 - avg_words_per_second: 5342.5 - ETA: >2025-11-14 21:02:47
2025-11-14 17:44:20 (IST) - 0:12:20 - train - INFO - [DocStream] step=116 microbatch=0 samples=2 unique_docs=1 runs=4315.wav[segments=8-9]
2025-11-14 17:44:26 (IST) - 0:12:26 - train - INFO - [TTT] Step 116: grad_norm=1.000e+00, param_norm=125.9670, delta_norm=3.767e-01, relative_change=0.2991% (9 params)
2025-11-14 17:44:26 (IST) - 0:12:26 - train - INFO - step: 000116 - done (%): 5.8 - loss: 2.205 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5361.8 - avg_words_per_second: 5342.6 - ETA: >2025-11-14 21:02:47
2025-11-14 17:44:26 (IST) - 0:12:27 - train - INFO - [DocStream] step=117 microbatch=0 samples=2 unique_docs=1 runs=4315.wav[segments=10-11]
2025-11-14 17:44:32 (IST) - 0:12:33 - train - INFO - [TTT] Step 117: grad_norm=9.734e-01, param_norm=125.9705, delta_norm=3.646e-01, relative_change=0.2895% (9 params)
2025-11-14 17:44:32 (IST) - 0:12:33 - train - INFO - step: 000117 - done (%): 5.8 - loss: 2.121 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5357.4 - avg_words_per_second: 5342.7 - ETA: >2025-11-14 21:02:47
2025-11-14 17:44:32 (IST) - 0:12:33 - train - INFO - [DocStream] step=118 microbatch=0 samples=2 unique_docs=1 runs=4316.wav[segments=0-1]
2025-11-14 17:44:32 (IST) - 0:12:33 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4315.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4316.wav
2025-11-14 17:44:32 (IST) - 0:12:33 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:32 (IST) - 0:12:33 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:32 (IST) - 0:12:33 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:38 (IST) - 0:12:39 - train - INFO - [TTT] Step 118: grad_norm=1.000e+00, param_norm=125.9586, delta_norm=3.679e-01, relative_change=0.2921% (9 params)
2025-11-14 17:44:38 (IST) - 0:12:39 - train - INFO - step: 000118 - done (%): 5.9 - loss: 3.693 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5347.4 - avg_words_per_second: 5342.8 - ETA: >2025-11-14 21:02:47
2025-11-14 17:44:39 (IST) - 0:12:39 - train - INFO - [DocStream] step=119 microbatch=0 samples=2 unique_docs=1 runs=4316.wav[segments=2-3]
2025-11-14 17:44:45 (IST) - 0:12:45 - train - INFO - [TTT] Step 119: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=3.513e-01, relative_change=0.2789% (9 params)
2025-11-14 17:44:45 (IST) - 0:12:45 - train - INFO - step: 000119 - done (%): 6.0 - loss: 2.651 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5358.1 - avg_words_per_second: 5342.9 - ETA: >2025-11-14 21:02:46
2025-11-14 17:44:45 (IST) - 0:12:46 - train - INFO - [DocStream] step=120 microbatch=0 samples=2 unique_docs=1 runs=4316.wav[segments=4-5]
2025-11-14 17:44:51 (IST) - 0:12:52 - train - INFO - [TTT] Step 120: grad_norm=1.000e+00, param_norm=125.9599, delta_norm=3.616e-01, relative_change=0.2871% (9 params)
2025-11-14 17:44:51 (IST) - 0:12:52 - train - INFO - step: 000120 - done (%): 6.0 - loss: 1.994 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5355.7 - avg_words_per_second: 5343.0 - ETA: >2025-11-14 21:02:46
2025-11-14 17:44:51 (IST) - 0:12:52 - train - INFO - [DocStream] step=121 microbatch=0 samples=2 unique_docs=1 runs=4316.wav[segments=6-7]
2025-11-14 17:44:57 (IST) - 0:12:58 - train - INFO - [TTT] Step 121: grad_norm=1.000e+00, param_norm=125.9615, delta_norm=3.497e-01, relative_change=0.2776% (9 params)
2025-11-14 17:44:57 (IST) - 0:12:58 - train - INFO - step: 000121 - done (%): 6.0 - loss: 2.200 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5352.0 - avg_words_per_second: 5343.1 - ETA: >2025-11-14 21:02:46
2025-11-14 17:44:57 (IST) - 0:12:58 - train - INFO - [DocStream] step=122 microbatch=0 samples=2 unique_docs=1 runs=4316.wav[segments=8-9]
2025-11-14 17:45:03 (IST) - 0:13:04 - train - INFO - [TTT] Step 122: grad_norm=1.000e+00, param_norm=125.9638, delta_norm=3.557e-01, relative_change=0.2824% (9 params)
2025-11-14 17:45:03 (IST) - 0:13:04 - train - INFO - step: 000122 - done (%): 6.1 - loss: 2.610 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5360.6 - avg_words_per_second: 5343.2 - ETA: >2025-11-14 21:02:46
2025-11-14 17:45:04 (IST) - 0:13:05 - train - INFO - [DocStream] step=123 microbatch=0 samples=2 unique_docs=1 runs=4316.wav[segments=10-11]
2025-11-14 17:45:10 (IST) - 0:13:11 - train - INFO - [TTT] Step 123: grad_norm=1.000e+00, param_norm=125.9663, delta_norm=3.516e-01, relative_change=0.2791% (9 params)
2025-11-14 17:45:10 (IST) - 0:13:11 - train - INFO - step: 000123 - done (%): 6.2 - loss: 1.727 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5358.3 - avg_words_per_second: 5343.4 - ETA: >2025-11-14 21:02:45
2025-11-14 17:45:10 (IST) - 0:13:11 - train - INFO - [DocStream] step=124 microbatch=0 samples=2 unique_docs=1 runs=4325.wav[segments=0-1]
2025-11-14 17:45:10 (IST) - 0:13:11 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4316.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4325.wav
2025-11-14 17:45:10 (IST) - 0:13:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:10 (IST) - 0:13:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:10 (IST) - 0:13:11 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:16 (IST) - 0:13:17 - train - INFO - [TTT] Step 124: grad_norm=1.000e+00, param_norm=125.9586, delta_norm=3.570e-01, relative_change=0.2835% (9 params)
2025-11-14 17:45:16 (IST) - 0:13:17 - train - INFO - step: 000124 - done (%): 6.2 - loss: 3.879 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5356.3 - avg_words_per_second: 5343.5 - ETA: >2025-11-14 21:02:45
2025-11-14 17:45:16 (IST) - 0:13:17 - train - INFO - [DocStream] step=125 microbatch=0 samples=2 unique_docs=1 runs=4325.wav[segments=2-3]
2025-11-14 17:45:22 (IST) - 0:13:23 - train - INFO - [TTT] Step 125: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=3.728e-01, relative_change=0.2960% (9 params)
2025-11-14 17:45:22 (IST) - 0:13:23 - train - INFO - step: 000125 - done (%): 6.2 - loss: 3.393 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5352.5 - avg_words_per_second: 5343.5 - ETA: >2025-11-14 21:02:45
2025-11-14 17:45:23 (IST) - 0:13:23 - train - INFO - [DocStream] step=126 microbatch=0 samples=2 unique_docs=1 runs=4325.wav[segments=4-5]
2025-11-14 17:45:29 (IST) - 0:13:29 - train - INFO - [TTT] Step 126: grad_norm=1.000e+00, param_norm=125.9599, delta_norm=3.622e-01, relative_change=0.2875% (9 params)
2025-11-14 17:45:29 (IST) - 0:13:29 - train - INFO - step: 000126 - done (%): 6.3 - loss: 2.217 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5345.7 - avg_words_per_second: 5343.6 - ETA: >2025-11-14 21:02:45
2025-11-14 17:45:29 (IST) - 0:13:30 - train - INFO - [DocStream] step=127 microbatch=0 samples=2 unique_docs=1 runs=4325.wav[segments=6-7]
2025-11-14 17:45:35 (IST) - 0:13:36 - train - INFO - [TTT] Step 127: grad_norm=1.000e+00, param_norm=125.9615, delta_norm=3.659e-01, relative_change=0.2905% (9 params)
2025-11-14 17:45:35 (IST) - 0:13:36 - train - INFO - step: 000127 - done (%): 6.3 - loss: 1.560 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.2 - avg_words_per_second: 5343.7 - ETA: >2025-11-14 21:02:45
2025-11-14 17:45:35 (IST) - 0:13:36 - train - INFO - [DocStream] step=128 microbatch=0 samples=2 unique_docs=1 runs=4325.wav[segments=8-9]
2025-11-14 17:45:41 (IST) - 0:13:42 - train - INFO - [TTT] Step 128: grad_norm=1.000e+00, param_norm=125.9636, delta_norm=3.485e-01, relative_change=0.2767% (9 params)
2025-11-14 17:45:41 (IST) - 0:13:42 - train - INFO - step: 000128 - done (%): 6.4 - loss: 1.790 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5356.9 - avg_words_per_second: 5343.8 - ETA: >2025-11-14 21:02:44
2025-11-14 17:45:42 (IST) - 0:13:42 - train - INFO - [DocStream] step=129 microbatch=0 samples=2 unique_docs=1 runs=4325.wav[segments=10-11]
2025-11-14 17:45:48 (IST) - 0:13:48 - train - INFO - [TTT] Step 129: grad_norm=1.000e+00, param_norm=125.9660, delta_norm=3.287e-01, relative_change=0.2610% (9 params)
2025-11-14 17:45:48 (IST) - 0:13:48 - train - INFO - step: 000129 - done (%): 6.5 - loss: 1.459 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5357.8 - avg_words_per_second: 5343.9 - ETA: >2025-11-14 21:02:44
2025-11-14 17:45:48 (IST) - 0:13:49 - train - INFO - [DocStream] step=130 microbatch=0 samples=2 unique_docs=1 runs=4335.wav[segments=0-1]
2025-11-14 17:45:48 (IST) - 0:13:49 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4325.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4335.wav
2025-11-14 17:45:48 (IST) - 0:13:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:48 (IST) - 0:13:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:48 (IST) - 0:13:49 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:54 (IST) - 0:13:55 - train - INFO - [TTT] Step 130: grad_norm=1.000e+00, param_norm=125.9586, delta_norm=3.474e-01, relative_change=0.2758% (9 params)
2025-11-14 17:45:54 (IST) - 0:13:55 - train - INFO - step: 000130 - done (%): 6.5 - loss: 5.135 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.9 - avg_words_per_second: 5344.0 - ETA: >2025-11-14 21:02:44
2025-11-14 17:45:54 (IST) - 0:13:55 - train - INFO - [DocStream] step=131 microbatch=0 samples=2 unique_docs=1 runs=4335.wav[segments=2-3]
2025-11-14 17:46:00 (IST) - 0:14:01 - train - INFO - [TTT] Step 131: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=3.659e-01, relative_change=0.2905% (9 params)
2025-11-14 17:46:00 (IST) - 0:14:01 - train - INFO - step: 000131 - done (%): 6.5 - loss: 4.708 - lr: 1.0e-04 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5359.6 - avg_words_per_second: 5344.1 - ETA: >2025-11-14 21:02:44
2025-11-14 17:46:01 (IST) - 0:14:01 - train - INFO - [DocStream] step=132 microbatch=0 samples=2 unique_docs=1 runs=4335.wav[segments=4-5]
