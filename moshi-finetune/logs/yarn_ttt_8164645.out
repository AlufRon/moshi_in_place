==================================================
Job started at: Fri 14 Nov 2025 17:40:28 IST
Running on node: ise-6000-01.auth.ad.bgu.ac.il
GPU info:
Fri Nov 14 17:40:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:E1:00.0 Off |                  Off |
| 30%   27C    P8             25W /  300W |       2MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==================================================
Set CUDA_VISIBLE_DEVICES=0
Starting YARN + TTT training...
Context Extension: 4x (3000 -> 12000 tokens)
TTT Layers: 3 (layers 10, 20, 30)
Base Model: Frozen (only TTT params trained)
Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.
2025-11-14 17:40:35 (IST) - 0:00:04 - distributed - INFO - torch.cuda.device_count: 1
2025-11-14 17:40:35 (IST) - 0:00:04 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0
2025-11-14 17:40:35 (IST) - 0:00:04 - distributed - INFO - local rank: 0
2025-11-14 17:40:36 (IST) - 0:00:04 - train - INFO - Going to init comms...
2025-11-14 17:40:36 (IST) - 0:00:04 - train - INFO - Run dir: /sise/eliyanac-group/ron_al/ttt_training_run2
2025-11-14 17:40:36 (IST) - 0:00:04 - train - INFO - Removing run dir /sise/eliyanac-group/ron_al/ttt_training_run2...
2025-11-14 17:40:36 (IST) - 0:00:05 - train - INFO - TrainArgs: {'batch_size': 2,
 'ckpt_freq': 100,
 'data': {'eval_data': '',
          'shuffle': False,
          'train_data': '/sise/eliyanac-group/ron_al/talkbank_callhome_english/talkbank.jsonl'},
 'do_ckpt': True,
 'do_eval': False,
 'duration_sec': 150.0,
 'eval_freq': 100,
 'first_codebook_weight_multiplier': 100.0,
 'full_finetuning': False,
 'gradient_checkpointing': True,
 'log_freq': 1,
 'lora': {'enable': False, 'ft_embed': False, 'rank': 64, 'scaling': 2.0},
 'max_norm': 1.0,
 'max_steps': 2000,
 'moshi_paths': {'config_path': None,
                 'hf_repo_id': 'kyutai/moshiko-pytorch-bf16',
                 'mimi_path': None,
                 'moshi_path': None,
                 'tokenizer_path': None},
 'num_ckpt_keep': 3,
 'num_microbatches': 1,
 'optim': {'lr': 0.0001, 'pct_start': 0.05, 'weight_decay': 0.1},
 'overwrite_run_dir': True,
 'param_dtype': 'bfloat16',
 'run_dir': '/sise/eliyanac-group/ron_al/ttt_training_run2',
 'save_adapters': True,
 'seed': 0,
 'text_padding_weight': 0.5,
 'ttt': {'chunk_size': 256,
         'conv_kernel_size': 2,
         'delta_clip_fro_norm': 100.0,
         'enabled': True,
         'layer_frequency': 10,
         'learning_rate': 0.0001,
         'start_layer': 10,
         'unfreeze_ttt_layers': False},
 'wandb': {'key': '',
           'offline': False,
           'project': 'moshi_in_place',
           'run_name': 'run2'},
 'world_size': 1,
 'yarn': {'beta_fast': 32,
          'beta_slow': 1,
          'enabled': True,
          'mscale': 1.0,
          'mscale_all_dim': 0.0,
          'original_max_seq_len': 3000,
          'scale': 4.0}}
2025-11-14 17:40:36 (IST) - 0:00:05 - metrics_logger - INFO - initializing wandb
2025-11-14 17:40:39 (IST) - 0:00:07 - train - INFO - Loading Mimi and Moshi...
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - TTT (Test-Time Training) ENABLED
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Layer frequency: 10
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Start layer: 10
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Chunk size: 256
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Learning rate: 0.0001
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Conv kernel: 2
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - YaRN (Context Window Extension) ENABLED
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Scale: 4.0x
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Original max seq len: 3000
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Beta fast: 32
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO -   Beta slow: 1
2025-11-14 17:40:40 (IST) - 0:00:08 - finetune.wrapped_model - INFO - ======================================================================
[YaRN] Enabled with scale=4.0, original_len=3000
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[TTT] Enabled TTT gating: chunk_size=256, lr=0.0001, dim=4096, hidden=11264
[YaRN] Initializing RoPE buffers on device=meta
[YaRN] RoPE buffers initialized successfully
2025-11-14 17:40:40 (IST) - 0:00:09 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...
2025-11-14 17:40:40 (IST) - 0:00:09 - finetune.wrapped_model - INFO - Initializing TTT w_down from pretrained checkpoint...
2025-11-14 17:40:40 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ transformer.layers.10.gating.w_down <- transformer.layers.10.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:40:40 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ transformer.layers.20.gating.w_down <- transformer.layers.20.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:40:40 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ transformer.layers.30.gating.w_down <- transformer.layers.30.gating.linear_out.weight (shape: torch.Size([4096, 11264]), dtype: float32)
2025-11-14 17:40:40 (IST) - 0:00:09 - finetune.wrapped_model - INFO - Initializing TTT layers ...
2025-11-14 17:40:41 (IST) - 0:00:09 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:40:41 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.10.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:40:41 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.10.gating.w_down_pretrained from w_down
2025-11-14 17:40:41 (IST) - 0:00:10 - finetune.wrapped_model - WARNING - Buffer transformer.layers.10.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:40:42 (IST) - 0:00:10 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:40:42 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.20.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:40:42 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.20.gating.w_down_pretrained from w_down
2025-11-14 17:40:42 (IST) - 0:00:11 - finetune.wrapped_model - WARNING - Buffer transformer.layers.20.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:40:43 (IST) - 0:00:11 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.conv1d.conv.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO -   ✓ Small-random-initialized transformer.layers.30.gating.target_generator.W_target.weight (std=1e-4) for warm-start with gradient flow
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.layers.30.gating.w_down_pretrained from w_down
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - WARNING - Buffer transformer.layers.30.gating.ttt_clip_event_counter still meta - initializing as zeros
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO - Initializing YaRN RoPE buffers ...
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO -   ✓ Initialized transformer.rope.inv_freq (shape: torch.Size([64]), scale: 4.0x)
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO - Pinned 3 w_down parameters to float32 for TTT precision
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO - Finished initialization!
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO - TTT ACTIVE: 3 layers enabled
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO - TTT layer indices: [10, 20, 30]
2025-11-14 17:40:43 (IST) - 0:00:12 - finetune.wrapped_model - INFO - ======================================================================
2025-11-14 17:40:50 (IST) - 0:00:19 - train - INFO - [DocStream] step=1 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=0-1]
2025-11-14 17:40:50 (IST) - 0:00:19 - train - INFO - [TTT RESET] Document switch detected: None -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav
2025-11-14 17:40:50 (IST) - 0:00:19 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:50 (IST) - 0:00:19 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:50 (IST) - 0:00:19 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.367037
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.000000
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.000000
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.483805
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.000000
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.000000
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.794491
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.000000
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   requires_grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   has grad: True
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO -   grad norm: 0.000000
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - =========================

2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - [TTT] Step 1: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.697e-02, relative_change=0.0373% (9 params)
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - step: 000001 - done (%): 0.1 - loss: 3.378 - lr: 4.0e-06 - peak_alloc_mem (GB): 35.6 - alloc_mem (GB): 21.0 - words_per_second: 2524.1 - avg_words_per_second: 2524.1 - ETA: >2025-11-15 01:06:27
2025-11-14 17:40:59 (IST) - 0:00:28 - train - INFO - [DocStream] step=2 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=2-3]
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.383014
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.496659
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.778864
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   requires_grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   has grad: True
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - =========================

2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - [TTT] Step 2: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.879e-02, relative_change=0.0308% (9 params)
2025-11-14 17:41:05 (IST) - 0:00:34 - train - INFO - step: 000002 - done (%): 0.1 - loss: 3.898 - lr: 4.0e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5538.7 - avg_words_per_second: 3467.9 - ETA: >2025-11-14 23:05:10
2025-11-14 17:41:06 (IST) - 0:00:35 - train - INFO - [DocStream] step=3 microbatch=0 samples=2 unique_docs=1 runs=0638.wav[segments=4-5]
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.424343
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.515840
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.744205
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   requires_grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   has grad: True
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - =========================

2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - [TTT] Step 3: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.488e-02, relative_change=0.0277% (9 params)
2025-11-14 17:41:12 (IST) - 0:00:40 - train - INFO - step: 000003 - done (%): 0.1 - loss: 3.316 - lr: 4.1e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5120.9 - avg_words_per_second: 3886.0 - ETA: >2025-11-14 22:30:16
2025-11-14 17:41:12 (IST) - 0:00:41 - train - INFO - [DocStream] step=4 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=0-1]
2025-11-14 17:41:12 (IST) - 0:00:41 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/0638.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav
2025-11-14 17:41:12 (IST) - 0:00:41 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:12 (IST) - 0:00:41 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:12 (IST) - 0:00:41 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - 
=== TTT Parameter Debug ===
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.10.gating.w_down:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.406590
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.10.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.10.gating.target_generator.W_target.weight:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.20.gating.w_down:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.474107
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.20.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.20.gating.target_generator.W_target.weight:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.30.gating.w_down:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.780965
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.30.gating.target_generator.conv1d.conv.weight:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - transformer.layers.30.gating.target_generator.W_target.weight:
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   requires_grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   has grad: True
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO -   grad norm: 0.000000
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - =========================

2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - [TTT] Step 4: grad_norm=1.000e+00, param_norm=125.9593, delta_norm=3.275e-02, relative_change=0.0260% (9 params)
2025-11-14 17:41:18 (IST) - 0:00:46 - train - INFO - step: 000004 - done (%): 0.2 - loss: 4.809 - lr: 4.2e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5476.3 - avg_words_per_second: 4190.2 - ETA: >2025-11-14 22:09:14
2025-11-14 17:41:18 (IST) - 0:00:47 - train - INFO - [DocStream] step=5 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=2-3]
2025-11-14 17:41:24 (IST) - 0:00:53 - train - INFO - [TTT] Step 5: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.221e-02, relative_change=0.0256% (9 params)
2025-11-14 17:41:24 (IST) - 0:00:53 - train - INFO - step: 000005 - done (%): 0.2 - loss: 4.922 - lr: 4.4e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5448.4 - avg_words_per_second: 4393.1 - ETA: >2025-11-14 21:56:51
2025-11-14 17:41:24 (IST) - 0:00:53 - train - INFO - [DocStream] step=6 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=4-5]
2025-11-14 17:41:30 (IST) - 0:00:59 - train - INFO - [TTT] Step 6: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.271e-02, relative_change=0.0260% (9 params)
2025-11-14 17:41:30 (IST) - 0:00:59 - train - INFO - step: 000006 - done (%): 0.3 - loss: 3.316 - lr: 4.6e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5444.1 - avg_words_per_second: 4539.2 - ETA: >2025-11-14 21:48:36
2025-11-14 17:41:31 (IST) - 0:00:59 - train - INFO - [DocStream] step=7 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=6-7]
2025-11-14 17:41:36 (IST) - 0:01:05 - train - INFO - [TTT] Step 7: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.191e-02, relative_change=0.0253% (9 params)
2025-11-14 17:41:36 (IST) - 0:01:05 - train - INFO - step: 000007 - done (%): 0.3 - loss: 2.587 - lr: 4.9e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5440.3 - avg_words_per_second: 4649.2 - ETA: >2025-11-14 21:42:44
2025-11-14 17:41:37 (IST) - 0:01:05 - train - INFO - [DocStream] step=8 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=8-9]
2025-11-14 17:41:43 (IST) - 0:01:11 - train - INFO - [TTT] Step 8: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.219e-02, relative_change=0.0256% (9 params)
2025-11-14 17:41:43 (IST) - 0:01:11 - train - INFO - step: 000008 - done (%): 0.4 - loss: 2.286 - lr: 5.2e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5431.1 - avg_words_per_second: 4734.4 - ETA: >2025-11-14 21:38:23
2025-11-14 17:41:43 (IST) - 0:01:12 - train - INFO - [DocStream] step=9 microbatch=0 samples=2 unique_docs=1 runs=4065.wav[segments=10-11]
2025-11-14 17:41:49 (IST) - 0:01:18 - train - INFO - [TTT] Step 9: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.274e-02, relative_change=0.0260% (9 params)
2025-11-14 17:41:49 (IST) - 0:01:18 - train - INFO - step: 000009 - done (%): 0.5 - loss: 2.287 - lr: 5.5e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5403.7 - avg_words_per_second: 4800.4 - ETA: >2025-11-14 21:35:07
2025-11-14 17:41:49 (IST) - 0:01:18 - train - INFO - [DocStream] step=10 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=0-1]
2025-11-14 17:41:49 (IST) - 0:01:18 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4065.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav
2025-11-14 17:41:49 (IST) - 0:01:18 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:49 (IST) - 0:01:18 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:49 (IST) - 0:01:18 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:41:55 (IST) - 0:01:24 - train - INFO - [TTT] Step 10: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.360e-02, relative_change=0.0267% (9 params)
2025-11-14 17:41:55 (IST) - 0:01:24 - train - INFO - step: 000010 - done (%): 0.5 - loss: 5.038 - lr: 5.9e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5389.9 - avg_words_per_second: 4853.5 - ETA: >2025-11-14 21:32:33
2025-11-14 17:41:55 (IST) - 0:01:24 - train - INFO - [DocStream] step=11 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=2-3]
2025-11-14 17:42:01 (IST) - 0:01:30 - train - INFO - [TTT] Step 11: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=3.495e-02, relative_change=0.0277% (9 params)
2025-11-14 17:42:01 (IST) - 0:01:30 - train - INFO - step: 000011 - done (%): 0.6 - loss: 4.569 - lr: 6.4e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5379.6 - avg_words_per_second: 4897.1 - ETA: >2025-11-14 21:30:29
2025-11-14 17:42:02 (IST) - 0:01:30 - train - INFO - [DocStream] step=12 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=4-5]
2025-11-14 17:42:08 (IST) - 0:01:36 - train - INFO - [TTT] Step 12: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.656e-02, relative_change=0.0290% (9 params)
2025-11-14 17:42:08 (IST) - 0:01:36 - train - INFO - step: 000012 - done (%): 0.6 - loss: 2.698 - lr: 6.9e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5372.8 - avg_words_per_second: 4933.5 - ETA: >2025-11-14 21:28:48
2025-11-14 17:42:08 (IST) - 0:01:37 - train - INFO - [DocStream] step=13 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=6-7]
2025-11-14 17:42:14 (IST) - 0:01:43 - train - INFO - [TTT] Step 13: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=3.768e-02, relative_change=0.0299% (9 params)
2025-11-14 17:42:14 (IST) - 0:01:43 - train - INFO - step: 000013 - done (%): 0.7 - loss: 3.039 - lr: 7.4e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5360.0 - avg_words_per_second: 4963.8 - ETA: >2025-11-14 21:27:24
2025-11-14 17:42:14 (IST) - 0:01:43 - train - INFO - [DocStream] step=14 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=8-9]
2025-11-14 17:42:20 (IST) - 0:01:49 - train - INFO - [TTT] Step 14: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=3.952e-02, relative_change=0.0314% (9 params)
2025-11-14 17:42:20 (IST) - 0:01:49 - train - INFO - step: 000014 - done (%): 0.7 - loss: 2.849 - lr: 8.0e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5358.9 - avg_words_per_second: 4990.1 - ETA: >2025-11-14 21:26:12
2025-11-14 17:42:21 (IST) - 0:01:49 - train - INFO - [DocStream] step=15 microbatch=0 samples=2 unique_docs=1 runs=4074.wav[segments=10-11]
2025-11-14 17:42:27 (IST) - 0:01:55 - train - INFO - [TTT] Step 15: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=4.244e-02, relative_change=0.0337% (9 params)
2025-11-14 17:42:27 (IST) - 0:01:55 - train - INFO - step: 000015 - done (%): 0.8 - loss: 2.746 - lr: 8.7e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5358.4 - avg_words_per_second: 5013.1 - ETA: >2025-11-14 21:25:10
2025-11-14 17:42:27 (IST) - 0:01:56 - train - INFO - [DocStream] step=16 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=0-1]
2025-11-14 17:42:27 (IST) - 0:01:56 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4074.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav
2025-11-14 17:42:27 (IST) - 0:01:56 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:27 (IST) - 0:01:56 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:27 (IST) - 0:01:56 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:42:33 (IST) - 0:02:02 - train - INFO - [TTT] Step 16: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=4.425e-02, relative_change=0.0351% (9 params)
2025-11-14 17:42:33 (IST) - 0:02:02 - train - INFO - step: 000016 - done (%): 0.8 - loss: 4.238 - lr: 9.3e-06 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5346.5 - avg_words_per_second: 5032.7 - ETA: >2025-11-14 21:24:18
2025-11-14 17:42:33 (IST) - 0:02:02 - train - INFO - [DocStream] step=17 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=2-3]
2025-11-14 17:42:39 (IST) - 0:02:08 - train - INFO - [TTT] Step 17: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=4.679e-02, relative_change=0.0371% (9 params)
2025-11-14 17:42:39 (IST) - 0:02:08 - train - INFO - step: 000017 - done (%): 0.8 - loss: 3.979 - lr: 1.0e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5325.8 - avg_words_per_second: 5049.1 - ETA: >2025-11-14 21:23:34
2025-11-14 17:42:40 (IST) - 0:02:08 - train - INFO - [DocStream] step=18 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=4-5]
2025-11-14 17:42:46 (IST) - 0:02:14 - train - INFO - [TTT] Step 18: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=4.926e-02, relative_change=0.0391% (9 params)
2025-11-14 17:42:46 (IST) - 0:02:14 - train - INFO - step: 000018 - done (%): 0.9 - loss: 2.508 - lr: 1.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5342.6 - avg_words_per_second: 5064.5 - ETA: >2025-11-14 21:22:54
2025-11-14 17:42:46 (IST) - 0:02:15 - train - INFO - [DocStream] step=19 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=6-7]
2025-11-14 17:42:52 (IST) - 0:02:21 - train - INFO - [TTT] Step 19: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=5.192e-02, relative_change=0.0412% (9 params)
2025-11-14 17:42:52 (IST) - 0:02:21 - train - INFO - step: 000019 - done (%): 0.9 - loss: 3.824 - lr: 1.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5327.0 - avg_words_per_second: 5077.7 - ETA: >2025-11-14 21:22:19
2025-11-14 17:42:52 (IST) - 0:02:21 - train - INFO - [DocStream] step=20 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=8-9]
2025-11-14 17:42:58 (IST) - 0:02:27 - train - INFO - [TTT] Step 20: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=5.548e-02, relative_change=0.0440% (9 params)
2025-11-14 17:42:58 (IST) - 0:02:27 - train - INFO - step: 000020 - done (%): 1.0 - loss: 3.386 - lr: 1.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5343.3 - avg_words_per_second: 5090.3 - ETA: >2025-11-14 21:21:46
2025-11-14 17:42:59 (IST) - 0:02:27 - train - INFO - [DocStream] step=21 microbatch=0 samples=2 unique_docs=1 runs=4077.wav[segments=10-11]
2025-11-14 17:43:05 (IST) - 0:02:33 - train - INFO - [TTT] Step 21: grad_norm=9.074e-01, param_norm=125.9588, delta_norm=5.798e-02, relative_change=0.0460% (9 params)
2025-11-14 17:43:05 (IST) - 0:02:33 - train - INFO - step: 000021 - done (%): 1.1 - loss: 1.924 - lr: 1.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5327.4 - avg_words_per_second: 5101.1 - ETA: >2025-11-14 21:21:18
2025-11-14 17:43:05 (IST) - 0:02:34 - train - INFO - [DocStream] step=22 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=0-1]
2025-11-14 17:43:05 (IST) - 0:02:34 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4077.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav
2025-11-14 17:43:05 (IST) - 0:02:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:05 (IST) - 0:02:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:05 (IST) - 0:02:34 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:11 (IST) - 0:02:40 - train - INFO - [TTT] Step 22: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=6.221e-02, relative_change=0.0494% (9 params)
2025-11-14 17:43:11 (IST) - 0:02:40 - train - INFO - step: 000022 - done (%): 1.1 - loss: 3.864 - lr: 1.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5327.5 - avg_words_per_second: 5111.0 - ETA: >2025-11-14 21:20:52
2025-11-14 17:43:11 (IST) - 0:02:40 - train - INFO - [DocStream] step=23 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=2-3]
2025-11-14 17:43:17 (IST) - 0:02:46 - train - INFO - [TTT] Step 23: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=6.682e-02, relative_change=0.0530% (9 params)
2025-11-14 17:43:17 (IST) - 0:02:46 - train - INFO - step: 000023 - done (%): 1.1 - loss: 3.712 - lr: 1.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5328.4 - avg_words_per_second: 5120.1 - ETA: >2025-11-14 21:20:29
2025-11-14 17:43:18 (IST) - 0:02:46 - train - INFO - [DocStream] step=24 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=4-5]
2025-11-14 17:43:24 (IST) - 0:02:52 - train - INFO - [TTT] Step 24: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=7.161e-02, relative_change=0.0569% (9 params)
2025-11-14 17:43:24 (IST) - 0:02:52 - train - INFO - step: 000024 - done (%): 1.2 - loss: 2.495 - lr: 1.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5325.1 - avg_words_per_second: 5128.3 - ETA: >2025-11-14 21:20:08
2025-11-14 17:43:24 (IST) - 0:02:53 - train - INFO - [DocStream] step=25 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=6-7]
2025-11-14 17:43:30 (IST) - 0:02:59 - train - INFO - [TTT] Step 25: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=7.577e-02, relative_change=0.0602% (9 params)
2025-11-14 17:43:30 (IST) - 0:02:59 - train - INFO - step: 000025 - done (%): 1.2 - loss: 2.505 - lr: 1.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5311.4 - avg_words_per_second: 5135.4 - ETA: >2025-11-14 21:19:50
2025-11-14 17:43:30 (IST) - 0:02:59 - train - INFO - [DocStream] step=26 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=8-9]
2025-11-14 17:43:36 (IST) - 0:03:05 - train - INFO - [TTT] Step 26: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=7.803e-02, relative_change=0.0619% (9 params)
2025-11-14 17:43:36 (IST) - 0:03:05 - train - INFO - step: 000026 - done (%): 1.3 - loss: 1.751 - lr: 1.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5326.1 - avg_words_per_second: 5142.5 - ETA: >2025-11-14 21:19:32
2025-11-14 17:43:37 (IST) - 0:03:05 - train - INFO - [DocStream] step=27 microbatch=0 samples=2 unique_docs=1 runs=4092.wav[segments=10-11]
2025-11-14 17:43:43 (IST) - 0:03:11 - train - INFO - [TTT] Step 27: grad_norm=8.355e-01, param_norm=125.9588, delta_norm=8.102e-02, relative_change=0.0643% (9 params)
2025-11-14 17:43:43 (IST) - 0:03:11 - train - INFO - step: 000027 - done (%): 1.4 - loss: 2.360 - lr: 1.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5326.5 - avg_words_per_second: 5149.1 - ETA: >2025-11-14 21:19:15
2025-11-14 17:43:43 (IST) - 0:03:12 - train - INFO - [DocStream] step=28 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=0-1]
2025-11-14 17:43:43 (IST) - 0:03:12 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4092.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav
2025-11-14 17:43:43 (IST) - 0:03:12 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:43 (IST) - 0:03:12 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:43 (IST) - 0:03:12 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:43:49 (IST) - 0:03:18 - train - INFO - [TTT] Step 28: grad_norm=1.000e+00, param_norm=125.9591, delta_norm=8.630e-02, relative_change=0.0685% (9 params)
2025-11-14 17:43:49 (IST) - 0:03:18 - train - INFO - step: 000028 - done (%): 1.4 - loss: 3.956 - lr: 2.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5322.1 - avg_words_per_second: 5155.1 - ETA: >2025-11-14 21:19:00
2025-11-14 17:43:49 (IST) - 0:03:18 - train - INFO - [DocStream] step=29 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=2-3]
2025-11-14 17:43:55 (IST) - 0:03:24 - train - INFO - [TTT] Step 29: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=9.262e-02, relative_change=0.0735% (9 params)
2025-11-14 17:43:55 (IST) - 0:03:24 - train - INFO - step: 000029 - done (%): 1.4 - loss: 3.924 - lr: 2.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5314.1 - avg_words_per_second: 5160.4 - ETA: >2025-11-14 21:18:46
2025-11-14 17:43:56 (IST) - 0:03:24 - train - INFO - [DocStream] step=30 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=4-5]
2025-11-14 17:44:02 (IST) - 0:03:30 - train - INFO - [TTT] Step 30: grad_norm=9.195e-01, param_norm=125.9588, delta_norm=9.696e-02, relative_change=0.0770% (9 params)
2025-11-14 17:44:02 (IST) - 0:03:30 - train - INFO - step: 000030 - done (%): 1.5 - loss: 1.921 - lr: 2.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5327.7 - avg_words_per_second: 5165.8 - ETA: >2025-11-14 21:18:32
2025-11-14 17:44:02 (IST) - 0:03:31 - train - INFO - [DocStream] step=31 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=6-7]
2025-11-14 17:44:08 (IST) - 0:03:37 - train - INFO - [TTT] Step 31: grad_norm=8.393e-01, param_norm=125.9588, delta_norm=1.019e-01, relative_change=0.0809% (9 params)
2025-11-14 17:44:08 (IST) - 0:03:37 - train - INFO - step: 000031 - done (%): 1.6 - loss: 2.196 - lr: 2.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5321.6 - avg_words_per_second: 5170.7 - ETA: >2025-11-14 21:18:20
2025-11-14 17:44:08 (IST) - 0:03:37 - train - INFO - [DocStream] step=32 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=8-9]
2025-11-14 17:44:14 (IST) - 0:03:43 - train - INFO - [TTT] Step 32: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.033e-01, relative_change=0.0820% (9 params)
2025-11-14 17:44:14 (IST) - 0:03:43 - train - INFO - step: 000032 - done (%): 1.6 - loss: 1.997 - lr: 2.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5299.6 - avg_words_per_second: 5174.6 - ETA: >2025-11-14 21:18:10
2025-11-14 17:44:15 (IST) - 0:03:43 - train - INFO - [DocStream] step=33 microbatch=0 samples=2 unique_docs=1 runs=4104.wav[segments=10-11]
2025-11-14 17:44:21 (IST) - 0:03:49 - train - INFO - [TTT] Step 33: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=1.070e-01, relative_change=0.0850% (9 params)
2025-11-14 17:44:21 (IST) - 0:03:49 - train - INFO - step: 000033 - done (%): 1.6 - loss: 1.840 - lr: 2.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5322.6 - avg_words_per_second: 5179.0 - ETA: >2025-11-14 21:17:59
2025-11-14 17:44:21 (IST) - 0:03:50 - train - INFO - [DocStream] step=34 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=0-1]
2025-11-14 17:44:21 (IST) - 0:03:50 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4104.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav
2025-11-14 17:44:21 (IST) - 0:03:50 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:21 (IST) - 0:03:50 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:21 (IST) - 0:03:50 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:27 (IST) - 0:03:56 - train - INFO - [TTT] Step 34: grad_norm=1.000e+00, param_norm=125.9590, delta_norm=1.114e-01, relative_change=0.0885% (9 params)
2025-11-14 17:44:27 (IST) - 0:03:56 - train - INFO - step: 000034 - done (%): 1.7 - loss: 4.182 - lr: 2.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5318.6 - avg_words_per_second: 5183.0 - ETA: >2025-11-14 21:17:49
2025-11-14 17:44:27 (IST) - 0:03:56 - train - INFO - [DocStream] step=35 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=2-3]
2025-11-14 17:44:33 (IST) - 0:04:02 - train - INFO - [TTT] Step 35: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.184e-01, relative_change=0.0940% (9 params)
2025-11-14 17:44:33 (IST) - 0:04:02 - train - INFO - step: 000035 - done (%): 1.8 - loss: 3.352 - lr: 2.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5319.7 - avg_words_per_second: 5186.8 - ETA: >2025-11-14 21:17:39
2025-11-14 17:44:34 (IST) - 0:04:02 - train - INFO - [DocStream] step=36 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=4-5]
2025-11-14 17:44:40 (IST) - 0:04:08 - train - INFO - [TTT] Step 36: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=1.235e-01, relative_change=0.0981% (9 params)
2025-11-14 17:44:40 (IST) - 0:04:08 - train - INFO - step: 000036 - done (%): 1.8 - loss: 2.836 - lr: 3.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5310.5 - avg_words_per_second: 5190.1 - ETA: >2025-11-14 21:17:31
2025-11-14 17:44:40 (IST) - 0:04:09 - train - INFO - [DocStream] step=37 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=6-7]
2025-11-14 17:44:46 (IST) - 0:04:15 - train - INFO - [TTT] Step 37: grad_norm=9.860e-01, param_norm=125.9588, delta_norm=1.305e-01, relative_change=0.1036% (9 params)
2025-11-14 17:44:46 (IST) - 0:04:15 - train - INFO - step: 000037 - done (%): 1.9 - loss: 2.548 - lr: 3.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5311.6 - avg_words_per_second: 5193.4 - ETA: >2025-11-14 21:17:23
2025-11-14 17:44:46 (IST) - 0:04:15 - train - INFO - [DocStream] step=38 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=8-9]
2025-11-14 17:44:52 (IST) - 0:04:21 - train - INFO - [TTT] Step 38: grad_norm=7.922e-01, param_norm=125.9589, delta_norm=1.349e-01, relative_change=0.1071% (9 params)
2025-11-14 17:44:52 (IST) - 0:04:21 - train - INFO - step: 000038 - done (%): 1.9 - loss: 2.290 - lr: 3.3e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5314.4 - avg_words_per_second: 5196.5 - ETA: >2025-11-14 21:17:15
2025-11-14 17:44:53 (IST) - 0:04:21 - train - INFO - [DocStream] step=39 microbatch=0 samples=2 unique_docs=1 runs=4112.wav[segments=10-11]
2025-11-14 17:44:59 (IST) - 0:04:28 - train - INFO - [TTT] Step 39: grad_norm=7.940e-01, param_norm=125.9592, delta_norm=1.387e-01, relative_change=0.1101% (9 params)
2025-11-14 17:44:59 (IST) - 0:04:28 - train - INFO - step: 000039 - done (%): 1.9 - loss: 1.915 - lr: 3.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5305.8 - avg_words_per_second: 5199.2 - ETA: >2025-11-14 21:17:08
2025-11-14 17:44:59 (IST) - 0:04:28 - train - INFO - [DocStream] step=40 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=0-1]
2025-11-14 17:44:59 (IST) - 0:04:28 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4112.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav
2025-11-14 17:44:59 (IST) - 0:04:28 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:59 (IST) - 0:04:28 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:44:59 (IST) - 0:04:28 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:05 (IST) - 0:04:34 - train - INFO - [TTT] Step 40: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=1.450e-01, relative_change=0.1151% (9 params)
2025-11-14 17:45:05 (IST) - 0:04:34 - train - INFO - step: 000040 - done (%): 2.0 - loss: 4.980 - lr: 3.6e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5307.0 - avg_words_per_second: 5201.9 - ETA: >2025-11-14 21:17:02
2025-11-14 17:45:05 (IST) - 0:04:34 - train - INFO - [DocStream] step=41 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=2-3]
2025-11-14 17:45:12 (IST) - 0:04:40 - train - INFO - [TTT] Step 41: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.514e-01, relative_change=0.1202% (9 params)
2025-11-14 17:45:12 (IST) - 0:04:40 - train - INFO - step: 000041 - done (%): 2.0 - loss: 3.761 - lr: 3.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5311.2 - avg_words_per_second: 5204.5 - ETA: >2025-11-14 21:16:55
2025-11-14 17:45:12 (IST) - 0:04:41 - train - INFO - [DocStream] step=42 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=4-5]
2025-11-14 17:45:18 (IST) - 0:04:47 - train - INFO - [TTT] Step 42: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=1.577e-01, relative_change=0.1252% (9 params)
2025-11-14 17:45:18 (IST) - 0:04:47 - train - INFO - step: 000042 - done (%): 2.1 - loss: 3.611 - lr: 3.9e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5301.0 - avg_words_per_second: 5206.7 - ETA: >2025-11-14 21:16:50
2025-11-14 17:45:18 (IST) - 0:04:47 - train - INFO - [DocStream] step=43 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=6-7]
2025-11-14 17:45:24 (IST) - 0:04:53 - train - INFO - [TTT] Step 43: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=1.633e-01, relative_change=0.1296% (9 params)
2025-11-14 17:45:24 (IST) - 0:04:53 - train - INFO - step: 000043 - done (%): 2.1 - loss: 1.782 - lr: 4.1e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5303.9 - avg_words_per_second: 5208.9 - ETA: >2025-11-14 21:16:44
2025-11-14 17:45:25 (IST) - 0:04:53 - train - INFO - [DocStream] step=44 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=8-9]
2025-11-14 17:45:31 (IST) - 0:04:59 - train - INFO - [TTT] Step 44: grad_norm=1.000e+00, param_norm=125.9592, delta_norm=1.644e-01, relative_change=0.1305% (9 params)
2025-11-14 17:45:31 (IST) - 0:04:59 - train - INFO - step: 000044 - done (%): 2.2 - loss: 2.462 - lr: 4.2e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5307.9 - avg_words_per_second: 5211.2 - ETA: >2025-11-14 21:16:39
2025-11-14 17:45:31 (IST) - 0:05:00 - train - INFO - [DocStream] step=45 microbatch=0 samples=2 unique_docs=1 runs=4145.wav[segments=10-11]
2025-11-14 17:45:37 (IST) - 0:05:06 - train - INFO - [TTT] Step 45: grad_norm=9.079e-01, param_norm=125.9597, delta_norm=1.690e-01, relative_change=0.1342% (9 params)
2025-11-14 17:45:37 (IST) - 0:05:06 - train - INFO - step: 000045 - done (%): 2.2 - loss: 2.109 - lr: 4.4e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5300.4 - avg_words_per_second: 5213.1 - ETA: >2025-11-14 21:16:34
2025-11-14 17:45:37 (IST) - 0:05:06 - train - INFO - [DocStream] step=46 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=0-1]
2025-11-14 17:45:37 (IST) - 0:05:06 - train - INFO - [TTT RESET] Document switch detected: /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4145.wav -> /sise/eliyanac-group/ron_al/talkbank_callhome_english/wav/4156.wav
2025-11-14 17:45:37 (IST) - 0:05:06 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 71.462822 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:37 (IST) - 0:05:06 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.982422, w_down norm 72.818527 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:37 (IST) - 0:05:06 - moshi.modules.ttt_module - INFO - [TTT RESET][train] target_generator norm 0.980469, w_down norm 73.856461 (dtype: torch.float32); both learn via backprop during training
2025-11-14 17:45:43 (IST) - 0:05:12 - train - INFO - [TTT] Step 46: grad_norm=1.000e+00, param_norm=125.9589, delta_norm=1.757e-01, relative_change=0.1395% (9 params)
2025-11-14 17:45:43 (IST) - 0:05:12 - train - INFO - step: 000046 - done (%): 2.3 - loss: 4.402 - lr: 4.5e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5211.0 - avg_words_per_second: 5213.1 - ETA: >2025-11-14 21:16:34
2025-11-14 17:45:44 (IST) - 0:05:13 - train - INFO - [DocStream] step=47 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=2-3]
2025-11-14 17:45:50 (IST) - 0:05:19 - train - INFO - [TTT] Step 47: grad_norm=1.000e+00, param_norm=125.9587, delta_norm=1.864e-01, relative_change=0.1480% (9 params)
2025-11-14 17:45:50 (IST) - 0:05:19 - train - INFO - step: 000047 - done (%): 2.4 - loss: 4.594 - lr: 4.7e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5303.3 - avg_words_per_second: 5214.9 - ETA: >2025-11-14 21:16:29
2025-11-14 17:45:50 (IST) - 0:05:19 - train - INFO - [DocStream] step=48 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=4-5]
2025-11-14 17:45:56 (IST) - 0:05:25 - train - INFO - [TTT] Step 48: grad_norm=1.000e+00, param_norm=125.9588, delta_norm=1.940e-01, relative_change=0.1540% (9 params)
2025-11-14 17:45:56 (IST) - 0:05:25 - train - INFO - step: 000048 - done (%): 2.4 - loss: 4.697 - lr: 4.8e-05 - peak_alloc_mem (GB): 38.8 - alloc_mem (GB): 21.0 - words_per_second: 5299.6 - avg_words_per_second: 5216.7 - ETA: >2025-11-14 21:16:25
2025-11-14 17:45:57 (IST) - 0:05:25 - train - INFO - [DocStream] step=49 microbatch=0 samples=2 unique_docs=1 runs=4156.wav[segments=6-7]
