# In-Place TTT Training Integration with moshi-finetune

## Summary: YES, We Can Use moshi-finetune! âœ…

The `moshi-finetune` codebase is **perfectly compatible** with our In-Place TTT implementation. We only need minor additions to pass TTT configuration through the training pipeline.

---

## How moshi-finetune Works

### Architecture Overview
```
train.py
  â”œâ”€ Loads TrainArgs from YAML config
  â”œâ”€ Creates CheckpointInfo (from HF or local)
  â”œâ”€ Gets FSDP-wrapped model via get_fsdp_model()
  â”‚   â””â”€ Calls checkpointer_info.get_moshi(lm_kwargs_overrides={...})
  â”‚       â””â”€ Calls get_moshi_lm() with lm_kwargs
  â”‚           â””â”€ Creates LMModel(**lm_kwargs)
  â”‚
  â””â”€ Training loop:
      - forward: output = model(codes, condition_tensors)
      - backward: loss.backward()
      - optimizer step
      - checkpoint saving
```

### Key Observation
The `lm_kwargs_overrides` parameter allows **arbitrary kwargs** to be passed to `LMModel.__init__()`. This is exactly what we need for TTT!

---

## Integration Strategy

### Option 1: Minimal Changes (RECOMMENDED)
Add TTT parameters to the existing config structure without breaking anything.

#### Changes Required:

##### 1. Add TTT config to `finetune/args.py`
```python
@dataclass
class TTTArgs(Serializable):
    """In-Place Test-Time Training configuration."""
    enabled: bool = False
    layer_frequency: int = 6  # Apply TTT every Nth layer
    chunk_size: int = 256     # Tokens per TTT update chunk
    learning_rate: float = 1e-3  # TTT fast weight learning rate
    conv_kernel_size: int = 2    # Conv1D kernel for target generation
    start_layer: int = 5         # First layer to apply TTT (0-indexed)

    def __post_init__(self) -> None:
        if self.enabled:
            assert self.layer_frequency > 0
            assert self.chunk_size > 0
            assert self.learning_rate > 0
            assert self.conv_kernel_size > 0
            assert self.start_layer >= 0


@dataclass
class TrainArgs(Serializable):
    # ... existing fields ...
    
    # NEW: TTT configuration
    ttt: TTTArgs = field(default_factory=TTTArgs)
```

##### 2. Modify `finetune/wrapped_model.py`
```python
def get_fsdp_model(
    args: TrainArgs, checkpointer_info: CheckpointInfo
) -> FullyShardedDataParallel | LMModel:
    # ... existing code ...
    
    with torch.device("meta"):
        model = checkpointer_info.get_moshi(
            device="meta",
            dtype=param_dtype,
            lm_kwargs_overrides={
                "gradient_checkpointing": args.gradient_checkpointing,
                "lora": args.lora.enable,
                "lora_rank": args.lora.rank,
                "lora_scaling": args.lora.scaling,
                # NEW: TTT parameters
                "ttt_enabled": args.ttt.enabled,
                "ttt_layer_frequency": args.ttt.layer_frequency,
                "ttt_chunk_size": args.ttt.chunk_size,
                "ttt_learning_rate": args.ttt.learning_rate,
                "ttt_conv_kernel_size": args.ttt.conv_kernel_size,
            },
            load_weight=False,
        )
    
    # ... rest unchanged ...
```

##### 3. Update example YAML config
```yaml
# example/moshi_7B_ttt.yaml

# ... all existing fields ...

# NEW: In-Place TTT configuration
ttt:
  enabled: true              # Enable TTT
  layer_frequency: 6         # Apply TTT every 6th layer (5, 11, 17, 23, 29)
  chunk_size: 256           # Process 256 tokens per chunk
  learning_rate: 0.001      # TTT fast weight learning rate (eta)
  conv_kernel_size: 2       # Look at next token
  start_layer: 5            # Start TTT from layer 5

# Note: TTT is compatible with both LoRA and full finetuning
# For best results with TTT, consider:
# - full_finetuning: true (to also train W_target and Conv1D)
# - gradient_checkpointing: true (TTT adds memory overhead)
```

---

## Training Flow with TTT

### Forward Pass (Automatic)
```python
# In train.py line 256:
output = model(codes=codes, condition_tensors=condition_tensors)

# This automatically triggers:
# 1. LMModel.forward() â†’ forward_text()
# 2. Token embeddings extracted and saved
# 3. StreamingTransformer.forward(x, token_embeddings)
# 4. For TTT-enabled layers (5, 11, 17, 23, 29):
#    a. TTTGating._ttt_forward(x, token_embeddings)
#    b. Compute Z (intermediate activations)
#    c. Generate VÌ‚ = Conv1D(token_embeddings) @ W_target
#    d. Chunk-wise parallel TTT update
#    e. Apply updated W_down to get output
```

### Backward Pass (Automatic)
```python
# In train.py line 274:
mb_loss.backward()

# Gradients flow through:
# 1. Standard next-token prediction loss
# 2. Through TTT outputs
# 3. To W_target (learnable target projection)
# 4. To Conv1D weights (learnable future-token weights)
# 5. To W_down (fast weights base)
# 6. To all other model parameters
```

### What Gets Trained

#### With LoRA + TTT:
- LoRA adapters (A, B matrices)
- TTT components:
  - `W_target` (target projection in each TTT layer)
  - `Conv1D` weights (future token attention in each TTT layer)
  - `W_down` base weights (fast weights initialization)

#### With Full Finetuning + TTT:
- ALL model parameters
- TTT components (same as above)

---

## Memory Considerations

### Memory Impact
```
Base model: M parameters
TTT adds:
  - Per TTT layer: W_target (d_model Ã— d_model) + Conv1D weights (small)
  - Runtime: W_down states per chunk (batch_size Ã— d_ff Ã— d_model)
  
For 5 TTT layers with d_model=128, d_ff=512:
  - W_target: 5 Ã— (128 Ã— 128) Ã— 4 bytes â‰ˆ 320 KB
  - Conv1D: 5 Ã— (128 Ã— 128 Ã— kernel_size) Ã— 4 bytes â‰ˆ 640 KB (kernel_size=2)
  - Runtime states: batch_size Ã— 5 Ã— (512 Ã— 128) Ã— 4 bytes
    - batch_size=16: â‰ˆ 20 MB
    - batch_size=32: â‰ˆ 40 MB
    
Total overhead: ~1-2% of model memory, NEGLIGIBLE!
```

### Gradient Checkpointing
- Already enabled in moshi-finetune: `gradient_checkpointing: true`
- Works seamlessly with TTT
- Reduces memory at cost of recomputation

---

## FSDP Compatibility

### Current FSDP Policy
```python
# From wrapped_model.py:
transformer_block_wrap_policy = functools.partial(
    torch_wrap.transformer_auto_wrap_policy,
    transformer_layer_cls=(StreamingTransformerLayer,),
)
```

### TTT Compatibility: âœ… WORKS OUT OF THE BOX
- Each `StreamingTransformerLayer` is wrapped separately
- TTT layers are just `StreamingTransformerLayer` with `TTTGating` instead of `ActivationGating`
- No changes needed to FSDP policy!
- TTT components (`W_target`, `Conv1D`) are part of the layer and get sharded automatically

---

## Training Recipe Recommendations

### For Best TTT Results

#### 1. **Full Finetuning (Recommended)**
```yaml
full_finetuning: true
lora:
  enable: false

ttt:
  enabled: true
  layer_frequency: 6
  chunk_size: 256
  learning_rate: 0.001
  conv_kernel_size: 2

optim:
  lr: 1e-5  # Lower LR for full finetuning
  weight_decay: 0.1
```

**Why?** TTT needs to learn `W_target` and `Conv1D` - full finetuning allows all components to adapt together.

#### 2. **LoRA + TTT (Memory Efficient)**
```yaml
full_finetuning: false
lora:
  enable: true
  rank: 128
  scaling: 2.0

ttt:
  enabled: true
  layer_frequency: 6
  chunk_size: 256
  learning_rate: 0.001
  conv_kernel_size: 2

optim:
  lr: 2e-6  # Standard LoRA LR
  weight_decay: 0.1
```

**Why?** TTT components are trained, but base model uses LoRA adapters for efficiency.

#### 3. **Progressive Training (Paper's Approach)**
```yaml
# Stage 1: Shorter contexts (32k)
duration_sec: 50  # ~32k tokens at 24 fps
batch_size: 16
max_steps: 10000

# Stage 2: Longer contexts (128k)  
duration_sec: 200  # ~128k tokens
batch_size: 4  # Smaller batch for longer sequences
max_steps: 5000
```

**Why?** Paper showed progressive context length training improves long-context performance.

---

## Testing Strategy

### Phase 1: Sanity Check
```yaml
ttt:
  enabled: true
  layer_frequency: 32  # Only layer 32 (last layer for testing)
  chunk_size: 256
  
max_steps: 100
batch_size: 2
duration_sec: 10
```

**Goal**: Verify TTT doesn't break training, gradients flow correctly.

### Phase 2: Single Layer
```yaml
ttt:
  enabled: true
  layer_frequency: 6
  start_layer: 29  # Only last TTT layer
  chunk_size: 256
  
max_steps: 1000
```

**Goal**: Verify single TTT layer learns and improves performance.

### Phase 3: Full Deployment
```yaml
ttt:
  enabled: true
  layer_frequency: 6  # All 5 layers
  chunk_size: 256
  
max_steps: 10000+
```

**Goal**: Full training run with performance evaluation.

---

## Monitoring TTT Training

### Key Metrics to Track

#### 1. **TTT Component Norms**
Add to training loop:
```python
if state.step % args.log_freq == 0:
    ttt_metrics = {}
    for name, module in model.named_modules():
        if hasattr(module, 'target_generator'):
            # Track W_target norm
            w_target_norm = module.target_generator.W_target.weight.norm().item()
            ttt_metrics[f'ttt/{name}/w_target_norm'] = w_target_norm
            
            # Track Conv1D weights norm
            conv_norm = module.target_generator.conv1d.conv.weight.norm().item()
            ttt_metrics[f'ttt/{name}/conv_norm'] = conv_norm
    
    metrics_logger.log(ttt_metrics, step=state.step)
```

#### 2. **Fast Weight Update Magnitudes**
Track how much W_down changes during TTT updates (add instrumentation in TTTGating).

#### 3. **Per-Layer Loss Contribution**
Optional: Track loss before/after each TTT layer to see which layers help most.

---

## Potential Issues & Solutions

### Issue 1: Gradient Explosion
**Symptom**: Loss becomes NaN, gradients explode  
**Solution**: 
- Lower `ttt.learning_rate` (try 1e-4 instead of 1e-3)
- Increase `max_norm` gradient clipping
- Reduce `chunk_size` for more stable updates

### Issue 2: No Performance Improvement
**Symptom**: TTT enabled but no loss decrease  
**Solution**:
- Verify `ttt.enabled: true` in config
- Check TTT layers are actually used (add print statements)
- Increase training steps (TTT needs time to learn)
- Try full finetuning instead of LoRA

### Issue 3: OOM (Out of Memory)
**Symptom**: CUDA OOM during training  
**Solution**:
- Reduce `batch_size`
- Reduce `chunk_size` (256 â†’ 128)
- Enable `gradient_checkpointing: true`
- Reduce `ttt.layer_frequency` (fewer TTT layers)

---

## Complete Training Command

```bash
# Activate environment
conda activate moshi_ttt

# Navigate to finetune directory
cd /home/alufr/moshi_in_place_ttt/moshi-finetune

# Run training with TTT
torchrun --nproc_per_node=8 train.py \
  --config example/moshi_7B_ttt.yaml
```

---

## Summary: Required Changes

### New Files (0)
None! Everything reuses existing infrastructure.

### Modified Files (2)

1. **`moshi-finetune/finetune/args.py`** (+20 lines)
   - Add `TTTArgs` dataclass
   - Add `ttt: TTTArgs` field to `TrainArgs`

2. **`moshi-finetune/finetune/wrapped_model.py`** (+6 lines)
   - Pass TTT kwargs to `lm_kwargs_overrides`

### New Config Files (1)

3. **`moshi-finetune/example/moshi_7B_ttt.yaml`** (NEW)
   - Copy from `moshi_7B.yaml`
   - Add `ttt:` section

**Total**: ~30 lines of changes to integrate TTT training!

---

## Conclusion

âœ… **moshi-finetune is FULLY COMPATIBLE with In-Place TTT**  
âœ… **Minimal changes required** (~30 lines)  
âœ… **FSDP works out of the box**  
âœ… **Compatible with both LoRA and full finetuning**  
âœ… **Gradient flow is automatic**  
âœ… **Memory overhead is negligible** (~1-2%)  

**Next Steps:**
1. Implement core TTT modules (from IMPLEMENTATION_PLAN.md)
2. Add TTT config to moshi-finetune (this document)
3. Test with small sanity check
4. Scale up to full training

We can proceed with confidence! ðŸš€
