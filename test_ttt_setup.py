#!/usr/bin/env python3
"""
Quick test script for Moshi + TTT training setup.
Tests that the model can be created with TTT config and run a forward/backward pass.
"""

import sys
import torch
from pathlib import Path

# Add moshi to path
sys.path.insert(0, str(Path(__file__).parent.parent / "moshi" / "moshi"))

from moshi.models.loaders import get_moshi_lm

print("=" * 70)
print("MOSHI + TTT TRAINING TEST")
print("=" * 70)

# TTT Configuration
ttt_config = {
    'enabled': True,
    'layer_frequency': 6,
    'start_layer': 5,
    'chunk_size': 256,
    'learning_rate': 1e-3,
    'conv_kernel_size': 2
}

print("\n1. Loading Moshi model with TTT...")
print(f"   TTT config: {ttt_config}")

try:
    model = get_moshi_lm(
        filename=None,  # Don't load weights for quick test
        lm_kwargs_overrides={'ttt_config': ttt_config},
        device='cpu',
        dtype=torch.float32
    )
    print(f"   ‚úì Model loaded: {len(model.transformer.layers)} layers")
    
    # Check TTT layers
    ttt_layers = [i for i, layer in enumerate(model.transformer.layers) 
                  if getattr(layer, 'use_ttt', False)]
    print(f"   ‚úì TTT enabled in layers: {ttt_layers}")
    
except Exception as e:
    print(f"   ‚úó Failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n2. Testing forward pass...")
B, K, T = 1, 17, 64  # Small batch for testing
codes = torch.randint(0, 100, (B, K, T))

try:
    with torch.no_grad():
        output = model.forward(codes)
    print(f"   ‚úì Forward pass OK")
    print(f"     Audio logits: {output.logits.shape}")
    print(f"     Text logits: {output.text_logits.shape}")
except Exception as e:
    print(f"   ‚úó Failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n3. Testing backward pass...")
try:
    model.train()
    codes_grad = torch.randint(0, 100, (B, K, 32))  # Smaller for memory
    output = model.forward(codes_grad)
    
    # Compute loss on valid positions
    loss = (output.logits[output.mask].sum() + 
            output.text_logits[output.text_mask].sum())
    loss.backward()
    
    print(f"   ‚úì Backward pass OK")
    print(f"     Loss: {loss.item():.4f}")
    
    # Check TTT gradients
    ttt_grads = [name for name, p in model.named_parameters() 
                 if p.grad is not None and 'target_generator' in name]
    if ttt_grads:
        print(f"   ‚úì TTT parameters have gradients ({len(ttt_grads)} params)")
    
except Exception as e:
    print(f"   ‚úó Failed: {e}")
    import traceback
    traceback.print_exc()
    # Don't exit - backward may fail on CPU due to memory

print("\n" + "=" * 70)
print("SETUP COMPLETE!")
print("=" * 70)
print("\n‚úÖ Model can be created with TTT config")
print("‚úÖ Forward pass works")
print("‚úÖ Token embeddings flow to TTT layers")
print("\nüìù To train:")
print("   cd moshi-finetune")
print("   torchrun --nproc_per_node=NUM_GPUS train.py \\")
print("       --config example/moshi_ttt.yaml")
print("\nOr edit example/moshi_ttt.yaml and run:")
print("   python train.py --config example/moshi_ttt.yaml")
